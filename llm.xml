<?xml version="1.0" encoding="UTF-8"?>
<project name="Veotools Documentation" version="0.1.9">
  <description>Python SDK and MCP server for video generation with Google Veo</description>
  <generated>2025-09-12T06:23:32.866980</generated>
  <modules>
    <module path="/Users/boat/Projects/veotools/src/veotools/__init__.py">
      <docstring><![CDATA[Veo Tools - A toolkit for AI-powered video generation and stitching.]]></docstring>
      <function name="init" signature="init(api_key: str, log_level: str)" line="78">
      </function>
    </module>
    <module path="/Users/boat/Projects/veotools/src/veotools/api/__init__.py">
    </module>
    <module path="/Users/boat/Projects/veotools/src/veotools/api/bridge.py">
      <class name="Bridge" line="9">
        <docstring><![CDATA[A fluent API bridge for chaining video generation and processing operations.

The Bridge class provides a convenient, chainable interface for combining multiple
video operations like generation, stitching, and media management. It maintains
an internal workflow and media queue to track operations and intermediate results.

Attributes:
    workflow: Workflow object tracking all operations performed.
    media_queue: List of media file paths in processing order.
    results: List of VideoResult objects from generation operations.
    storage: StorageManager instance for file operations.

Examples:
    Basic text-to-video generation:
    >>> bridge = Bridge("my_project")
    >>> result = bridge.generate("A cat playing").save()

    Chain multiple generations and stitch:
    >>> bridge = (Bridge("movie_project")
    ...     .generate("Opening scene")
    ...     .generate("Middle scene")
    ...     .generate("Ending scene")
    ...     .stitch(overlap=1.0)
    ...     .save(Path("final_movie.mp4")))

    Image-to-video with continuation:
    >>> bridge = (Bridge()
    ...     .add_media("photo.jpg")
    ...     .generate("The person starts walking")
    ...     .generate("They walk into the distance")
    ...     .stitch())]]></docstring>
        <methods>
          <method name="__init__" signature="__init__(self, name: Optional[str])">
          </method>
          <method name="with_progress" signature="with_progress(self, callback: Callable) -> 'Bridge'">
            <docstring><![CDATA[Set a progress callback for all subsequent operations.

Args:
    callback: Function called with progress updates (message: str, percent: int).

Returns:
    Bridge: Self for method chaining.

Examples:
    >>> def show_progress(msg, pct):
    ...     print(f"{msg}: {pct}%")
    >>> bridge = Bridge().with_progress(show_progress)]]></docstring>
          </method>
          <method name="add_media" signature="add_media(self, media: Union[str, Path, List[Union[str, Path]]]) -> 'Bridge'">
            <docstring><![CDATA[Add media files to the processing queue.

Adds one or more media files (images or videos) to the internal queue.
These files can be used as inputs for subsequent generation operations.

Args:
    media: Single media path, or list of media paths to add to the queue.

Returns:
    Bridge: Self for method chaining.

Examples:
    Add a single image:
    >>> bridge = Bridge().add_media("photo.jpg")

    Add multiple videos:
    >>> files = ["video1.mp4", "video2.mp4", "video3.mp4"]
    >>> bridge = Bridge().add_media(files)

    Chain with Path objects:
    >>> bridge = Bridge().add_media(Path("input.mp4"))]]></docstring>
          </method>
          <method name="generate" signature="generate(self, prompt: str, model: str) -> 'Bridge'">
            <docstring><![CDATA[Generate a video using text prompt and optional media input.

Generates a video based on the prompt and the most recent media in the queue.
The generation method is automatically selected based on the media type:
- No media: text-to-video generation
- Image media: image-to-video generation
- Video media: video continuation generation

Args:
    prompt: Text description for video generation.
    model: Veo model to use. Defaults to "veo-3.0-fast-generate-preview".
    **kwargs: Additional generation parameters including:
        - extract_at: Time offset for video continuation (float)
        - duration_seconds: Video duration (int)
        - person_generation: Person policy (str)
        - enhance: Whether to enhance prompt (bool)

Returns:
    Bridge: Self for method chaining.

Raises:
    RuntimeError: If video generation fails.

Examples:
    Text-to-video generation:
    >>> bridge = Bridge().generate("A sunset over mountains")

    Image-to-video with existing media:
    >>> bridge = (Bridge()
    ...     .add_media("landscape.jpg")
    ...     .generate("Clouds moving across the sky"))

    Video continuation:
    >>> bridge = (Bridge()
    ...     .add_media("scene1.mp4")
    ...     .generate("The action continues", extract_at=-2.0))

    Custom model and parameters:
    >>> bridge = Bridge().generate(
    ...     "A dancing robot",
    ...     model="veo-2.0",
    ...     duration_seconds=10,
    ...     enhance=True
    ... )]]></docstring>
          </method>
          <method name="generate_transition" signature="generate_transition(self, prompt: Optional[str], model: str) -> 'Bridge'">
            <docstring><![CDATA[Generate a transition video between the last two media items.

Creates a smooth transition video that bridges the gap between the two most
recent media items in the queue. The transition is generated from a frame
extracted near the end of the second-to-last video.

Args:
    prompt: Description of the desired transition. If None, uses a default
        "smooth cinematic transition between scenes".
    model: Veo model to use. Defaults to "veo-3.0-fast-generate-preview".

Returns:
    Bridge: Self for method chaining.

Raises:
    ValueError: If fewer than 2 media items are in the queue.

Examples:
    Generate default transition:
    >>> bridge = (Bridge()
    ...     .add_media(["scene1.mp4", "scene2.mp4"])
    ...     .generate_transition())

    Custom transition prompt:
    >>> bridge = (Bridge()
    ...     .generate("Day scene")
    ...     .generate("Night scene")
    ...     .generate_transition("Gradual sunset transition"))

Note:
    The transition video is inserted between the last two media items,
    creating a sequence like: [media_a, transition, media_b, ...]]]></docstring>
          </method>
          <method name="stitch" signature="stitch(self, overlap: float) -> 'Bridge'">
            <docstring><![CDATA[Stitch all videos in the queue into a single continuous video.

Combines all video files in the media queue into one seamless video.
Non-video files (images) are automatically filtered out. The result
replaces the entire media queue.

Args:
    overlap: Duration in seconds to trim from the end of each video
        (except the last) for smooth transitions. Defaults to 1.0.

Returns:
    Bridge: Self for method chaining.

Raises:
    ValueError: If fewer than 2 videos are available for stitching.

Examples:
    Stitch with default overlap:
    >>> bridge = (Bridge()
    ...     .generate("Scene 1")
    ...     .generate("Scene 2")
    ...     .generate("Scene 3")
    ...     .stitch())

    Stitch without overlap:
    >>> bridge = bridge.stitch(overlap=0.0)

    Stitch with longer transitions:
    >>> bridge = bridge.stitch(overlap=2.5)

Note:
    After stitching, the media queue contains only the final stitched video.]]></docstring>
          </method>
          <method name="save" signature="save(self, output_path: Optional[Union[str, Path]]) -> Path">
            <docstring><![CDATA[Save the final result to a specified path or return the current path.

Saves the most recent media file in the queue to the specified output path,
or returns the current path if no output path is provided.

Args:
    output_path: Optional destination path. If provided, copies the current
        result to this location. If None, returns the current file path.

Returns:
    Path: The path where the final result is located.

Raises:
    ValueError: If no media is available to save.

Examples:
    Save to custom location:
    >>> final_path = bridge.save("my_video.mp4")
    >>> print(f"Video saved to: {final_path}")

    Get current result path:
    >>> current_path = bridge.save()
    >>> print(f"Current result: {current_path}")

    Save with Path object:
    >>> output_dir = Path("outputs")
    >>> final_path = bridge.save(output_dir / "final_video.mp4")]]></docstring>
          </method>
          <method name="get_workflow" signature="get_workflow(self) -> Workflow">
            <docstring><![CDATA[Get the workflow object containing all performed operations.

Returns:
    Workflow: The workflow tracking all operations and their parameters.

Examples:
    >>> bridge = Bridge("project").generate("A scene").stitch()
    >>> workflow = bridge.get_workflow()
    >>> print(workflow.name)]]></docstring>
          </method>
          <method name="to_dict" signature="to_dict(self) -> dict">
            <docstring><![CDATA[Convert the workflow to a dictionary representation.

Returns:
    dict: Dictionary containing workflow steps and metadata.

Examples:
    >>> bridge = Bridge("test").generate("Scene")
    >>> workflow_dict = bridge.to_dict()
    >>> print(workflow_dict.keys())]]></docstring>
          </method>
          <method name="clear" signature="clear(self) -> 'Bridge'">
            <docstring><![CDATA[Clear the media queue, removing all queued media files.

Returns:
    Bridge: Self for method chaining.

Examples:
    >>> bridge = Bridge().add_media(["a.mp4", "b.mp4"]).clear()
    >>> # Media queue is now empty]]></docstring>
          </method>
        </methods>
      </class>
    </module>
    <module path="/Users/boat/Projects/veotools/src/veotools/api/mcp_api.py">
      <docstring><![CDATA[MCP-friendly API wrappers for Veo Tools.

This module exposes small, deterministic, JSON-first functions intended for
use in Model Context Protocol (MCP) servers. It builds on top of the existing
blocking SDK functions by providing a non-blocking job lifecycle:

- generate_start(params) -> submits a generation job and returns immediately
- generate_get(job_id) -> fetches job status/progress/result
- generate_cancel(job_id) -> requests cancellation for a running job

It also provides environment/system helpers:
- preflight() -> checks API key, ffmpeg, and filesystem permissions
- version() -> returns package and key dependency versions

Design notes:
- Jobs are persisted as JSON files under StorageManager's base directory
  ("output/ops"). This allows stateless MCP handlers to inspect progress
  and results across processes.
- A background thread runs the blocking generation call and updates job state
  via the JobStore. Cancellation is cooperative: the on_progress callback
  checks a cancel flag in the persisted job state and raises Cancelled.]]></docstring>
      <class name="Cancelled" line="50">
        <docstring><![CDATA[Exception raised to signal cooperative cancellation of a generation job.

This exception is raised internally when a job's cancel_requested flag is set
to True, allowing for graceful termination of long-running operations.]]></docstring>
      </class>
      <class name="JobRecord" line="77">
        <docstring><![CDATA[Data class representing a generation job's state and metadata.

Stores all information about a generation job including status, progress,
parameters, results, and error information. Used for job persistence and
state management across processes.

Attributes:
    job_id: Unique identifier for the job.
    status: Current job status (pending|processing|complete|failed|cancelled).
    progress: Progress percentage (0-100).
    message: Current status message.
    created_at: Unix timestamp when job was created.
    updated_at: Unix timestamp of last update.
    cancel_requested: Whether cancellation has been requested.
    kind: Generation type (text|image|video).
    params: Dictionary of generation parameters.
    result: Optional result data when job completes.
    error_code: Optional error code if job fails.
    error_message: Optional error description if job fails.
    remote_operation_id: Optional ID from the remote API operation.]]></docstring>
        <methods>
          <method name="to_json" signature="to_json(self) -> str">
            <docstring><![CDATA[Convert the job record to JSON string representation.

Returns:
    str: JSON string representation of the job record.]]></docstring>
          </method>
        </methods>
      </class>
      <class name="JobStore" line="122">
        <docstring><![CDATA[File-based persistence layer for generation jobs.

Manages storage and retrieval of job records using JSON files in the
filesystem. Each job is stored as a separate JSON file under the
`output/ops/{job_id}.json` path structure.

This design allows stateless MCP handlers to inspect job progress
and results across different processes and sessions.

Attributes:
    storage: StorageManager instance for base path management.
    ops_dir: Directory path where job files are stored.]]></docstring>
        <methods>
          <method name="__init__" signature="__init__(self, storage: Optional[StorageManager])">
            <docstring><![CDATA[Initialize the job store with optional custom storage manager.

Args:
    storage: Optional StorageManager instance. If None, creates a new one.]]></docstring>
          </method>
          <method name="_path" signature="_path(self, job_id: str) -> Path">
            <docstring><![CDATA[Get the file system path for a job record.

Args:
    job_id: The unique job identifier.
    
Returns:
    Path: File system path where the job record should be stored.]]></docstring>
          </method>
          <method name="create" signature="create(self, record: JobRecord) -> None">
            <docstring><![CDATA[Create a new job record on disk.

Args:
    record: JobRecord instance to persist.
    
Raises:
    OSError: If file creation fails.]]></docstring>
          </method>
          <method name="read" signature="read(self, job_id: str) -> Optional[JobRecord]">
            <docstring><![CDATA[Read a job record from disk.

Args:
    job_id: The unique job identifier.
    
Returns:
    JobRecord: The job record if found, None otherwise.
    
Raises:
    json.JSONDecodeError: If the stored JSON is invalid.]]></docstring>
          </method>
          <method name="update" signature="update(self, record: JobRecord) -> JobRecord">
            <docstring><![CDATA[Update a job record with new values and persist to disk.

Args:
    record: The JobRecord instance to update.
    **updates: Key-value pairs of attributes to update.
    
Returns:
    JobRecord: The updated job record.
    
Raises:
    OSError: If file write fails.]]></docstring>
          </method>
          <method name="request_cancel" signature="request_cancel(self, job_id: str) -> Optional[JobRecord]">
            <docstring><![CDATA[Request cancellation of a job by setting the cancel flag.

Args:
    job_id: The unique job identifier.
    
Returns:
    JobRecord: Updated job record if found, None otherwise.
    
Raises:
    OSError: If file write fails.]]></docstring>
          </method>
        </methods>
      </class>
      <function name="preflight" signature="preflight() -> Dict[str, Any]" line="233">
        <docstring><![CDATA[Check environment and system prerequisites for video generation.

Performs comprehensive system checks to ensure all required dependencies
and configurations are available for successful video generation operations.
This includes API key validation, FFmpeg availability, and filesystem permissions.

Returns:
    dict: JSON-serializable dictionary containing:
        - ok (bool): Overall system readiness status
        - gemini_api_key (bool): Whether GEMINI_API_KEY is set
        - ffmpeg (dict): FFmpeg installation status and version info
        - write_permissions (bool): Whether output directory is writable
        - base_path (str): Absolute path to the base output directory

Examples:
    >>> status = preflight()
    >>> if not status['ok']:
    ...     print("System not ready for generation")
    ...     if not status['gemini_api_key']:
    ...         print("Please set GEMINI_API_KEY environment variable")
    ...     if not status['ffmpeg']['installed']:
    ...         print("Please install FFmpeg for video processing")
    >>> else:
    ...     print(f"System ready! Output directory: {status['base_path']}")

Note:
    This function is designed to be called before starting any video generation
    operations to ensure the environment is properly configured.]]></docstring>
      </function>
      <function name="version" signature="version() -> Dict[str, Any]" line="305">
        <docstring><![CDATA[Report package and dependency versions in a JSON-friendly format.

Collects version information for veotools and its key dependencies,
providing a comprehensive overview of the current software environment.
Useful for debugging and support purposes.

Returns:
    dict: Dictionary containing:
        - veotools (str|None): veotools package version
        - dependencies (dict): Versions of key Python packages:
            - google-genai: Google GenerativeAI library version
            - opencv-python: OpenCV library version  
            - requests: HTTP requests library version
            - python-dotenv: Environment file loader version
        - ffmpeg (str|None): FFmpeg version string if available
        
Examples:
    >>> versions = version()
    >>> print(f"veotools: {versions['veotools']}")
    >>> print(f"Google GenAI: {versions['dependencies']['google-genai']}")
    >>> if versions['ffmpeg']:
    ...     print(f"FFmpeg: {versions['ffmpeg']}")
    >>> else:
    ...     print("FFmpeg not available")
    
Note:
    Returns None for any package that cannot be found or queried.
    This is expected behavior and not an error condition.]]></docstring>
      </function>
      <function name="_build_job" signature="_build_job(kind: str, params: Dict[str, Any]) -> JobRecord" line="372">
        <docstring><![CDATA[Create a new job record with initial values.

Args:
    kind: Type of generation job (text|image|video).
    params: Generation parameters dictionary.
    
Returns:
    JobRecord: New job record with unique ID and initial status.]]></docstring>
      </function>
      <function name="_validate_generate_inputs" signature="_validate_generate_inputs(params: Dict[str, Any]) -> None" line="396">
        <docstring><![CDATA[Validate generation parameters for consistency and file existence.

Args:
    params: Generation parameters to validate.
    
Raises:
    ValueError: If prompt is missing/invalid or multiple input types specified.
    FileNotFoundError: If specified input files don't exist.]]></docstring>
      </function>
      <function name="generate_start" signature="generate_start(params: Dict[str, Any]) -> Dict[str, Any]" line="423">
        <docstring><![CDATA[Start a video generation job and return immediately with job details.

Initiates a video generation job in the background and returns immediately
with job tracking information. The actual generation runs asynchronously
and can be monitored using generate_get().

Args:
    params: Generation parameters dictionary containing:
        - prompt (str): Required text description for generation
        - model (str, optional): Model to use (defaults to veo-3.0-fast-generate-preview)
        - input_image_path (str, optional): Path to input image for image-to-video
        - input_video_path (str, optional): Path to input video for continuation
        - extract_at (float, optional): Time offset for video continuation
        - options (dict, optional): Additional model-specific options

Returns:
    dict: Job information containing:
        - job_id (str): Unique job identifier for tracking
        - status (str): Initial job status ("processing")
        - progress (int): Initial progress (0)
        - message (str): Status message
        - kind (str): Generation type (text|image|video)
        - created_at (float): Job creation timestamp

Raises:
    ValueError: If required parameters are missing or invalid.
    FileNotFoundError: If input media files don't exist.

Examples:
    Start text-to-video generation:
    >>> job = generate_start({"prompt": "A sunset over mountains"})
    >>> print(f"Job started: {job['job_id']}")

    Start image-to-video generation:
    >>> job = generate_start({
    ...     "prompt": "The person starts walking",
    ...     "input_image_path": "photo.jpg"
    ... })

    Start video continuation:
    >>> job = generate_start({
    ...     "prompt": "The action continues",
    ...     "input_video_path": "scene1.mp4",
    ...     "extract_at": -2.0
    ... })

    Start with custom model and options:
    >>> job = generate_start({
    ...     "prompt": "A dancing robot",
    ...     "model": "veo-2.0",
    ...     "options": {"duration_seconds": 10, "enhance": True}
    ... })

Note:
    The job runs in a background thread. Use generate_get() to check
    progress and retrieve results when complete.]]></docstring>
      </function>
      <function name="generate_get" signature="generate_get(job_id: str) -> Dict[str, Any]" line="517">
        <docstring><![CDATA[Get the current status and results of a generation job.

Retrieves the current state of a generation job including progress,
status, and results if complete. This function can be called repeatedly
to monitor job progress.

Args:
    job_id: The unique job identifier returned by generate_start().

Returns:
    dict: Job status information containing:
        - job_id (str): The job identifier
        - status (str): Current status (processing|complete|failed|cancelled)
        - progress (int): Progress percentage (0-100)
        - message (str): Current status message
        - kind (str): Generation type (text|image|video)
        - remote_operation_id (str|None): Remote API operation ID if available
        - updated_at (float): Last update timestamp
        - result (dict, optional): Generation results when status is "complete"
        - error_code (str, optional): Error code if status is "failed"
        - error_message (str, optional): Error description if status is "failed"

    If job_id is not found, returns:
        - error_code (str): "VALIDATION"
        - error_message (str): Error description

Examples:
    Check job progress:
    >>> status = generate_get(job_id)
    >>> print(f"Progress: {status['progress']}% - {status['message']}")

    Wait for completion:
    >>> import time
    >>> while True:
    ...     status = generate_get(job_id)
    ...     if status['status'] == 'complete':
    ...         print(f"Video ready: {status['result']['path']}")
    ...         break
    ...     elif status['status'] == 'failed':
    ...         print(f"Generation failed: {status['error_message']}")
    ...         break
    ...     time.sleep(5)

    Handle different outcomes:
    >>> status = generate_get(job_id)
    >>> if status['status'] == 'complete':
    ...     video_path = status['result']['path']
    ...     metadata = status['result']['metadata']
    ...     print(f"Success! Video: {video_path}")
    ...     print(f"Duration: {metadata['duration']}s")
    ... elif status['status'] == 'failed':
    ...     print(f"Error ({status['error_code']}): {status['error_message']}")
    ... else:
    ...     print(f"Still processing: {status['progress']}%")]]></docstring>
      </function>
      <function name="generate_cancel" signature="generate_cancel(job_id: str) -> Dict[str, Any]" line="596">
        <docstring><![CDATA[Request cancellation of a running generation job.

Attempts to cancel a generation job that is currently processing.
Cancellation is cooperative - the job will stop at the next progress
update checkpoint. Already completed or failed jobs cannot be cancelled.

Args:
    job_id: The unique job identifier to cancel.

Returns:
    dict: Cancellation response containing:
        - job_id (str): The job identifier
        - status (str): "cancelling" if request was accepted

    If job_id is not found, returns:
        - error_code (str): "VALIDATION"
        - error_message (str): Error description

Examples:
    Cancel a running job:
    >>> response = generate_cancel(job_id)
    >>> if 'error_code' not in response:
    ...     print(f"Cancellation requested for job {response['job_id']}")
    ... else:
    ...     print(f"Cancel failed: {response['error_message']}")

    Check if cancellation succeeded:
    >>> generate_cancel(job_id)
    >>> time.sleep(2)
    >>> status = generate_get(job_id)
    >>> if status['status'] == 'cancelled':
    ...     print("Job successfully cancelled")

Note:
    Cancellation may not be immediate - the job will stop at the next
    progress checkpoint. Monitor with generate_get() to confirm cancellation.]]></docstring>
      </function>
      <function name="_run_generation" signature="_run_generation(job_id: str) -> None" line="642">
        <docstring><![CDATA[Background worker function that runs the actual generation process.

This function runs in a separate thread and handles the entire generation
lifecycle including progress reporting, cooperative cancellation, and
error handling. Updates job state throughout the process.

Args:
    job_id: The unique job identifier to process.
    
Note:
    This is an internal function called by the background thread system.
    It should not be called directly.]]></docstring>
      </function>
      <function name="_sanitize_result" signature="_sanitize_result(result: Dict[str, Any]) -> Dict[str, Any]" line="723">
        <docstring><![CDATA[Ensure result dictionary is JSON-serializable with proper types.

Args:
    result: Result dictionary to sanitize.
    
Returns:
    dict: Sanitized result with Path objects converted to strings.]]></docstring>
      </function>
      <function name="list_models" signature="list_models(include_remote: bool) -> Dict[str, Any]" line="763">
        <docstring><![CDATA[List available video generation models with their capabilities.

Retrieves information about available Veo models including their capabilities,
default settings, and performance characteristics. Combines static model
registry with optional remote model discovery.

Args:
    include_remote: Whether to include models discovered from the remote API.
        If True, attempts to fetch additional model information from Google's API.
        If False, returns only the static model registry. Defaults to True.

Returns:
    dict: Model information containing:
        - models (list): List of model dictionaries, each containing:
            - id (str): Model identifier (e.g., "veo-3.0-fast-generate-preview")
            - name (str): Human-readable model name
            - capabilities (dict): Feature flags:
                - supports_duration (bool): Can specify custom duration
                - supports_enhance (bool): Can enhance prompts
                - supports_fps (bool): Can specify frame rate
                - supports_audio (bool): Can generate audio
            - default_duration (float|None): Default video duration in seconds
            - generation_time (float|None): Estimated generation time in seconds
            - source (str): Data source ("static", "remote", or "static+remote")

Examples:
    List all available models:
    >>> models = list_models()
    >>> for model in models['models']:
    ...     print(f"{model['name']} ({model['id']})")
    ...     if model['capabilities']['supports_duration']:
    ...         print(f"  Default duration: {model['default_duration']}s")

    Find models with specific capabilities:
    >>> models = list_models()
    >>> audio_models = [
    ...     m for m in models['models']
    ...     if m['capabilities']['supports_audio']
    ... ]
    >>> print(f"Found {len(audio_models)} models with audio support")

    Use only static model registry:
    >>> models = list_models(include_remote=False)
    >>> static_models = [m for m in models['models'] if m['source'] == 'static']

Note:
    Results are cached for 10 minutes to improve performance. Remote model
    discovery failures are silently ignored - static registry is always available.]]></docstring>
      </function>
      <function name="cache_create_from_files" signature="cache_create_from_files(model: str, files: list[str], system_instruction: Optional[str]) -> Dict[str, Any]" line="883">
        <docstring><![CDATA[Create a cached content handle from local file paths.

Uploads local files to create a cached content context that can be reused
across multiple API calls for efficiency. This is particularly useful when
working with large files or when making multiple requests with the same context.

Args:
    model: The model identifier to associate with the cached content.
    files: List of local file paths to upload and cache.
    system_instruction: Optional system instruction to include with the cache.

Returns:
    dict: Cache creation result containing:
        - name (str): Unique cache identifier for future reference
        - model (str): The associated model identifier
        - system_instruction (str|None): The system instruction if provided
        - contents_count (int): Number of files successfully cached

    On failure, returns:
        - error_code (str): Error classification
        - error_message (str): Detailed error description

Examples:
    Cache multiple reference images:
    >>> result = cache_create_from_files(
    ...     "veo-3.0-fast-generate-preview",
    ...     ["ref1.jpg", "ref2.jpg", "ref3.jpg"],
    ...     "These are reference images for style consistency"
    ... )
    >>> if 'name' in result:
    ...     cache_name = result['name']
    ...     print(f"Cache created: {cache_name}")
    ... else:
    ...     print(f"Cache creation failed: {result['error_message']}")

    Cache video reference:
    >>> result = cache_create_from_files(
    ...     "veo-2.0",
    ...     ["reference_video.mp4"]
    ... )

Raises:
    The function catches all exceptions and returns them as error dictionaries
    rather than raising them directly.

Note:
    Files are uploaded to Google's servers as part of the caching process.
    Ensure you have appropriate permissions for the files and comply with
    Google's usage policies.]]></docstring>
      </function>
      <function name="cache_get" signature="cache_get(name: str) -> Dict[str, Any]" line="961">
        <docstring><![CDATA[Retrieve cached content metadata by cache name.

Fetches information about a previously created cached content entry,
including lifecycle information like expiration times and creation dates.

Args:
    name: The unique cache identifier returned by cache_create_from_files().

Returns:
    dict: Cache metadata containing:
        - name (str): The cache identifier
        - ttl (str|None): Time-to-live if available
        - expire_time (str|None): Expiration timestamp if available
        - create_time (str|None): Creation timestamp if available

    On failure, returns:
        - error_code (str): Error classification
        - error_message (str): Detailed error description

Examples:
    Check cache status:
    >>> cache_info = cache_get(cache_name)
    >>> if 'error_code' not in cache_info:
    ...     print(f"Cache {cache_info['name']} is active")
    ...     if cache_info.get('expire_time'):
    ...         print(f"Expires: {cache_info['expire_time']}")
    ... else:
    ...     print(f"Cache not found: {cache_info['error_message']}")

Note:
    Available metadata fields may vary depending on the Google GenAI
    library version and cache configuration.]]></docstring>
      </function>
      <function name="cache_list" signature="cache_list() -> Dict[str, Any]" line="1013">
        <docstring><![CDATA[List all cached content entries with their metadata.

Retrieves a list of all cached content entries accessible to the current
API key, including their metadata and lifecycle information.

Returns:
    dict: Cache listing containing:
        - caches (list): List of cache entries, each containing:
            - name (str): Cache identifier
            - model (str|None): Associated model if available
            - display_name (str|None): Human-readable name if available
            - create_time (str|None): Creation timestamp if available
            - update_time (str|None): Last update timestamp if available
            - expire_time (str|None): Expiration timestamp if available
            - usage_metadata (dict|None): Usage statistics if available

    On failure, returns:
        - error_code (str): Error classification
        - error_message (str): Detailed error description

Examples:
    List all caches:
    >>> cache_list_result = cache_list()
    >>> if 'caches' in cache_list_result:
    ...     for cache in cache_list_result['caches']:
    ...         print(f"Cache: {cache['name']}")
    ...         if cache.get('model'):
    ...             print(f"  Model: {cache['model']}")
    ...         if cache.get('expire_time'):
    ...             print(f"  Expires: {cache['expire_time']}")
    ... else:
    ...     print(f"Failed to list caches: {cache_list_result['error_message']}")

    Find caches by model:
    >>> result = cache_list()
    >>> if 'caches' in result:
    ...     veo3_caches = [
    ...         c for c in result['caches']
    ...         if c.get('model', '').startswith('veo-3')
    ...     ]

Note:
    Metadata availability depends on the Google GenAI library version
    and individual cache configurations.]]></docstring>
      </function>
      <function name="cache_update" signature="cache_update(name: str, ttl_seconds: Optional[int], expire_time_iso: Optional[str]) -> Dict[str, Any]" line="1078">
        <docstring><![CDATA[Update TTL or expiration time for a cached content entry.

Modifies the lifecycle settings of an existing cached content entry.
You can specify either a TTL (time-to-live) in seconds or an absolute
expiration time, but not both.

Args:
    name: The unique cache identifier to update.
    ttl_seconds: Optional time-to-live in seconds (e.g., 300 for 5 minutes).
    expire_time_iso: Optional timezone-aware ISO-8601 datetime string
        (e.g., "2024-01-15T10:30:00Z").

Returns:
    dict: Update result containing:
        - name (str): The cache identifier
        - expire_time (str|None): New expiration time if available
        - ttl (str|None): New TTL setting if available
        - update_time (str|None): Update timestamp if available

    On failure, returns:
        - error_code (str): Error classification
        - error_message (str): Detailed error description

Examples:
    Extend cache TTL to 1 hour:
    >>> result = cache_update(cache_name, ttl_seconds=3600)
    >>> if 'error_code' not in result:
    ...     print(f"Cache TTL updated: {result.get('ttl')}")
    ... else:
    ...     print(f"Update failed: {result['error_message']}")

    Set specific expiration time:
    >>> result = cache_update(
    ...     cache_name,
    ...     expire_time_iso="2024-12-31T23:59:59Z"
    ... )

    Extend by 30 minutes:
    >>> result = cache_update(cache_name, ttl_seconds=1800)

Raises:
    Returns error dict instead of raising exceptions directly.

Note:
    - Only one of ttl_seconds or expire_time_iso should be provided
    - TTL is relative to the current time when the update is processed
    - expire_time_iso should be in UTC timezone for consistency]]></docstring>
      </function>
      <function name="cache_delete" signature="cache_delete(name: str) -> Dict[str, Any]" line="1155">
        <docstring><![CDATA[Delete a cached content entry by name.

Permanently removes a cached content entry and all associated files
from the system. This action cannot be undone.

Args:
    name: The unique cache identifier to delete.

Returns:
    dict: Deletion result containing:
        - deleted (bool): True if deletion was successful
        - name (str): The cache identifier that was deleted

    On failure, returns:
        - error_code (str): Error classification
        - error_message (str): Detailed error description

Examples:
    Delete a specific cache:
    >>> result = cache_delete(cache_name)
    >>> if result.get('deleted'):
    ...     print(f"Cache {result['name']} deleted successfully")
    ... else:
    ...     print(f"Deletion failed: {result.get('error_message')}")

    Delete with error handling:
    >>> result = cache_delete("non-existent-cache")
    >>> if 'error_code' in result:
    ...     print(f"Error: {result['error_message']}")

Note:
    Deletion is permanent and cannot be reversed. Ensure you no longer
    need the cached content before calling this function.]]></docstring>
      </function>
    </module>
    <module path="/Users/boat/Projects/veotools/src/veotools/cli.py">
      <docstring><![CDATA[Veotools command-line interface (no extra deps).

Usage examples:
  veo preflight
  veo list-models --remote
  veo generate --prompt "cat riding a hat" --model veo-3.0-fast-generate-preview
  veo continue --video dog.mp4 --prompt "the dog finds a treasure chest" --overlap 1.0]]></docstring>
      <function name="_print_progress" signature="_print_progress(message: str, percent: int)" line="20">
      </function>
      <function name="cmd_preflight" signature="cmd_preflight(_: argparse.Namespace) -> int" line="29">
      </function>
      <function name="cmd_list_models" signature="cmd_list_models(ns: argparse.Namespace) -> int" line="36">
      </function>
      <function name="cmd_generate" signature="cmd_generate(ns: argparse.Namespace) -> int" line="47">
      </function>
      <function name="cmd_continue" signature="cmd_continue(ns: argparse.Namespace) -> int" line="95">
      </function>
      <function name="build_parser" signature="build_parser() -> argparse.ArgumentParser" line="136">
      </function>
      <function name="main" signature="main(argv: Optional[list[str]]) -> int" line="179">
      </function>
    </module>
    <module path="/Users/boat/Projects/veotools/src/veotools/core.py">
      <class name="VeoClient" line="11">
        <docstring><![CDATA[Singleton client for Google GenAI API interactions.

This class implements a singleton pattern to ensure only one client instance
is created throughout the application lifecycle. It manages the authentication
and connection to Google's Generative AI API.

Attributes:
    client: The underlying Google GenAI client instance.

Raises:
    ValueError: If GEMINI_API_KEY environment variable is not set.

Examples:
    >>> client = VeoClient()
    >>> api_client = client.client
    >>> # Use api_client for API calls]]></docstring>
        <methods>
          <method name="__new__" signature="__new__(cls)">
            <docstring><![CDATA[Create or return the singleton instance.

Returns:
    VeoClient: The singleton VeoClient instance.]]></docstring>
          </method>
          <method name="__init__" signature="__init__(self)">
            <docstring><![CDATA[Initialize the GenAI client with API key from environment.

The client is only initialized once, even if __init__ is called multiple times.

Raises:
    ValueError: If GEMINI_API_KEY is not found in environment variables.]]></docstring>
          </method>
          <method name="client" signature="client(self)">
            <docstring><![CDATA[Get the Google GenAI client instance.

Returns:
    genai.Client: The initialized GenAI client.]]></docstring>
          </method>
        </methods>
      </class>
      <class name="StorageManager" line="65">
        <methods>
          <method name="__init__" signature="__init__(self, base_path: Optional[str])">
            <docstring><![CDATA[Manage output directories for videos, frames, and temp files.

Default resolution order for base path:
1. VEO_OUTPUT_DIR environment variable (if set)
2. Current working directory (./output)
3. Package-adjacent directory (../output) as a last resort]]></docstring>
          </method>
          <method name="get_video_path" signature="get_video_path(self, filename: str) -> Path">
            <docstring><![CDATA[Get the full path for a video file.

Args:
    filename: Name of the video file.
    
Returns:
    Path: Full path to the video file in the videos directory.
    
Examples:
    >>> manager = StorageManager()
    >>> path = manager.get_video_path("output.mp4")
    >>> print(path)  # /path/to/output/videos/output.mp4]]></docstring>
          </method>
          <method name="get_frame_path" signature="get_frame_path(self, filename: str) -> Path">
            <docstring><![CDATA[Get the full path for a frame image file.

Args:
    filename: Name of the frame file.
    
Returns:
    Path: Full path to the frame file in the frames directory.
    
Examples:
    >>> manager = StorageManager()
    >>> path = manager.get_frame_path("frame_001.jpg")
    >>> print(path)  # /path/to/output/frames/frame_001.jpg]]></docstring>
          </method>
          <method name="get_temp_path" signature="get_temp_path(self, filename: str) -> Path">
            <docstring><![CDATA[Get the full path for a temporary file.

Args:
    filename: Name of the temporary file.
    
Returns:
    Path: Full path to the file in the temp directory.
    
Examples:
    >>> manager = StorageManager()
    >>> path = manager.get_temp_path("processing.tmp")
    >>> print(path)  # /path/to/output/temp/processing.tmp]]></docstring>
          </method>
          <method name="cleanup_temp" signature="cleanup_temp(self)">
            <docstring><![CDATA[Remove all files from the temporary directory.

This method safely removes all files in the temp directory while preserving
the directory structure. Errors during deletion are silently ignored.

Examples:
    >>> manager = StorageManager()
    >>> manager.cleanup_temp()
    >>> # All temp files are now deleted]]></docstring>
          </method>
          <method name="get_url" signature="get_url(self, path: Path) -> Optional[str]">
            <docstring><![CDATA[Convert a file path to a file:// URL.

Args:
    path: Path to the file.
    
Returns:
    Optional[str]: File URL if the file exists, None otherwise.
    
Examples:
    >>> manager = StorageManager()
    >>> video_path = manager.get_video_path("test.mp4")
    >>> url = manager.get_url(video_path)
    >>> print(url)  # file:///absolute/path/to/output/videos/test.mp4]]></docstring>
          </method>
        </methods>
      </class>
      <class name="ProgressTracker" line="193">
        <docstring><![CDATA[Track and report progress for long-running operations.

This class provides a simple interface for tracking progress updates during
video generation and processing operations. It supports custom callbacks
or falls back to logging.

Attributes:
    callback: Function to call with progress updates.
    current_progress: Current progress percentage (0-100).
    logger: Logger instance for default progress reporting.

Examples:
    >>> def my_callback(msg: str, pct: int):
    ...     print(f"{msg}: {pct}%")
    >>> tracker = ProgressTracker(callback=my_callback)
    >>> tracker.start("Processing")
    >>> tracker.update("Halfway", 50)
    >>> tracker.complete("Done")]]></docstring>
        <methods>
          <method name="__init__" signature="__init__(self, callback: Optional[Callable])">
            <docstring><![CDATA[Initialize the progress tracker.

Args:
    callback: Optional callback function that receives (message, percent).
             If not provided, uses default logging.]]></docstring>
          </method>
          <method name="default_progress" signature="default_progress(self, message: str, percent: int)">
            <docstring><![CDATA[Default progress callback that logs to the logger.

Args:
    message: Progress message.
    percent: Progress percentage.]]></docstring>
          </method>
          <method name="update" signature="update(self, message: str, percent: int)">
            <docstring><![CDATA[Update progress and trigger callback.

Args:
    message: Progress message to display.
    percent: Current progress percentage (0-100).]]></docstring>
          </method>
          <method name="start" signature="start(self, message: str)">
            <docstring><![CDATA[Mark the start of an operation (0% progress).

Args:
    message: Starting message, defaults to "Starting".]]></docstring>
          </method>
          <method name="complete" signature="complete(self, message: str)">
            <docstring><![CDATA[Mark the completion of an operation (100% progress).

Args:
    message: Completion message, defaults to "Complete".]]></docstring>
          </method>
        </methods>
      </class>
      <class name="ModelConfig" line="259">
        <docstring><![CDATA[Configuration and capabilities for different Veo video generation models.

This class manages model-specific configurations and builds generation
configs based on model capabilities. It handles feature availability,
parameter validation, and safety settings.

Attributes:
    MODELS: Dictionary of available models and their configurations.]]></docstring>
        <methods>
          <method name="get_config" signature="get_config(cls, model: str) -> dict">
            <docstring><![CDATA[Get configuration for a specific model.

Args:
    model: Model identifier (with or without "models/" prefix).
    
Returns:
    dict: Model configuration dictionary containing capabilities and defaults.
    
Examples:
    >>> config = ModelConfig.get_config("veo-3.0-fast-generate-preview")
    >>> print(config["name"])  # "Veo 3.0 Fast"
    >>> print(config["supports_duration"])  # False]]></docstring>
          </method>
          <method name="build_generation_config" signature="build_generation_config(cls, model: str) -> types.GenerateVideosConfig">
            <docstring><![CDATA[Build a generation configuration based on model capabilities.

This method creates a GenerateVideosConfig object with parameters
appropriate for the specified model. It validates parameters against
model capabilities and handles safety settings.

Args:
    model: Model identifier to use for generation.
    **kwargs: Generation parameters including:
        - number_of_videos: Number of videos to generate (default: 1)
        - duration_seconds: Video duration (if supported by model)
        - enhance_prompt: Whether to enhance the prompt (if supported)
        - fps: Frames per second (if supported)
        - aspect_ratio: Video aspect ratio (e.g., "16:9")
        - negative_prompt: Negative prompt for generation
        - person_generation: Person generation setting
        - safety_settings: List of safety settings
        - cached_content: Cached content handle

Returns:
    types.GenerateVideosConfig: Configuration object for video generation.
    
Raises:
    ValueError: If aspect_ratio is not supported by the model.
    
Examples:
    >>> config = ModelConfig.build_generation_config(
    ...     "veo-3.0-fast-generate-preview",
    ...     number_of_videos=2,
    ...     aspect_ratio="16:9"
    ... )]]></docstring>
          </method>
        </methods>
      </class>
    </module>
    <module path="/Users/boat/Projects/veotools/src/veotools/generate/__init__.py">
      <docstring><![CDATA[Video generation module for Veo Tools.]]></docstring>
    </module>
    <module path="/Users/boat/Projects/veotools/src/veotools/generate/video.py">
      <docstring><![CDATA[Video generation functions for Veo Tools.]]></docstring>
      <function name="_validate_person_generation" signature="_validate_person_generation(model: str, mode: str, person_generation: Optional[str]) -> None" line="15">
        <docstring><![CDATA[Validate person_generation parameter based on model and generation mode.

Validates the person_generation parameter against the constraints defined for different
Veo model versions and generation modes. Veo 3.0 and 2.0 have different allowed values
for text vs image/video generation modes.

Args:
    model: The model identifier (e.g., "veo-3.0-fast-generate-preview").
    mode: Generation mode - "text", "image", or "video" (video treated like image-seeded).
    person_generation: Person generation policy - "allow_all", "allow_adult", or "dont_allow".

Raises:
    ValueError: If person_generation value is not allowed for the given model and mode.

Note:
    - Veo 3.0 text mode: allows "allow_all"
    - Veo 3.0 image/video mode: allows "allow_adult"
    - Veo 2.0 text mode: allows "allow_all", "allow_adult", "dont_allow"
    - Veo 2.0 image/video mode: allows "allow_adult", "dont_allow"]]></docstring>
      </function>
      <function name="generate_from_text" signature="generate_from_text(prompt: str, model: str, duration_seconds: Optional[int], on_progress: Optional[Callable]) -> VideoResult" line="57">
        <docstring><![CDATA[Generate a video from a text prompt using Google's Veo models.

Creates a video from a text description using the specified Veo model. The function
handles the entire generation pipeline including job submission, progress tracking,
video download, and metadata extraction.

Args:
    prompt: Text description of the video to generate.
    model: Veo model to use for generation. Defaults to "veo-3.0-fast-generate-preview".
    duration_seconds: Desired video duration in seconds. If None, uses model default.
    on_progress: Optional callback function called with progress updates (message, percent).
    **kwargs: Additional generation parameters including:
        - person_generation: Person generation policy ("allow_all", "allow_adult", "dont_allow")
        - enhance: Whether to enhance the prompt
        - fps: Target frames per second
        - audio: Whether to generate audio

Returns:
    VideoResult: Object containing the generated video path, metadata, and operation details.

Raises:
    ValueError: If person_generation parameter is invalid for the model/mode combination.
    RuntimeError: If video generation fails or no video is returned.
    FileNotFoundError: If required files are not accessible.

Examples:
    Generate a simple video:
    >>> result = generate_from_text("A cat playing in a garden")
    >>> print(f"Video saved to: {result.path}")

    Generate with custom duration and progress tracking:
    >>> def progress_handler(message, percent):
    ...     print(f"{message}: {percent}%")
    >>> result = generate_from_text(
    ...     "A sunset over the ocean",
    ...     duration_seconds=10,
    ...     on_progress=progress_handler
    ... )]]></docstring>
      </function>
      <function name="generate_from_image" signature="generate_from_image(image_path: Path, prompt: str, model: str, on_progress: Optional[Callable]) -> VideoResult" line="188">
        <docstring><![CDATA[Generate a video from an image and text prompt using Google's Veo models.

Creates a video animation starting from a static image, guided by a text prompt.
The function loads the image, submits the generation job, tracks progress, and
downloads the resulting video with metadata extraction.

Args:
    image_path: Path to the input image file (jpg, png, etc.).
    prompt: Text description of how the image should be animated.
    model: Veo model to use for generation. Defaults to "veo-3.0-fast-generate-preview".
    on_progress: Optional callback function called with progress updates (message, percent).
    **kwargs: Additional generation parameters including:
        - person_generation: Person generation policy ("allow_adult", "dont_allow")
        - duration_seconds: Video duration in seconds
        - enhance: Whether to enhance the prompt
        - fps: Target frames per second

Returns:
    VideoResult: Object containing the generated video path, metadata, and operation details.

Raises:
    ValueError: If person_generation parameter is invalid for image mode.
    RuntimeError: If video generation fails or the API returns an error.
    FileNotFoundError: If the input image file is not found.

Examples:
    Animate a static image:
    >>> from pathlib import Path
    >>> result = generate_from_image(
    ...     Path("photo.jpg"),
    ...     "The person starts walking forward"
    ... )
    >>> print(f"Animation saved to: {result.path}")

    Generate with progress tracking:
    >>> def show_progress(msg, pct):
    ...     print(f"{msg}: {pct}%")
    >>> result = generate_from_image(
    ...     Path("landscape.png"),
    ...     "Clouds moving across the sky",
    ...     on_progress=show_progress
    ... )]]></docstring>
      </function>
      <function name="generate_from_video" signature="generate_from_video(video_path: Path, prompt: str, extract_at: float, model: str, on_progress: Optional[Callable]) -> VideoResult" line="333">
        <docstring><![CDATA[Generate a video continuation from an existing video using Google's Veo models.

Creates a new video that continues from a frame extracted from an existing video.
The function extracts a frame at the specified time offset, then uses it as the
starting point for generating a continuation guided by the text prompt.

Args:
    video_path: Path to the input video file.
    prompt: Text description of how the video should continue.
    extract_at: Time offset in seconds for frame extraction. Negative values count
        from the end (e.g., -1.0 extracts 1 second before the end). Defaults to -1.0.
    model: Veo model to use for generation. Defaults to "veo-3.0-fast-generate-preview".
    on_progress: Optional callback function called with progress updates (message, percent).
    **kwargs: Additional generation parameters including:
        - person_generation: Person generation policy ("allow_adult", "dont_allow")
        - duration_seconds: Video duration in seconds
        - enhance: Whether to enhance the prompt
        - fps: Target frames per second

Returns:
    VideoResult: Object containing the generated video path, metadata, and operation details.

Raises:
    ValueError: If person_generation parameter is invalid for video continuation mode.
    RuntimeError: If frame extraction fails or video generation fails.
    FileNotFoundError: If the input video file is not found.

Examples:
    Continue a video from the end:
    >>> result = generate_from_video(
    ...     Path("scene1.mp4"),
    ...     "The character turns around and walks away"
    ... )

    Continue from a specific timestamp:
    >>> result = generate_from_video(
    ...     Path("action.mp4"),
    ...     "The explosion gets bigger",
    ...     extract_at=5.5  # Extract at 5.5 seconds
    ... )

    Continue with progress tracking:
    >>> def track_progress(msg, pct):
    ...     print(f"Progress: {msg} ({pct}%)")
    >>> result = generate_from_video(
    ...     Path("dance.mp4"),
    ...     "The dancer spins faster",
    ...     extract_at=-2.0,
    ...     on_progress=track_progress
    ... )]]></docstring>
      </function>
      <function name="_download_video" signature="_download_video(video: types.Video, output_path: Path, client) -> Path" line="422">
        <docstring><![CDATA[Download a generated video from Google's API to local storage.

Downloads video content from either a URI or direct data blob provided by the
Google GenAI API. Handles authentication headers and writes the video to the
specified output path.

Args:
    video: Video object from Google GenAI API containing URI or data.
    output_path: Local path where the video should be saved.
    client: Google GenAI client instance (currently unused but kept for compatibility).

Returns:
    Path: The output path where the video was saved.

Raises:
    RuntimeError: If the video object contains neither URI nor data.
    requests.HTTPError: If the download request fails.
    OSError: If writing to the output path fails.

Note:
    This function requires the GEMINI_API_KEY environment variable to be set
    for URI-based downloads.]]></docstring>
      </function>
    </module>
    <module path="/Users/boat/Projects/veotools/src/veotools/models.py">
      <class name="JobStatus" line="7">
        <docstring><![CDATA[Enumeration of possible job statuses for video generation tasks.

Attributes:
    PENDING: Job has been created but not yet started.
    PROCESSING: Job is currently being processed.
    COMPLETE: Job has finished successfully.
    FAILED: Job has failed with an error.]]></docstring>
      </class>
      <class name="VideoMetadata" line="21">
        <docstring><![CDATA[Metadata information for a video file.

Attributes:
    fps: Frames per second of the video.
    duration: Duration of the video in seconds.
    width: Width of the video in pixels.
    height: Height of the video in pixels.
    frame_count: Total number of frames in the video.

Examples:
    >>> metadata = VideoMetadata(fps=30.0, duration=10.0, width=1920, height=1080)
    >>> print(metadata.frame_count)  # 300
    >>> print(metadata.to_dict())]]></docstring>
        <methods>
          <method name="__init__" signature="__init__(self, fps: float, duration: float, width: int, height: int)">
            <docstring><![CDATA[Initialize video metadata.

Args:
    fps: Frames per second (default: 24.0).
    duration: Video duration in seconds (default: 0.0).
    width: Video width in pixels (default: 0).
    height: Video height in pixels (default: 0).]]></docstring>
          </method>
          <method name="to_dict" signature="to_dict(self) -> Dict[str, Any]">
            <docstring><![CDATA[Convert metadata to a dictionary.

Returns:
    Dict[str, Any]: Dictionary containing all metadata fields.]]></docstring>
          </method>
        </methods>
      </class>
      <class name="VideoResult" line="66">
        <docstring><![CDATA[Result object for video generation operations.

This class encapsulates all information about a video generation task,
including its status, progress, metadata, and any errors.

Attributes:
    id: Unique identifier for this result.
    path: Path to the generated video file.
    url: URL to access the video (if available).
    operation_id: Google API operation ID for tracking.
    status: Current status of the generation job.
    progress: Progress percentage (0-100).
    metadata: Video metadata (fps, duration, resolution).
    prompt: Text prompt used for generation.
    model: Model used for generation.
    error: Error information if generation failed.
    created_at: Timestamp when the job was created.
    completed_at: Timestamp when the job completed.

Examples:
    >>> result = VideoResult()
    >>> result.update_progress("Generating", 50)
    >>> print(result.status)  # JobStatus.PROCESSING
    >>> result.update_progress("Complete", 100)
    >>> print(result.status)  # JobStatus.COMPLETE]]></docstring>
        <methods>
          <method name="__init__" signature="__init__(self, path: Optional[Path], operation_id: Optional[str])">
            <docstring><![CDATA[Initialize a video result.

Args:
    path: Optional path to the video file.
    operation_id: Optional Google API operation ID.]]></docstring>
          </method>
          <method name="to_dict" signature="to_dict(self) -> Dict[str, Any]">
            <docstring><![CDATA[Convert the result to a JSON-serializable dictionary.

Returns:
    Dict[str, Any]: Dictionary representation of the video result.]]></docstring>
          </method>
          <method name="update_progress" signature="update_progress(self, message: str, percent: int)">
            <docstring><![CDATA[Update the progress of the video generation.

Automatically updates the status based on progress:
- 0%: PENDING
- 1-99%: PROCESSING
- 100%: COMPLETE

Args:
    message: Progress message (currently unused but kept for API compatibility).
    percent: Progress percentage (0-100).]]></docstring>
          </method>
          <method name="mark_failed" signature="mark_failed(self, error: Exception)">
            <docstring><![CDATA[Mark the job as failed with an error.

Args:
    error: The exception that caused the failure.]]></docstring>
          </method>
        </methods>
      </class>
      <class name="WorkflowStep" line="163">
        <docstring><![CDATA[Individual step in a video processing workflow.

Attributes:
    id: Unique identifier for this step.
    action: Action to perform (e.g., "generate", "stitch").
    params: Parameters for the action.
    result: Result of executing this step.
    created_at: Timestamp when the step was created.]]></docstring>
        <methods>
          <method name="__init__" signature="__init__(self, action: str, params: Dict[str, Any])">
            <docstring><![CDATA[Initialize a workflow step.

Args:
    action: The action to perform.
    params: Parameters for the action.]]></docstring>
          </method>
          <method name="to_dict" signature="to_dict(self) -> Dict[str, Any]">
            <docstring><![CDATA[Convert the step to a dictionary.

Returns:
    Dict[str, Any]: Dictionary representation of the workflow step.]]></docstring>
          </method>
        </methods>
      </class>
      <class name="Workflow" line="200">
        <docstring><![CDATA[Container for a multi-step video processing workflow.

Workflows allow chaining multiple operations like generation,
stitching, and processing into a single managed flow.

Attributes:
    id: Unique identifier for this workflow.
    name: Human-readable name for the workflow.
    steps: List of workflow steps to execute.
    current_step: Index of the currently executing step.
    created_at: Timestamp when the workflow was created.
    updated_at: Timestamp of the last update.

Examples:
    >>> workflow = Workflow("my_video_project")
    >>> workflow.add_step("generate", {"prompt": "sunset"})
    >>> workflow.add_step("stitch", {"videos": ["a.mp4", "b.mp4"]})
    >>> print(len(workflow.steps))  # 2]]></docstring>
        <methods>
          <method name="__init__" signature="__init__(self, name: Optional[str])">
            <docstring><![CDATA[Initialize a workflow.

Args:
    name: Optional name for the workflow. If not provided,
         generates a timestamp-based name.]]></docstring>
          </method>
          <method name="add_step" signature="add_step(self, action: str, params: Dict[str, Any]) -> WorkflowStep">
            <docstring><![CDATA[Add a new step to the workflow.

Args:
    action: The action to perform.
    params: Parameters for the action.
    
Returns:
    WorkflowStep: The created workflow step.]]></docstring>
          </method>
          <method name="to_dict" signature="to_dict(self) -> Dict[str, Any]">
            <docstring><![CDATA[Convert the workflow to a dictionary.

Returns:
    Dict[str, Any]: Dictionary representation of the workflow.]]></docstring>
          </method>
          <method name="from_dict" signature="from_dict(cls, data: Dict[str, Any]) -> 'Workflow'">
            <docstring><![CDATA[Create a workflow from a dictionary.

Args:
    data: Dictionary containing workflow data.
    
Returns:
    Workflow: Reconstructed workflow instance.
    
Examples:
    >>> data = {"id": "123", "name": "test", "current_step": 2}
    >>> workflow = Workflow.from_dict(data)
    >>> print(workflow.name)  # "test"]]></docstring>
          </method>
        </methods>
      </class>
    </module>
    <module path="/Users/boat/Projects/veotools/src/veotools/process/__init__.py">
      <docstring><![CDATA[Video processing module for Veo Tools.]]></docstring>
    </module>
    <module path="/Users/boat/Projects/veotools/src/veotools/process/extractor.py">
      <docstring><![CDATA[Frame extraction and video info utilities for Veo Tools.

Enhancements:
- `get_video_info` now first attempts to use `ffprobe` for accurate metadata
  (fps, duration, width, height). If `ffprobe` is unavailable, it falls back
  to OpenCV-based probing.]]></docstring>
      <function name="extract_frame" signature="extract_frame(video_path: Path, time_offset: float, output_path: Optional[Path]) -> Path" line="18">
        <docstring><![CDATA[Extract a single frame from a video at the specified time offset.

Extracts and saves a frame from a video file as a JPEG image. Supports both
positive time offsets (from start) and negative offsets (from end). Uses
OpenCV for video processing and automatically manages storage paths.

Args:
    video_path: Path to the input video file.
    time_offset: Time in seconds where to extract the frame. Positive values
        are from the start, negative values from the end. Defaults to -1.0
        (1 second from the end).
    output_path: Optional custom path for saving the extracted frame. If None,
        auto-generates a path using StorageManager.

Returns:
    Path: The path where the extracted frame was saved.

Raises:
    FileNotFoundError: If the input video file doesn't exist.
    RuntimeError: If frame extraction fails (e.g., invalid time offset).

Examples:
    Extract the last frame:
    >>> frame_path = extract_frame(Path("video.mp4"))
    >>> print(f"Frame saved to: {frame_path}")

    Extract frame at 5 seconds:
    >>> frame_path = extract_frame(Path("video.mp4"), time_offset=5.0)

    Extract with custom output path:
    >>> custom_path = Path("my_frame.jpg")
    >>> frame_path = extract_frame(
    ...     Path("video.mp4"),
    ...     time_offset=10.0,
    ...     output_path=custom_path
    ... )]]></docstring>
      </function>
      <function name="extract_frames" signature="extract_frames(video_path: Path, times: list, output_dir: Optional[Path]) -> list" line="96">
        <docstring><![CDATA[Extract multiple frames from a video at specified time offsets.

Extracts and saves multiple frames from a video file as JPEG images. Each
time offset can be positive (from start) or negative (from end). Uses
OpenCV for efficient batch frame extraction.

Args:
    video_path: Path to the input video file.
    times: List of time offsets in seconds. Each can be positive (from start)
        or negative (from end).
    output_dir: Optional directory for saving frames. If None, uses
        StorageManager's default frame directory.

Returns:
    list: List of Path objects where the extracted frames were saved.
        Order matches the input times list.

Raises:
    FileNotFoundError: If the input video file doesn't exist.

Examples:
    Extract frames at multiple timestamps:
    >>> frame_paths = extract_frames(
    ...     Path("video.mp4"),
    ...     [0.0, 5.0, 10.0, -1.0]  # Start, 5s, 10s, and 1s from end
    ... )
    >>> print(f"Extracted {len(frame_paths)} frames")

    Extract to custom directory:
    >>> output_dir = Path("extracted_frames")
    >>> frame_paths = extract_frames(
    ...     Path("movie.mp4"),
    ...     [1.0, 2.0, 3.0],
    ...     output_dir=output_dir
    ... )

Note:
    Failed frame extractions are silently skipped. The returned list may
    contain fewer paths than input times if some extractions fail.]]></docstring>
      </function>
      <function name="get_video_info" signature="get_video_info(video_path: Path) -> dict" line="180">
        <docstring><![CDATA[Extract comprehensive metadata from a video file.

Retrieves video metadata including frame rate, duration, dimensions, and frame count.
First attempts to use ffprobe for accurate metadata extraction, falling back to
OpenCV if ffprobe is unavailable. This dual approach ensures maximum compatibility
and accuracy.

Args:
    video_path: Path to the input video file.

Returns:
    dict: Video metadata containing:
        - fps (float): Frames per second
        - frame_count (int): Total number of frames
        - width (int): Video width in pixels
        - height (int): Video height in pixels
        - duration (float): Video duration in seconds

Raises:
    FileNotFoundError: If the input video file doesn't exist.

Examples:
    Get basic video information:
    >>> info = get_video_info(Path("video.mp4"))
    >>> print(f"Duration: {info['duration']:.2f}s")
    >>> print(f"Resolution: {info['width']}x{info['height']}")
    >>> print(f"Frame rate: {info['fps']} fps")

    Check if video has expected properties:
    >>> info = get_video_info(Path("movie.mp4"))
    >>> if info['fps'] > 30:
    ...     print("High frame rate video")
    >>> if info['width'] >= 1920:
    ...     print("HD or higher resolution")

Note:
    - ffprobe (from FFmpeg) provides more accurate metadata when available
    - OpenCV fallback may have slight inaccuracies in frame rate calculation
    - All numeric values are guaranteed to be non-negative
    - Returns 0.0 for fps/duration if video properties cannot be determined]]></docstring>
      </function>
    </module>
    <module path="/Users/boat/Projects/veotools/src/veotools/server/__init__.py">
    </module>
    <module path="/Users/boat/Projects/veotools/src/veotools/server/mcp_server.py">
      <docstring><![CDATA[Built-in MCP server entry point for Veotools.

Run:
  veo-mcp            # via console script
  python -m veotools.mcp_server]]></docstring>
      <function name="preflight" signature="preflight() -> dict" line="23">
        <docstring><![CDATA[Check environment and system prerequisites.

Returns a JSON dict with: ok, gemini_api_key, ffmpeg {installed, version},
write_permissions, base_path.]]></docstring>
      </function>
      <function name="version" signature="version() -> dict" line="33">
        <docstring><![CDATA[Report package and dependency versions.

Returns keys: veotools, dependencies {google-genai, opencv-python, ...}, ffmpeg.]]></docstring>
      </function>
      <function name="list_models" signature="list_models(include_remote: bool) -> dict" line="42">
        <docstring><![CDATA[List available models with capability flags.

- include_remote: when true, merges remote discovery from the API.
Returns { models: [ {id, name, capabilities, default_duration, generation_time, source} ] }.]]></docstring>
      </function>
      <function name="cache_create_from_files" signature="cache_create_from_files(model: str, files: list[str], system_instruction: str | None) -> dict" line="52">
        <docstring><![CDATA[Create a cached content handle from local files.

Returns {name, model, contents_count}.]]></docstring>
      </function>
      <function name="cache_get" signature="cache_get(name: str) -> dict" line="61">
        <docstring><![CDATA[Get cached content metadata by name.]]></docstring>
      </function>
      <function name="cache_list" signature="cache_list() -> dict" line="67">
        <docstring><![CDATA[List cached content metadata entries.]]></docstring>
      </function>
      <function name="cache_update" signature="cache_update(name: str, ttl_seconds: int | None, expire_time_iso: str | None) -> dict" line="73">
        <docstring><![CDATA[Update TTL or expiry time for a cache.]]></docstring>
      </function>
      <function name="cache_delete" signature="cache_delete(name: str) -> dict" line="79">
        <docstring><![CDATA[Delete a cached content entry by name.]]></docstring>
      </function>
      <function name="generate_start" signature="generate_start(prompt: str, model: Optional[str], input_image_path: Optional[str], input_video_path: Optional[str], extract_at: Optional[float], options: Optional[Dict]) -> dict" line="85">
        <docstring><![CDATA[Start a video generation job.

- prompt: required text prompt
- model: e.g., "veo-3.0-fast-generate-preview"; if omitted, SDK default is used
- input_image_path: path to seed image for imagevideo
- input_video_path: path to source video for continuation
- extract_at: seconds offset for continuation (use -1.0 for last second)
- options: pass-through config, e.g. {aspect_ratio: "16:9", negative_prompt: "...",
  person_generation: "allow_all"}
Returns {job_id, status, progress, message, kind, created_at}.]]></docstring>
      </function>
      <function name="generate_cancel" signature="generate_cancel(job_id: str) -> dict" line="149">
        <docstring><![CDATA[Request cooperative cancellation for a running job.]]></docstring>
      </function>
      <function name="list_recent_videos" signature="list_recent_videos(limit: int) -> list[dict]" line="155">
        <docstring><![CDATA[List recent generated videos with URLs and metadata.

- limit: max items (default 10).
Returns array of {path, url, metadata, modified}.]]></docstring>
      </function>
      <function name="get_job" signature="get_job(job_id: str) -> dict" line="179">
        <docstring><![CDATA[Retrieve the persisted job record by id.]]></docstring>
      </function>
      <function name="main" signature="main() -> None" line="280">
      </function>
    </module>
    <module path="/Users/boat/Projects/veotools/src/veotools/stitch/__init__.py">
      <docstring><![CDATA[Video stitching module for Veo Tools.]]></docstring>
    </module>
    <module path="/Users/boat/Projects/veotools/src/veotools/stitch/seamless.py">
      <docstring><![CDATA[Seamless video stitching for Veo Tools.]]></docstring>
      <function name="stitch_videos" signature="stitch_videos(video_paths: List[Path], overlap: float, output_path: Optional[Path], on_progress: Optional[Callable]) -> VideoResult" line="12">
        <docstring><![CDATA[Seamlessly stitch multiple videos together into a single continuous video.

Combines multiple video files into one continuous video by concatenating them
with optional overlap trimming. All videos are resized to match the dimensions
of the first video. The output is optimized with H.264 encoding for broad
compatibility.

Args:
    video_paths: List of paths to video files to stitch together, in order.
    overlap: Duration in seconds to trim from the end of each video (except
        the last one) to create smooth transitions. Defaults to 1.0.
    output_path: Optional custom output path. If None, auto-generates a path
        using StorageManager.
    on_progress: Optional callback function called with progress updates (message, percent).

Returns:
    VideoResult: Object containing the stitched video path, metadata, and operation details.

Raises:
    ValueError: If no videos are provided or if fewer than 2 videos are found.
    FileNotFoundError: If any input video file doesn't exist.
    RuntimeError: If video processing fails.

Examples:
    Stitch videos with default overlap:
    >>> video_files = [Path("part1.mp4"), Path("part2.mp4"), Path("part3.mp4")]
    >>> result = stitch_videos(video_files)
    >>> print(f"Stitched video: {result.path}")

    Stitch without overlap:
    >>> result = stitch_videos(video_files, overlap=0.0)

    Stitch with progress tracking:
    >>> def show_progress(msg, pct):
    ...     print(f"Stitching: {msg} ({pct}%)")
    >>> result = stitch_videos(
    ...     video_files,
    ...     overlap=2.0,
    ...     on_progress=show_progress
    ... )

    Custom output location:
    >>> result = stitch_videos(
    ...     video_files,
    ...     output_path=Path("final_movie.mp4")
    ... )

Note:
    - Videos are resized to match the first video's dimensions
    - Uses H.264 encoding with CRF 23 for good quality/size balance
    - Automatically handles frame rate consistency
    - FFmpeg is used for final encoding if available, otherwise uses OpenCV]]></docstring>
      </function>
      <function name="stitch_with_transitions" signature="stitch_with_transitions(video_paths: List[Path], transition_videos: List[Path], output_path: Optional[Path], on_progress: Optional[Callable]) -> VideoResult" line="169">
        <docstring><![CDATA[Stitch videos together with custom transition videos between them.

Combines multiple videos by inserting transition videos between each pair
of main videos. The transitions are placed between consecutive videos to
create smooth, cinematic connections between scenes.

Args:
    video_paths: List of main video files to stitch together, in order.
    transition_videos: List of transition videos to insert between main videos.
        Must have exactly len(video_paths) - 1 transitions.
    output_path: Optional custom output path. If None, auto-generates a path
        using StorageManager.
    on_progress: Optional callback function called with progress updates (message, percent).

Returns:
    VideoResult: Object containing the final stitched video with transitions.

Raises:
    ValueError: If the number of transition videos doesn't match the requirement
        (should be one less than the number of main videos).
    FileNotFoundError: If any video file doesn't exist.

Examples:
    Add transitions between three video clips:
    >>> main_videos = [Path("scene1.mp4"), Path("scene2.mp4"), Path("scene3.mp4")]
    >>> transitions = [Path("fade1.mp4"), Path("fade2.mp4")]
    >>> result = stitch_with_transitions(main_videos, transitions)
    >>> print(f"Final video with transitions: {result.path}")

    With progress tracking:
    >>> def track_progress(msg, pct):
    ...     print(f"Processing: {msg} - {pct}%")
    >>> result = stitch_with_transitions(
    ...     main_videos,
    ...     transitions,
    ...     on_progress=track_progress
    ... )

Note:
    This function uses stitch_videos internally with overlap=0 to preserve
    transition videos exactly as provided.]]></docstring>
      </function>
      <function name="create_transition_points" signature="create_transition_points(video_a: Path, video_b: Path, extract_points: Optional[dict]) -> tuple" line="234">
        <docstring><![CDATA[Extract frames from two videos to analyze potential transition points.

Extracts representative frames from two videos that can be used to analyze
how well they might transition together. Typically extracts the ending frame
of the first video and the beginning frame of the second video.

Args:
    video_a: Path to the first video file.
    video_b: Path to the second video file.
    extract_points: Optional dictionary specifying extraction points:
        - "a_end": Time offset for frame extraction from video_a (default: -1.0)
        - "b_start": Time offset for frame extraction from video_b (default: 1.0)
        If None, uses default values.

Returns:
    tuple: A tuple containing (frame_a_path, frame_b_path) where:
        - frame_a_path: Path to extracted frame from video_a
        - frame_b_path: Path to extracted frame from video_b

Raises:
    FileNotFoundError: If either video file doesn't exist.
    RuntimeError: If frame extraction fails for either video.

Examples:
    Extract transition frames with defaults:
    >>> frame_a, frame_b = create_transition_points(
    ...     Path("clip1.mp4"),
    ...     Path("clip2.mp4")
    ... )
    >>> print(f"Transition frames: {frame_a}, {frame_b}")

    Custom extraction points:
    >>> points = {"a_end": -2.0, "b_start": 0.5}
    >>> frame_a, frame_b = create_transition_points(
    ...     Path("scene1.mp4"),
    ...     Path("scene2.mp4"),
    ...     extract_points=points
    ... )

Note:
    - Default extracts 1 second before the end of video_a
    - Default extracts 1 second after the start of video_b
    - Negative values in extract_points count from the end of the video
    - These frames can be used to analyze color, composition, or content
      similarity for better transition planning]]></docstring>
      </function>
    </module>
  </modules>
</project>