{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Veotools Documentation","text":"<p>Welcome to the Veotools documentation! Veotools is a Python SDK and MCP server for generating and extending videos with Google Veo.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>\ud83c\udfac Video Generation - Generate videos from text prompts, images, or existing videos</li> <li>\ud83d\udd17 Seamless Stitching - Combine videos with automatic overlap trimming</li> <li>\ud83e\udd16 MCP Integration - Built-in Model Context Protocol server for AI assistants</li> <li>\ud83e\udde0 Gemini Scene Planning - Structured multi-clip story plans with character consistency</li> <li>\ud83d\udee0\ufe0f Plan Execution - Render planned clips and stitch them into a finished video</li> <li>\ud83d\udcca Progress Tracking - Real-time progress updates for long-running operations</li> <li>\ud83d\udcbe Smart Caching - Context caching for improved performance</li> <li>\ud83d\udee1\ufe0f Safety Controls - Built-in safety settings pass-through</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>veo plan-run --idea \"Retro travel vlog\" --execute-model veo-3.0-generate-001 --seed-last-frame\n</code></pre> <pre><code>import veotools as veo\n\nveo.init()\n\nplan = veo.generate_scene_plan(\"Retro travel vlog\", number_of_scenes=4)\nresult = veo.execute_scene_plan(plan, model=\"veo-3.0-generate-001\", auto_seed_last_frame=True)\n\nprint(\"Final video:\", result.final_result.path if result.final_result else \"clips only\")\n</code></pre>"},{"location":"#installation","title":"Installation","text":"<pre><code># Basic installation\npip install veotools\n\n# With MCP server support\npip install \"veotools[mcp]\"\n\n# For development\npip install -e \".[dev,mcp,docs]\"\n</code></pre>"},{"location":"#navigation","title":"Navigation","text":"<ul> <li>API Reference - Complete API documentation with auto-generated docs from code</li> </ul>"},{"location":"#requirements","title":"Requirements","text":"<ul> <li>Python 3.10+</li> <li>Google Gemini API key</li> <li>ffmpeg (for video processing)</li> <li>OpenCV (installed automatically)</li> </ul>"},{"location":"#license","title":"License","text":"<p>MIT License - see LICENSE for details.</p>"},{"location":"llm/","title":"LLM Documentation","text":"<p>This page provides machine-readable documentation for Large Language Models (LLMs) to understand the Veotools codebase.</p>"},{"location":"llm/#available-formats","title":"Available Formats","text":"<ul> <li>Markdown/Text Format - Plain text markdown format (llm.txt standard)</li> <li>XML Format - Structured XML format for parsing</li> </ul>"},{"location":"llm/#about","title":"About","text":"<p>These files are automatically generated from the source code docstrings during the documentation build process. They contain:</p> <ul> <li>Complete API documentation</li> <li>All function and class signatures  </li> <li>Comprehensive docstrings</li> <li>Module structure and organization</li> </ul>"},{"location":"llm/#usage","title":"Usage","text":""},{"location":"llm/#for-llm-providers","title":"For LLM Providers","text":"<p>Add the following to your LLM's context:</p> <pre><code>https://your-docs-site.com/llm.txt\n</code></pre>"},{"location":"llm/#for-developers","title":"For Developers","text":"<p>When asking an LLM about this codebase, you can reference:</p> <pre><code>Please read the documentation at https://your-docs-site.com/llm.txt to understand the Veotools API\n</code></pre>"},{"location":"llm/#auto-generation","title":"Auto-Generation","text":"<p>These files are regenerated automatically whenever the documentation is built, ensuring they always reflect the current state of the codebase.</p> <p>Last generated: 2025-10-04T17:09:54.060594</p>"},{"location":"api/overview/","title":"API Reference Overview","text":"<p>The Veotools API is organized into several modules, each handling specific aspects of video generation and processing.</p>"},{"location":"api/overview/#complete-api-documentation","title":"Complete API Documentation","text":"<p>The following sections provide auto-generated documentation from the source code docstrings.</p>"},{"location":"api/overview/#core-module","title":"Core Module","text":""},{"location":"api/overview/#veotools.core","title":"veotools.core","text":"CLASS DESCRIPTION <code>VeoClient</code> <p>Singleton client for Google GenAI API interactions.</p> <code>StorageManager</code> <code>ProgressTracker</code> <p>Track and report progress for long-running operations.</p> <code>ModelConfig</code> <p>Configuration and capabilities for different Veo video generation models.</p>"},{"location":"api/overview/#veotools.core-classes","title":"Classes","text":""},{"location":"api/overview/#veotools.core.VeoClient","title":"VeoClient","text":"<pre><code>VeoClient()\n</code></pre> <p>Singleton client for Google GenAI API interactions.</p> <p>This class implements a singleton pattern to ensure only one client instance is created throughout the application lifecycle. It manages the authentication and connection to Google's Generative AI API.</p> ATTRIBUTE DESCRIPTION <code>client</code> <p>The underlying Google GenAI client instance.</p> <p> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If GEMINI_API_KEY environment variable is not set.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; client = VeoClient()\n&gt;&gt;&gt; api_client = client.client\n&gt;&gt;&gt; # Use api_client for API calls\n</code></pre> <p>Initialize the GenAI client with API key from environment.</p> <p>The client is only initialized once, even if init is called multiple times.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If GEMINI_API_KEY is not found in environment variables.</p> METHOD DESCRIPTION <code>__new__</code> <p>Create or return the singleton instance.</p> Source code in <code>src\\veotools\\core.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the GenAI client with API key from environment.\n\n    The client is only initialized once, even if __init__ is called multiple times.\n\n    Raises:\n        ValueError: If GEMINI_API_KEY is not found in environment variables.\n    \"\"\"\n    cls = type(self)\n    if cls._client is None:\n        provider = (os.getenv(\"VEO_PROVIDER\", \"google\") or \"google\").strip().lower()\n        cls._provider = provider\n\n        if provider == \"daydreams\":\n            api_key = os.getenv(\"DAYDREAMS_API_KEY\")\n            if not api_key:\n                raise ValueError(\"DAYDREAMS_API_KEY not found in environment\")\n            base_url = os.getenv(\"DAYDREAMS_BASE_URL\")\n            cls._client = DaydreamsRouterClient(api_key=api_key, base_url=base_url)\n        else:\n            api_key = os.getenv('GEMINI_API_KEY')\n            if not api_key:\n                raise ValueError(\"GEMINI_API_KEY not found in .env file\")\n            cls._provider = \"google\"\n            cls._client = genai.Client(api_key=api_key)\n</code></pre>"},{"location":"api/overview/#veotools.core.VeoClient-attributes","title":"Attributes","text":""},{"location":"api/overview/#veotools.core.VeoClient.client","title":"client  <code>property</code>","text":"<pre><code>client\n</code></pre> <p>Get the Google GenAI client instance.</p> RETURNS DESCRIPTION <p>genai.Client: The initialized GenAI client.</p>"},{"location":"api/overview/#veotools.core.VeoClient.provider","title":"provider  <code>property</code>","text":"<pre><code>provider: str\n</code></pre> <p>Return the active provider identifier (google or daydreams).</p>"},{"location":"api/overview/#veotools.core.VeoClient-functions","title":"Functions","text":""},{"location":"api/overview/#veotools.core.VeoClient.__new__","title":"__new__","text":"<pre><code>__new__()\n</code></pre> <p>Create or return the singleton instance.</p> RETURNS DESCRIPTION <code>VeoClient</code> <p>The singleton VeoClient instance.</p> Source code in <code>src\\veotools\\core.py</code> <pre><code>def __new__(cls):\n    \"\"\"Create or return the singleton instance.\n\n    Returns:\n        VeoClient: The singleton VeoClient instance.\n    \"\"\"\n    if cls._instance is None:\n        cls._instance = super().__new__(cls)\n    return cls._instance\n</code></pre>"},{"location":"api/overview/#veotools.core.StorageManager","title":"StorageManager","text":"<pre><code>StorageManager(base_path: Optional[str] = None)\n</code></pre> <p>Manage output directories for videos, frames, and temp files.</p> <p>Default resolution order for base path: 1. VEO_OUTPUT_DIR environment variable (if set) 2. Current working directory (./output) 3. Package-adjacent directory (../output) as a last resort</p> METHOD DESCRIPTION <code>get_video_path</code> <p>Get the full path for a video file.</p> <code>get_frame_path</code> <p>Get the full path for a frame image file.</p> <code>get_temp_path</code> <p>Get the full path for a temporary file.</p> <code>cleanup_temp</code> <p>Remove all files from the temporary directory.</p> <code>get_url</code> <p>Convert a file path to a file:// URL.</p> Source code in <code>src\\veotools\\core.py</code> <pre><code>def __init__(self, base_path: Optional[str] = None):\n    \"\"\"Manage output directories for videos, frames, and temp files.\n\n    Default resolution order for base path:\n    1. VEO_OUTPUT_DIR environment variable (if set)\n    2. Current working directory (./output)\n    3. Package-adjacent directory (../output) as a last resort\n    \"\"\"\n    resolved_base: Path\n\n    # 1) Environment override\n    env_base = os.getenv(\"VEO_OUTPUT_DIR\")\n    if base_path:\n        resolved_base = Path(base_path)\n    elif env_base:\n        resolved_base = Path(env_base)\n    else:\n        # 2) Prefer CWD/output for installed packages (CLI/scripts)\n        cwd_candidate = Path.cwd() / \"output\"\n        try:\n            cwd_candidate.mkdir(parents=True, exist_ok=True)\n            resolved_base = cwd_candidate\n        except Exception:\n            # 3) As a last resort, place beside the installed package\n            try:\n                package_root = Path(__file__).resolve().parents[2]\n                candidate = package_root / \"output\"\n                candidate.mkdir(parents=True, exist_ok=True)\n                resolved_base = candidate\n            except Exception:\n                # Final fallback: user home\n                resolved_base = Path.home() / \"output\"\n\n    self.base_path = resolved_base\n    self.base_path.mkdir(parents=True, exist_ok=True)\n\n    self.videos_dir = self.base_path / \"videos\"\n    self.frames_dir = self.base_path / \"frames\"\n    self.temp_dir = self.base_path / \"temp\"\n\n    for dir_path in [self.videos_dir, self.frames_dir, self.temp_dir]:\n        dir_path.mkdir(parents=True, exist_ok=True)\n</code></pre>"},{"location":"api/overview/#veotools.core.StorageManager-functions","title":"Functions","text":""},{"location":"api/overview/#veotools.core.StorageManager.get_video_path","title":"get_video_path","text":"<pre><code>get_video_path(filename: str) -&gt; Path\n</code></pre> <p>Get the full path for a video file.</p> PARAMETER DESCRIPTION <code>filename</code> <p>Name of the video file.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Path</code> <p>Full path to the video file in the videos directory.</p> <p> TYPE: <code>Path</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; manager = StorageManager()\n&gt;&gt;&gt; path = manager.get_video_path(\"output.mp4\")\n&gt;&gt;&gt; print(path)  # /path/to/output/videos/output.mp4\n</code></pre> Source code in <code>src\\veotools\\core.py</code> <pre><code>def get_video_path(self, filename: str) -&gt; Path:\n    \"\"\"Get the full path for a video file.\n\n    Args:\n        filename: Name of the video file.\n\n    Returns:\n        Path: Full path to the video file in the videos directory.\n\n    Examples:\n        &gt;&gt;&gt; manager = StorageManager()\n        &gt;&gt;&gt; path = manager.get_video_path(\"output.mp4\")\n        &gt;&gt;&gt; print(path)  # /path/to/output/videos/output.mp4\n    \"\"\"\n    return self.videos_dir / filename\n</code></pre>"},{"location":"api/overview/#veotools.core.StorageManager.get_frame_path","title":"get_frame_path","text":"<pre><code>get_frame_path(filename: str) -&gt; Path\n</code></pre> <p>Get the full path for a frame image file.</p> PARAMETER DESCRIPTION <code>filename</code> <p>Name of the frame file.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Path</code> <p>Full path to the frame file in the frames directory.</p> <p> TYPE: <code>Path</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; manager = StorageManager()\n&gt;&gt;&gt; path = manager.get_frame_path(\"frame_001.jpg\")\n&gt;&gt;&gt; print(path)  # /path/to/output/frames/frame_001.jpg\n</code></pre> Source code in <code>src\\veotools\\core.py</code> <pre><code>def get_frame_path(self, filename: str) -&gt; Path:\n    \"\"\"Get the full path for a frame image file.\n\n    Args:\n        filename: Name of the frame file.\n\n    Returns:\n        Path: Full path to the frame file in the frames directory.\n\n    Examples:\n        &gt;&gt;&gt; manager = StorageManager()\n        &gt;&gt;&gt; path = manager.get_frame_path(\"frame_001.jpg\")\n        &gt;&gt;&gt; print(path)  # /path/to/output/frames/frame_001.jpg\n    \"\"\"\n    return self.frames_dir / filename\n</code></pre>"},{"location":"api/overview/#veotools.core.StorageManager.get_temp_path","title":"get_temp_path","text":"<pre><code>get_temp_path(filename: str) -&gt; Path\n</code></pre> <p>Get the full path for a temporary file.</p> PARAMETER DESCRIPTION <code>filename</code> <p>Name of the temporary file.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Path</code> <p>Full path to the file in the temp directory.</p> <p> TYPE: <code>Path</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; manager = StorageManager()\n&gt;&gt;&gt; path = manager.get_temp_path(\"processing.tmp\")\n&gt;&gt;&gt; print(path)  # /path/to/output/temp/processing.tmp\n</code></pre> Source code in <code>src\\veotools\\core.py</code> <pre><code>def get_temp_path(self, filename: str) -&gt; Path:\n    \"\"\"Get the full path for a temporary file.\n\n    Args:\n        filename: Name of the temporary file.\n\n    Returns:\n        Path: Full path to the file in the temp directory.\n\n    Examples:\n        &gt;&gt;&gt; manager = StorageManager()\n        &gt;&gt;&gt; path = manager.get_temp_path(\"processing.tmp\")\n        &gt;&gt;&gt; print(path)  # /path/to/output/temp/processing.tmp\n    \"\"\"\n    return self.temp_dir / filename\n</code></pre>"},{"location":"api/overview/#veotools.core.StorageManager.cleanup_temp","title":"cleanup_temp","text":"<pre><code>cleanup_temp()\n</code></pre> <p>Remove all files from the temporary directory.</p> <p>This method safely removes all files in the temp directory while preserving the directory structure. Errors during deletion are silently ignored.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; manager = StorageManager()\n&gt;&gt;&gt; manager.cleanup_temp()\n&gt;&gt;&gt; # All temp files are now deleted\n</code></pre> Source code in <code>src\\veotools\\core.py</code> <pre><code>def cleanup_temp(self):\n    \"\"\"Remove all files from the temporary directory.\n\n    This method safely removes all files in the temp directory while preserving\n    the directory structure. Errors during deletion are silently ignored.\n\n    Examples:\n        &gt;&gt;&gt; manager = StorageManager()\n        &gt;&gt;&gt; manager.cleanup_temp()\n        &gt;&gt;&gt; # All temp files are now deleted\n    \"\"\"\n    for file in self.temp_dir.glob(\"*\"):\n        try:\n            file.unlink()\n        except:\n            pass\n</code></pre>"},{"location":"api/overview/#veotools.core.StorageManager.get_url","title":"get_url","text":"<pre><code>get_url(path: Path) -&gt; Optional[str]\n</code></pre> <p>Convert a file path to a file:// URL.</p> PARAMETER DESCRIPTION <code>path</code> <p>Path to the file.</p> <p> TYPE: <code>Path</code> </p> RETURNS DESCRIPTION <code>Optional[str]</code> <p>Optional[str]: File URL if the file exists, None otherwise.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; manager = StorageManager()\n&gt;&gt;&gt; video_path = manager.get_video_path(\"test.mp4\")\n&gt;&gt;&gt; url = manager.get_url(video_path)\n&gt;&gt;&gt; print(url)  # file:///absolute/path/to/output/videos/test.mp4\n</code></pre> Source code in <code>src\\veotools\\core.py</code> <pre><code>def get_url(self, path: Path) -&gt; Optional[str]:\n    \"\"\"Convert a file path to a file:// URL.\n\n    Args:\n        path: Path to the file.\n\n    Returns:\n        Optional[str]: File URL if the file exists, None otherwise.\n\n    Examples:\n        &gt;&gt;&gt; manager = StorageManager()\n        &gt;&gt;&gt; video_path = manager.get_video_path(\"test.mp4\")\n        &gt;&gt;&gt; url = manager.get_url(video_path)\n        &gt;&gt;&gt; print(url)  # file:///absolute/path/to/output/videos/test.mp4\n    \"\"\"\n    if path.exists():\n        return f\"file://{path.absolute()}\"\n    return None\n</code></pre>"},{"location":"api/overview/#veotools.core.ProgressTracker","title":"ProgressTracker","text":"<pre><code>ProgressTracker(callback: Optional[Callable] = None)\n</code></pre> <p>Track and report progress for long-running operations.</p> <p>This class provides a simple interface for tracking progress updates during video generation and processing operations. It supports custom callbacks or falls back to logging.</p> ATTRIBUTE DESCRIPTION <code>callback</code> <p>Function to call with progress updates.</p> <p> </p> <code>current_progress</code> <p>Current progress percentage (0-100).</p> <p> </p> <code>logger</code> <p>Logger instance for default progress reporting.</p> <p> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; def my_callback(msg: str, pct: int):\n...     print(f\"{msg}: {pct}%\")\n&gt;&gt;&gt; tracker = ProgressTracker(callback=my_callback)\n&gt;&gt;&gt; tracker.start(\"Processing\")\n&gt;&gt;&gt; tracker.update(\"Halfway\", 50)\n&gt;&gt;&gt; tracker.complete(\"Done\")\n</code></pre> <p>Initialize the progress tracker.</p> PARAMETER DESCRIPTION <code>callback</code> <p>Optional callback function that receives (message, percent).      If not provided, uses default logging.</p> <p> TYPE: <code>Optional[Callable]</code> DEFAULT: <code>None</code> </p> METHOD DESCRIPTION <code>default_progress</code> <p>Default progress callback that logs to the logger.</p> <code>update</code> <p>Update progress and trigger callback.</p> <code>start</code> <p>Mark the start of an operation (0% progress).</p> <code>complete</code> <p>Mark the completion of an operation (100% progress).</p> Source code in <code>src\\veotools\\core.py</code> <pre><code>def __init__(self, callback: Optional[Callable] = None):\n    \"\"\"Initialize the progress tracker.\n\n    Args:\n        callback: Optional callback function that receives (message, percent).\n                 If not provided, uses default logging.\n    \"\"\"\n    self.callback = callback or self.default_progress\n    self.current_progress = 0\n    self.logger = logging.getLogger(__name__)\n</code></pre>"},{"location":"api/overview/#veotools.core.ProgressTracker-functions","title":"Functions","text":""},{"location":"api/overview/#veotools.core.ProgressTracker.default_progress","title":"default_progress","text":"<pre><code>default_progress(message: str, percent: int)\n</code></pre> <p>Default progress callback that logs to the logger.</p> PARAMETER DESCRIPTION <code>message</code> <p>Progress message.</p> <p> TYPE: <code>str</code> </p> <code>percent</code> <p>Progress percentage.</p> <p> TYPE: <code>int</code> </p> Source code in <code>src\\veotools\\core.py</code> <pre><code>def default_progress(self, message: str, percent: int):\n    \"\"\"Default progress callback that logs to the logger.\n\n    Args:\n        message: Progress message.\n        percent: Progress percentage.\n    \"\"\"\n    self.logger.info(f\"{message}: {percent}%\")\n</code></pre>"},{"location":"api/overview/#veotools.core.ProgressTracker.update","title":"update","text":"<pre><code>update(message: str, percent: int)\n</code></pre> <p>Update progress and trigger callback.</p> PARAMETER DESCRIPTION <code>message</code> <p>Progress message to display.</p> <p> TYPE: <code>str</code> </p> <code>percent</code> <p>Current progress percentage (0-100).</p> <p> TYPE: <code>int</code> </p> Source code in <code>src\\veotools\\core.py</code> <pre><code>def update(self, message: str, percent: int):\n    \"\"\"Update progress and trigger callback.\n\n    Args:\n        message: Progress message to display.\n        percent: Current progress percentage (0-100).\n    \"\"\"\n    self.current_progress = percent\n    self.callback(message, percent)\n</code></pre>"},{"location":"api/overview/#veotools.core.ProgressTracker.start","title":"start","text":"<pre><code>start(message: str = 'Starting')\n</code></pre> <p>Mark the start of an operation (0% progress).</p> PARAMETER DESCRIPTION <code>message</code> <p>Starting message, defaults to \"Starting\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>'Starting'</code> </p> Source code in <code>src\\veotools\\core.py</code> <pre><code>def start(self, message: str = \"Starting\"):\n    \"\"\"Mark the start of an operation (0% progress).\n\n    Args:\n        message: Starting message, defaults to \"Starting\".\n    \"\"\"\n    self.update(message, 0)\n</code></pre>"},{"location":"api/overview/#veotools.core.ProgressTracker.complete","title":"complete","text":"<pre><code>complete(message: str = 'Complete')\n</code></pre> <p>Mark the completion of an operation (100% progress).</p> PARAMETER DESCRIPTION <code>message</code> <p>Completion message, defaults to \"Complete\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>'Complete'</code> </p> Source code in <code>src\\veotools\\core.py</code> <pre><code>def complete(self, message: str = \"Complete\"):\n    \"\"\"Mark the completion of an operation (100% progress).\n\n    Args:\n        message: Completion message, defaults to \"Complete\".\n    \"\"\"\n    self.update(message, 100)\n</code></pre>"},{"location":"api/overview/#veotools.core.ModelConfig","title":"ModelConfig","text":"<p>Configuration and capabilities for different Veo video generation models.</p> METHOD DESCRIPTION <code>build_generation_config</code> <p>Build a generation configuration based on model capabilities.</p>"},{"location":"api/overview/#veotools.core.ModelConfig-functions","title":"Functions","text":""},{"location":"api/overview/#veotools.core.ModelConfig.build_generation_config","title":"build_generation_config  <code>classmethod</code>","text":"<pre><code>build_generation_config(model: str, **kwargs) -&gt; GenerateVideosConfig\n</code></pre> <p>Build a generation configuration based on model capabilities.</p> Source code in <code>src\\veotools\\core.py</code> <pre><code>@classmethod\ndef build_generation_config(cls, model: str, **kwargs) -&gt; types.GenerateVideosConfig:\n    \"\"\"Build a generation configuration based on model capabilities.\"\"\"\n\n    normalized = cls.normalize_model(model)\n    config = cls.get_config(normalized)\n\n    params = {\n        \"number_of_videos\": kwargs.get(\"number_of_videos\", 1),\n    }\n\n    if config[\"supports_duration\"] and \"duration_seconds\" in kwargs:\n        params[\"duration_seconds\"] = kwargs[\"duration_seconds\"]\n\n    if config[\"supports_enhance\"]:\n        params[\"enhance_prompt\"] = kwargs.get(\"enhance_prompt\", False)\n\n    if config[\"supports_fps\"] and \"fps\" in kwargs:\n        params[\"fps\"] = kwargs[\"fps\"]\n\n    if config.get(\"supports_aspect_ratio\") and kwargs.get(\"aspect_ratio\"):\n        ar = str(kwargs[\"aspect_ratio\"])\n        if normalized.startswith(\"veo-3.0\"):\n            allowed = {\"16:9\"}\n        elif normalized.startswith(\"veo-2.0\"):\n            allowed = {\"16:9\", \"9:16\"}\n        else:\n            allowed = {\"16:9\"}\n        if ar not in allowed:\n            raise ValueError(\n                f\"aspect_ratio '{ar}' not supported for model '{normalized}'. Allowed: {sorted(allowed)}\"\n            )\n        params[\"aspect_ratio\"] = ar\n\n    if kwargs.get(\"negative_prompt\"):\n        params[\"negative_prompt\"] = kwargs[\"negative_prompt\"]\n\n    if kwargs.get(\"person_generation\"):\n        params[\"person_generation\"] = kwargs[\"person_generation\"]\n\n    safety_settings = kwargs.get(\"safety_settings\")\n    if safety_settings:\n        normalized_settings: list = []\n        for item in safety_settings:\n            try:\n                if hasattr(item, \"category\") and hasattr(item, \"threshold\"):\n                    normalized_settings.append(item)\n                elif isinstance(item, dict):\n                    normalized_settings.append(\n                        types.SafetySetting(\n                            category=item.get(\"category\"),\n                            threshold=item.get(\"threshold\"),\n                        )\n                    )\n            except Exception:\n                continue\n        if normalized_settings:\n            params[\"safety_settings\"] = normalized_settings\n\n    if kwargs.get(\"cached_content\"):\n        params[\"cached_content\"] = kwargs[\"cached_content\"]\n\n    try:\n        return types.GenerateVideosConfig(**params)\n    except TypeError:\n        for optional_key in [\"safety_settings\", \"cached_content\"]:\n            params.pop(optional_key, None)\n        return types.GenerateVideosConfig(**params)\n</code></pre>"},{"location":"api/overview/#models-module","title":"Models Module","text":""},{"location":"api/overview/#veotools.models","title":"veotools.models","text":"CLASS DESCRIPTION <code>JobStatus</code> <p>Enumeration of possible job statuses for video generation tasks.</p> <code>VideoMetadata</code> <p>Metadata information for a video file.</p> <code>VideoResult</code> <p>Result object for video generation operations.</p> <code>WorkflowStep</code> <p>Individual step in a video processing workflow.</p> <code>Workflow</code> <p>Container for a multi-step video processing workflow.</p>"},{"location":"api/overview/#veotools.models-classes","title":"Classes","text":""},{"location":"api/overview/#veotools.models.JobStatus","title":"JobStatus","text":"<p>               Bases: <code>Enum</code></p> <p>Enumeration of possible job statuses for video generation tasks.</p> ATTRIBUTE DESCRIPTION <code>PENDING</code> <p>Job has been created but not yet started.</p> <p> </p> <code>PROCESSING</code> <p>Job is currently being processed.</p> <p> </p> <code>COMPLETE</code> <p>Job has finished successfully.</p> <p> </p> <code>FAILED</code> <p>Job has failed with an error.</p> <p> </p>"},{"location":"api/overview/#veotools.models.VideoMetadata","title":"VideoMetadata","text":"<pre><code>VideoMetadata(fps: float = 24.0, duration: float = 0.0, width: int = 0, height: int = 0)\n</code></pre> <p>Metadata information for a video file.</p> ATTRIBUTE DESCRIPTION <code>fps</code> <p>Frames per second of the video.</p> <p> </p> <code>duration</code> <p>Duration of the video in seconds.</p> <p> </p> <code>width</code> <p>Width of the video in pixels.</p> <p> </p> <code>height</code> <p>Height of the video in pixels.</p> <p> </p> <code>frame_count</code> <p>Total number of frames in the video.</p> <p> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; metadata = VideoMetadata(fps=30.0, duration=10.0, width=1920, height=1080)\n&gt;&gt;&gt; print(metadata.frame_count)  # 300\n&gt;&gt;&gt; print(metadata.to_dict())\n</code></pre> <p>Initialize video metadata.</p> PARAMETER DESCRIPTION <code>fps</code> <p>Frames per second (default: 24.0).</p> <p> TYPE: <code>float</code> DEFAULT: <code>24.0</code> </p> <code>duration</code> <p>Video duration in seconds (default: 0.0).</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>width</code> <p>Video width in pixels (default: 0).</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>height</code> <p>Video height in pixels (default: 0).</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> METHOD DESCRIPTION <code>to_dict</code> <p>Convert metadata to a dictionary.</p> Source code in <code>src\\veotools\\models.py</code> <pre><code>def __init__(self, fps: float = 24.0, duration: float = 0.0, \n             width: int = 0, height: int = 0):\n    \"\"\"Initialize video metadata.\n\n    Args:\n        fps: Frames per second (default: 24.0).\n        duration: Video duration in seconds (default: 0.0).\n        width: Video width in pixels (default: 0).\n        height: Video height in pixels (default: 0).\n    \"\"\"\n    self.fps = fps\n    self.duration = duration\n    self.width = width\n    self.height = height\n    self.frame_count = int(fps * duration) if duration &gt; 0 else 0\n</code></pre>"},{"location":"api/overview/#veotools.models.VideoMetadata-functions","title":"Functions","text":""},{"location":"api/overview/#veotools.models.VideoMetadata.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert metadata to a dictionary.</p> RETURNS DESCRIPTION <code>Dict[str, Any]</code> <p>Dict[str, Any]: Dictionary containing all metadata fields.</p> Source code in <code>src\\veotools\\models.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"Convert metadata to a dictionary.\n\n    Returns:\n        Dict[str, Any]: Dictionary containing all metadata fields.\n    \"\"\"\n    return {\n        \"fps\": self.fps,\n        \"duration\": self.duration,\n        \"width\": self.width,\n        \"height\": self.height,\n        \"frame_count\": self.frame_count\n    }\n</code></pre>"},{"location":"api/overview/#veotools.models.VideoResult","title":"VideoResult","text":"<pre><code>VideoResult(path: Optional[Path] = None, operation_id: Optional[str] = None)\n</code></pre> <p>Result object for video generation operations.</p> <p>This class encapsulates all information about a video generation task, including its status, progress, metadata, and any errors.</p> ATTRIBUTE DESCRIPTION <code>id</code> <p>Unique identifier for this result.</p> <p> </p> <code>path</code> <p>Path to the generated video file.</p> <p> </p> <code>url</code> <p>URL to access the video (if available).</p> <p> </p> <code>operation_id</code> <p>Google API operation ID for tracking.</p> <p> </p> <code>status</code> <p>Current status of the generation job.</p> <p> </p> <code>progress</code> <p>Progress percentage (0-100).</p> <p> </p> <code>metadata</code> <p>Video metadata (fps, duration, resolution).</p> <p> </p> <code>prompt</code> <p>Text prompt used for generation.</p> <p> </p> <code>model</code> <p>Model used for generation.</p> <p> </p> <code>error</code> <p>Error information if generation failed.</p> <p> </p> <code>created_at</code> <p>Timestamp when the job was created.</p> <p> </p> <code>completed_at</code> <p>Timestamp when the job completed.</p> <p> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; result = VideoResult()\n&gt;&gt;&gt; result.update_progress(\"Generating\", 50)\n&gt;&gt;&gt; print(result.status)  # JobStatus.PROCESSING\n&gt;&gt;&gt; result.update_progress(\"Complete\", 100)\n&gt;&gt;&gt; print(result.status)  # JobStatus.COMPLETE\n</code></pre> <p>Initialize a video result.</p> PARAMETER DESCRIPTION <code>path</code> <p>Optional path to the video file.</p> <p> TYPE: <code>Optional[Path]</code> DEFAULT: <code>None</code> </p> <code>operation_id</code> <p>Optional Google API operation ID.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> METHOD DESCRIPTION <code>to_dict</code> <p>Convert the result to a JSON-serializable dictionary.</p> <code>update_progress</code> <p>Update the progress of the video generation.</p> <code>mark_failed</code> <p>Mark the job as failed with an error.</p> Source code in <code>src\\veotools\\models.py</code> <pre><code>def __init__(self, path: Optional[Path] = None, operation_id: Optional[str] = None):\n    \"\"\"Initialize a video result.\n\n    Args:\n        path: Optional path to the video file.\n        operation_id: Optional Google API operation ID.\n    \"\"\"\n    self.id = str(uuid4())\n    self.path = path\n    self.url = None\n    self.operation_id = operation_id\n    self.status = JobStatus.PENDING\n    self.progress = 0\n    self.metadata = VideoMetadata()\n    self.prompt = None\n    self.model = None\n    self.error = None\n    self.created_at = datetime.now()\n    self.completed_at = None\n</code></pre>"},{"location":"api/overview/#veotools.models.VideoResult-functions","title":"Functions","text":""},{"location":"api/overview/#veotools.models.VideoResult.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert the result to a JSON-serializable dictionary.</p> RETURNS DESCRIPTION <code>Dict[str, Any]</code> <p>Dict[str, Any]: Dictionary representation of the video result.</p> Source code in <code>src\\veotools\\models.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"Convert the result to a JSON-serializable dictionary.\n\n    Returns:\n        Dict[str, Any]: Dictionary representation of the video result.\n    \"\"\"\n    return {\n        \"id\": self.id,\n        \"path\": str(self.path) if self.path else None,\n        \"url\": self.url,\n        \"operation_id\": self.operation_id,\n        \"status\": self.status.value,\n        \"progress\": self.progress,\n        \"metadata\": self.metadata.to_dict(),\n        \"prompt\": self.prompt,\n        \"model\": self.model,\n        \"error\": str(self.error) if self.error else None,\n        \"created_at\": self.created_at.isoformat(),\n        \"completed_at\": self.completed_at.isoformat() if self.completed_at else None\n    }\n</code></pre>"},{"location":"api/overview/#veotools.models.VideoResult.update_progress","title":"update_progress","text":"<pre><code>update_progress(message: str, percent: int)\n</code></pre> <p>Update the progress of the video generation.</p> <p>Automatically updates the status based on progress: - 0%: PENDING - 1-99%: PROCESSING - 100%: COMPLETE</p> PARAMETER DESCRIPTION <code>message</code> <p>Progress message (currently unused but kept for API compatibility).</p> <p> TYPE: <code>str</code> </p> <code>percent</code> <p>Progress percentage (0-100).</p> <p> TYPE: <code>int</code> </p> Source code in <code>src\\veotools\\models.py</code> <pre><code>def update_progress(self, message: str, percent: int):\n    \"\"\"Update the progress of the video generation.\n\n    Automatically updates the status based on progress:\n    - 0%: PENDING\n    - 1-99%: PROCESSING\n    - 100%: COMPLETE\n\n    Args:\n        message: Progress message (currently unused but kept for API compatibility).\n        percent: Progress percentage (0-100).\n    \"\"\"\n    self.progress = percent\n    if percent &gt;= 100:\n        self.status = JobStatus.COMPLETE\n        self.completed_at = datetime.now()\n    elif percent &gt; 0:\n        self.status = JobStatus.PROCESSING\n</code></pre>"},{"location":"api/overview/#veotools.models.VideoResult.mark_failed","title":"mark_failed","text":"<pre><code>mark_failed(error: Exception)\n</code></pre> <p>Mark the job as failed with an error.</p> PARAMETER DESCRIPTION <code>error</code> <p>The exception that caused the failure.</p> <p> TYPE: <code>Exception</code> </p> Source code in <code>src\\veotools\\models.py</code> <pre><code>def mark_failed(self, error: Exception):\n    \"\"\"Mark the job as failed with an error.\n\n    Args:\n        error: The exception that caused the failure.\n    \"\"\"\n    self.status = JobStatus.FAILED\n    self.error = error\n    self.completed_at = datetime.now()\n</code></pre>"},{"location":"api/overview/#veotools.models.WorkflowStep","title":"WorkflowStep","text":"<pre><code>WorkflowStep(action: str, params: Dict[str, Any])\n</code></pre> <p>Individual step in a video processing workflow.</p> ATTRIBUTE DESCRIPTION <code>id</code> <p>Unique identifier for this step.</p> <p> </p> <code>action</code> <p>Action to perform (e.g., \"generate\", \"stitch\").</p> <p> </p> <code>params</code> <p>Parameters for the action.</p> <p> </p> <code>result</code> <p>Result of executing this step.</p> <p> </p> <code>created_at</code> <p>Timestamp when the step was created.</p> <p> </p> <p>Initialize a workflow step.</p> PARAMETER DESCRIPTION <code>action</code> <p>The action to perform.</p> <p> TYPE: <code>str</code> </p> <code>params</code> <p>Parameters for the action.</p> <p> TYPE: <code>Dict[str, Any]</code> </p> METHOD DESCRIPTION <code>to_dict</code> <p>Convert the step to a dictionary.</p> Source code in <code>src\\veotools\\models.py</code> <pre><code>def __init__(self, action: str, params: Dict[str, Any]):\n    \"\"\"Initialize a workflow step.\n\n    Args:\n        action: The action to perform.\n        params: Parameters for the action.\n    \"\"\"\n    self.id = str(uuid4())\n    self.action = action\n    self.params = params\n    self.result = None\n    self.created_at = datetime.now()\n</code></pre>"},{"location":"api/overview/#veotools.models.WorkflowStep-functions","title":"Functions","text":""},{"location":"api/overview/#veotools.models.WorkflowStep.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert the step to a dictionary.</p> RETURNS DESCRIPTION <code>Dict[str, Any]</code> <p>Dict[str, Any]: Dictionary representation of the workflow step.</p> Source code in <code>src\\veotools\\models.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"Convert the step to a dictionary.\n\n    Returns:\n        Dict[str, Any]: Dictionary representation of the workflow step.\n    \"\"\"\n    return {\n        \"id\": self.id,\n        \"action\": self.action,\n        \"params\": self.params,\n        \"result\": self.result.to_dict() if self.result else None,\n        \"created_at\": self.created_at.isoformat()\n    }\n</code></pre>"},{"location":"api/overview/#veotools.models.Workflow","title":"Workflow","text":"<pre><code>Workflow(name: Optional[str] = None)\n</code></pre> <p>Container for a multi-step video processing workflow.</p> <p>Workflows allow chaining multiple operations like generation, stitching, and processing into a single managed flow.</p> ATTRIBUTE DESCRIPTION <code>id</code> <p>Unique identifier for this workflow.</p> <p> </p> <code>name</code> <p>Human-readable name for the workflow.</p> <p> </p> <code>steps</code> <p>List of workflow steps to execute.</p> <p> TYPE: <code>List[WorkflowStep]</code> </p> <code>current_step</code> <p>Index of the currently executing step.</p> <p> </p> <code>created_at</code> <p>Timestamp when the workflow was created.</p> <p> </p> <code>updated_at</code> <p>Timestamp of the last update.</p> <p> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; workflow = Workflow(\"my_video_project\")\n&gt;&gt;&gt; workflow.add_step(\"generate\", {\"prompt\": \"sunset\"})\n&gt;&gt;&gt; workflow.add_step(\"stitch\", {\"videos\": [\"a.mp4\", \"b.mp4\"]})\n&gt;&gt;&gt; print(len(workflow.steps))  # 2\n</code></pre> <p>Initialize a workflow.</p> PARAMETER DESCRIPTION <code>name</code> <p>Optional name for the workflow. If not provided,  generates a timestamp-based name.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> METHOD DESCRIPTION <code>add_step</code> <p>Add a new step to the workflow.</p> <code>to_dict</code> <p>Convert the workflow to a dictionary.</p> <code>from_dict</code> <p>Create a workflow from a dictionary.</p> Source code in <code>src\\veotools\\models.py</code> <pre><code>def __init__(self, name: Optional[str] = None):\n    \"\"\"Initialize a workflow.\n\n    Args:\n        name: Optional name for the workflow. If not provided,\n             generates a timestamp-based name.\n    \"\"\"\n    self.id = str(uuid4())\n    self.name = name or f\"workflow_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n    self.steps: List[WorkflowStep] = []\n    self.current_step = 0\n    self.created_at = datetime.now()\n    self.updated_at = datetime.now()\n</code></pre>"},{"location":"api/overview/#veotools.models.Workflow-functions","title":"Functions","text":""},{"location":"api/overview/#veotools.models.Workflow.add_step","title":"add_step","text":"<pre><code>add_step(action: str, params: Dict[str, Any]) -&gt; WorkflowStep\n</code></pre> <p>Add a new step to the workflow.</p> PARAMETER DESCRIPTION <code>action</code> <p>The action to perform.</p> <p> TYPE: <code>str</code> </p> <code>params</code> <p>Parameters for the action.</p> <p> TYPE: <code>Dict[str, Any]</code> </p> RETURNS DESCRIPTION <code>WorkflowStep</code> <p>The created workflow step.</p> <p> TYPE: <code>WorkflowStep</code> </p> Source code in <code>src\\veotools\\models.py</code> <pre><code>def add_step(self, action: str, params: Dict[str, Any]) -&gt; WorkflowStep:\n    \"\"\"Add a new step to the workflow.\n\n    Args:\n        action: The action to perform.\n        params: Parameters for the action.\n\n    Returns:\n        WorkflowStep: The created workflow step.\n    \"\"\"\n    step = WorkflowStep(action, params)\n    self.steps.append(step)\n    self.updated_at = datetime.now()\n    return step\n</code></pre>"},{"location":"api/overview/#veotools.models.Workflow.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert the workflow to a dictionary.</p> RETURNS DESCRIPTION <code>Dict[str, Any]</code> <p>Dict[str, Any]: Dictionary representation of the workflow.</p> Source code in <code>src\\veotools\\models.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"Convert the workflow to a dictionary.\n\n    Returns:\n        Dict[str, Any]: Dictionary representation of the workflow.\n    \"\"\"\n    return {\n        \"id\": self.id,\n        \"name\": self.name,\n        \"steps\": [step.to_dict() for step in self.steps],\n        \"current_step\": self.current_step,\n        \"created_at\": self.created_at.isoformat(),\n        \"updated_at\": self.updated_at.isoformat()\n    }\n</code></pre>"},{"location":"api/overview/#veotools.models.Workflow.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(data: Dict[str, Any]) -&gt; Workflow\n</code></pre> <p>Create a workflow from a dictionary.</p> PARAMETER DESCRIPTION <code>data</code> <p>Dictionary containing workflow data.</p> <p> TYPE: <code>Dict[str, Any]</code> </p> RETURNS DESCRIPTION <code>Workflow</code> <p>Reconstructed workflow instance.</p> <p> TYPE: <code>Workflow</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; data = {\"id\": \"123\", \"name\": \"test\", \"current_step\": 2}\n&gt;&gt;&gt; workflow = Workflow.from_dict(data)\n&gt;&gt;&gt; print(workflow.name)  # \"test\"\n</code></pre> Source code in <code>src\\veotools\\models.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: Dict[str, Any]) -&gt; 'Workflow':\n    \"\"\"Create a workflow from a dictionary.\n\n    Args:\n        data: Dictionary containing workflow data.\n\n    Returns:\n        Workflow: Reconstructed workflow instance.\n\n    Examples:\n        &gt;&gt;&gt; data = {\"id\": \"123\", \"name\": \"test\", \"current_step\": 2}\n        &gt;&gt;&gt; workflow = Workflow.from_dict(data)\n        &gt;&gt;&gt; print(workflow.name)  # \"test\"\n    \"\"\"\n    workflow = cls(name=data.get(\"name\"))\n    workflow.id = data[\"id\"]\n    workflow.current_step = data.get(\"current_step\", 0)\n    return workflow\n</code></pre>"},{"location":"api/overview/#video-processing-module","title":"Video Processing Module","text":""},{"location":"api/overview/#veotools.process.extractor","title":"veotools.process.extractor","text":"<p>Frame extraction and video info utilities for Veo Tools.</p> <p>Enhancements: - <code>get_video_info</code> now first attempts to use <code>ffprobe</code> for accurate metadata   (fps, duration, width, height). If <code>ffprobe</code> is unavailable, it falls back   to OpenCV-based probing.</p> FUNCTION DESCRIPTION <code>extract_frame</code> <p>Extract a single frame from a video at the specified time offset.</p> <code>extract_frames</code> <p>Extract multiple frames from a video at specified time offsets.</p> <code>get_video_info</code> <p>Extract comprehensive metadata from a video file.</p>"},{"location":"api/overview/#veotools.process.extractor-classes","title":"Classes","text":""},{"location":"api/overview/#veotools.process.extractor-functions","title":"Functions","text":""},{"location":"api/overview/#veotools.process.extractor.extract_frame","title":"extract_frame","text":"<pre><code>extract_frame(video_path: Path, time_offset: float = -1.0, output_path: Optional[Path] = None) -&gt; Path\n</code></pre> <p>Extract a single frame from a video at the specified time offset.</p> <p>Extracts and saves a frame from a video file as a JPEG image. Supports both positive time offsets (from start) and negative offsets (from end). Uses OpenCV for video processing and automatically manages storage paths.</p> PARAMETER DESCRIPTION <code>video_path</code> <p>Path to the input video file.</p> <p> TYPE: <code>Path</code> </p> <code>time_offset</code> <p>Time in seconds where to extract the frame. Positive values are from the start, negative values from the end. Defaults to -1.0 (1 second from the end).</p> <p> TYPE: <code>float</code> DEFAULT: <code>-1.0</code> </p> <code>output_path</code> <p>Optional custom path for saving the extracted frame. If None, auto-generates a path using StorageManager.</p> <p> TYPE: <code>Optional[Path]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Path</code> <p>The path where the extracted frame was saved.</p> <p> TYPE: <code>Path</code> </p> RAISES DESCRIPTION <code>FileNotFoundError</code> <p>If the input video file doesn't exist.</p> <code>RuntimeError</code> <p>If frame extraction fails (e.g., invalid time offset).</p> <p>Examples:</p> <p>Extract the last frame:</p> <pre><code>&gt;&gt;&gt; frame_path = extract_frame(Path(\"video.mp4\"))\n&gt;&gt;&gt; print(f\"Frame saved to: {frame_path}\")\n</code></pre> <p>Extract frame at 5 seconds:</p> <pre><code>&gt;&gt;&gt; frame_path = extract_frame(Path(\"video.mp4\"), time_offset=5.0)\n</code></pre> <p>Extract with custom output path:</p> <pre><code>&gt;&gt;&gt; custom_path = Path(\"my_frame.jpg\")\n&gt;&gt;&gt; frame_path = extract_frame(\n...     Path(\"video.mp4\"),\n...     time_offset=10.0,\n...     output_path=custom_path\n... )\n</code></pre> Source code in <code>src\\veotools\\process\\extractor.py</code> <pre><code>def extract_frame(\n    video_path: Path,\n    time_offset: float = -1.0,\n    output_path: Optional[Path] = None\n) -&gt; Path:\n    \"\"\"Extract a single frame from a video at the specified time offset.\n\n    Extracts and saves a frame from a video file as a JPEG image. Supports both\n    positive time offsets (from start) and negative offsets (from end). Uses\n    OpenCV for video processing and automatically manages storage paths.\n\n    Args:\n        video_path: Path to the input video file.\n        time_offset: Time in seconds where to extract the frame. Positive values\n            are from the start, negative values from the end. Defaults to -1.0\n            (1 second from the end).\n        output_path: Optional custom path for saving the extracted frame. If None,\n            auto-generates a path using StorageManager.\n\n    Returns:\n        Path: The path where the extracted frame was saved.\n\n    Raises:\n        FileNotFoundError: If the input video file doesn't exist.\n        RuntimeError: If frame extraction fails (e.g., invalid time offset).\n\n    Examples:\n        Extract the last frame:\n        &gt;&gt;&gt; frame_path = extract_frame(Path(\"video.mp4\"))\n        &gt;&gt;&gt; print(f\"Frame saved to: {frame_path}\")\n\n        Extract frame at 5 seconds:\n        &gt;&gt;&gt; frame_path = extract_frame(Path(\"video.mp4\"), time_offset=5.0)\n\n        Extract with custom output path:\n        &gt;&gt;&gt; custom_path = Path(\"my_frame.jpg\")\n        &gt;&gt;&gt; frame_path = extract_frame(\n        ...     Path(\"video.mp4\"),\n        ...     time_offset=10.0,\n        ...     output_path=custom_path\n        ... )\n    \"\"\"\n    if not video_path.exists():\n        raise FileNotFoundError(f\"Video not found: {video_path}\")\n\n    storage = StorageManager()\n    cap = cv2.VideoCapture(str(video_path))\n\n    try:\n        fps = cap.get(cv2.CAP_PROP_FPS)\n        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n        duration = total_frames / fps if fps &gt; 0 else 0\n\n        if time_offset &lt; 0:\n            target_time = max(0, duration + time_offset)\n        else:\n            target_time = min(duration, time_offset)\n\n        target_frame = int(target_time * fps)\n\n        cap.set(cv2.CAP_PROP_POS_FRAMES, target_frame)\n        ret, frame = cap.read()\n\n        if not ret:\n            raise RuntimeError(f\"Failed to extract frame at {target_time:.1f}s\")\n\n        if output_path is None:\n            filename = f\"frame_{video_path.stem}_at_{target_time:.1f}s.jpg\"\n            output_path = storage.get_frame_path(filename)\n\n        cv2.imwrite(str(output_path), frame)\n\n        return output_path\n\n    finally:\n        cap.release()\n</code></pre>"},{"location":"api/overview/#veotools.process.extractor.extract_frames","title":"extract_frames","text":"<pre><code>extract_frames(video_path: Path, times: list, output_dir: Optional[Path] = None) -&gt; list\n</code></pre> <p>Extract multiple frames from a video at specified time offsets.</p> <p>Extracts and saves multiple frames from a video file as JPEG images. Each time offset can be positive (from start) or negative (from end). Uses OpenCV for efficient batch frame extraction.</p> PARAMETER DESCRIPTION <code>video_path</code> <p>Path to the input video file.</p> <p> TYPE: <code>Path</code> </p> <code>times</code> <p>List of time offsets in seconds. Each can be positive (from start) or negative (from end).</p> <p> TYPE: <code>list</code> </p> <code>output_dir</code> <p>Optional directory for saving frames. If None, uses StorageManager's default frame directory.</p> <p> TYPE: <code>Optional[Path]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>list</code> <p>List of Path objects where the extracted frames were saved. Order matches the input times list.</p> <p> TYPE: <code>list</code> </p> RAISES DESCRIPTION <code>FileNotFoundError</code> <p>If the input video file doesn't exist.</p> <p>Examples:</p> <p>Extract frames at multiple timestamps:</p> <pre><code>&gt;&gt;&gt; frame_paths = extract_frames(\n...     Path(\"video.mp4\"),\n...     [0.0, 5.0, 10.0, -1.0]  # Start, 5s, 10s, and 1s from end\n... )\n&gt;&gt;&gt; print(f\"Extracted {len(frame_paths)} frames\")\n</code></pre> <p>Extract to custom directory:</p> <pre><code>&gt;&gt;&gt; output_dir = Path(\"extracted_frames\")\n&gt;&gt;&gt; frame_paths = extract_frames(\n...     Path(\"movie.mp4\"),\n...     [1.0, 2.0, 3.0],\n...     output_dir=output_dir\n... )\n</code></pre> Note <p>Failed frame extractions are silently skipped. The returned list may contain fewer paths than input times if some extractions fail.</p> Source code in <code>src\\veotools\\process\\extractor.py</code> <pre><code>def extract_frames(\n    video_path: Path,\n    times: list,\n    output_dir: Optional[Path] = None\n) -&gt; list:\n    \"\"\"Extract multiple frames from a video at specified time offsets.\n\n    Extracts and saves multiple frames from a video file as JPEG images. Each\n    time offset can be positive (from start) or negative (from end). Uses\n    OpenCV for efficient batch frame extraction.\n\n    Args:\n        video_path: Path to the input video file.\n        times: List of time offsets in seconds. Each can be positive (from start)\n            or negative (from end).\n        output_dir: Optional directory for saving frames. If None, uses\n            StorageManager's default frame directory.\n\n    Returns:\n        list: List of Path objects where the extracted frames were saved.\n            Order matches the input times list.\n\n    Raises:\n        FileNotFoundError: If the input video file doesn't exist.\n\n    Examples:\n        Extract frames at multiple timestamps:\n        &gt;&gt;&gt; frame_paths = extract_frames(\n        ...     Path(\"video.mp4\"),\n        ...     [0.0, 5.0, 10.0, -1.0]  # Start, 5s, 10s, and 1s from end\n        ... )\n        &gt;&gt;&gt; print(f\"Extracted {len(frame_paths)} frames\")\n\n        Extract to custom directory:\n        &gt;&gt;&gt; output_dir = Path(\"extracted_frames\")\n        &gt;&gt;&gt; frame_paths = extract_frames(\n        ...     Path(\"movie.mp4\"),\n        ...     [1.0, 2.0, 3.0],\n        ...     output_dir=output_dir\n        ... )\n\n    Note:\n        Failed frame extractions are silently skipped. The returned list may\n        contain fewer paths than input times if some extractions fail.\n    \"\"\"\n    if not video_path.exists():\n        raise FileNotFoundError(f\"Video not found: {video_path}\")\n\n    storage = StorageManager()\n    cap = cv2.VideoCapture(str(video_path))\n    frames = []\n\n    try:\n        fps = cap.get(cv2.CAP_PROP_FPS)\n        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n        duration = total_frames / fps if fps &gt; 0 else 0\n\n        for i, time_offset in enumerate(times):\n            if time_offset &lt; 0:\n                target_time = max(0, duration + time_offset)\n            else:\n                target_time = min(duration, time_offset)\n\n            target_frame = int(target_time * fps)\n\n            cap.set(cv2.CAP_PROP_POS_FRAMES, target_frame)\n            ret, frame = cap.read()\n\n            if ret:\n                if output_dir:\n                    output_path = output_dir / f\"frame_{i:03d}_at_{target_time:.1f}s.jpg\"\n                else:\n                    filename = f\"frame_{video_path.stem}_{i:03d}_at_{target_time:.1f}s.jpg\"\n                    output_path = storage.get_frame_path(filename)\n\n                cv2.imwrite(str(output_path), frame)\n                frames.append(output_path)\n\n        return frames\n\n    finally:\n        cap.release()\n</code></pre>"},{"location":"api/overview/#veotools.process.extractor.get_video_info","title":"get_video_info","text":"<pre><code>get_video_info(video_path: Path) -&gt; dict\n</code></pre> <p>Extract comprehensive metadata from a video file.</p> <p>Retrieves video metadata including frame rate, duration, dimensions, and frame count. First attempts to use ffprobe for accurate metadata extraction, falling back to OpenCV if ffprobe is unavailable. This dual approach ensures maximum compatibility and accuracy.</p> PARAMETER DESCRIPTION <code>video_path</code> <p>Path to the input video file.</p> <p> TYPE: <code>Path</code> </p> RETURNS DESCRIPTION <code>dict</code> <p>Video metadata containing: - fps (float): Frames per second - frame_count (int): Total number of frames - width (int): Video width in pixels - height (int): Video height in pixels - duration (float): Video duration in seconds</p> <p> TYPE: <code>dict</code> </p> RAISES DESCRIPTION <code>FileNotFoundError</code> <p>If the input video file doesn't exist.</p> <p>Examples:</p> <p>Get basic video information:</p> <pre><code>&gt;&gt;&gt; info = get_video_info(Path(\"video.mp4\"))\n&gt;&gt;&gt; print(f\"Duration: {info['duration']:.2f}s\")\n&gt;&gt;&gt; print(f\"Resolution: {info['width']}x{info['height']}\")\n&gt;&gt;&gt; print(f\"Frame rate: {info['fps']} fps\")\n</code></pre> <p>Check if video has expected properties:</p> <pre><code>&gt;&gt;&gt; info = get_video_info(Path(\"movie.mp4\"))\n&gt;&gt;&gt; if info['fps'] &gt; 30:\n...     print(\"High frame rate video\")\n&gt;&gt;&gt; if info['width'] &gt;= 1920:\n...     print(\"HD or higher resolution\")\n</code></pre> Note <ul> <li>ffprobe (from FFmpeg) provides more accurate metadata when available</li> <li>OpenCV fallback may have slight inaccuracies in frame rate calculation</li> <li>All numeric values are guaranteed to be non-negative</li> <li>Returns 0.0 for fps/duration if video properties cannot be determined</li> </ul> Source code in <code>src\\veotools\\process\\extractor.py</code> <pre><code>def get_video_info(video_path: Path) -&gt; dict:\n    \"\"\"Extract comprehensive metadata from a video file.\n\n    Retrieves video metadata including frame rate, duration, dimensions, and frame count.\n    First attempts to use ffprobe for accurate metadata extraction, falling back to\n    OpenCV if ffprobe is unavailable. This dual approach ensures maximum compatibility\n    and accuracy.\n\n    Args:\n        video_path: Path to the input video file.\n\n    Returns:\n        dict: Video metadata containing:\n            - fps (float): Frames per second\n            - frame_count (int): Total number of frames\n            - width (int): Video width in pixels\n            - height (int): Video height in pixels\n            - duration (float): Video duration in seconds\n\n    Raises:\n        FileNotFoundError: If the input video file doesn't exist.\n\n    Examples:\n        Get basic video information:\n        &gt;&gt;&gt; info = get_video_info(Path(\"video.mp4\"))\n        &gt;&gt;&gt; print(f\"Duration: {info['duration']:.2f}s\")\n        &gt;&gt;&gt; print(f\"Resolution: {info['width']}x{info['height']}\")\n        &gt;&gt;&gt; print(f\"Frame rate: {info['fps']} fps\")\n\n        Check if video has expected properties:\n        &gt;&gt;&gt; info = get_video_info(Path(\"movie.mp4\"))\n        &gt;&gt;&gt; if info['fps'] &gt; 30:\n        ...     print(\"High frame rate video\")\n        &gt;&gt;&gt; if info['width'] &gt;= 1920:\n        ...     print(\"HD or higher resolution\")\n\n    Note:\n        - ffprobe (from FFmpeg) provides more accurate metadata when available\n        - OpenCV fallback may have slight inaccuracies in frame rate calculation\n        - All numeric values are guaranteed to be non-negative\n        - Returns 0.0 for fps/duration if video properties cannot be determined\n    \"\"\"\n    if not video_path.exists():\n        raise FileNotFoundError(f\"Video not found: {video_path}\")\n\n    # Try ffprobe for precise metadata\n    try:\n        cmd = [\n            \"ffprobe\", \"-v\", \"error\",\n            \"-print_format\", \"json\",\n            \"-show_format\",\n            \"-show_streams\",\n            str(video_path)\n        ]\n        res = subprocess.run(cmd, capture_output=True, text=True, check=True)\n        data = json.loads(res.stdout or \"{}\")\n        video_stream = None\n        for s in data.get(\"streams\", []):\n            if s.get(\"codec_type\") == \"video\":\n                video_stream = s\n                break\n        if video_stream:\n            # FPS can be in r_frame_rate or avg_frame_rate as \"num/den\"\n            fps_val = 0.0\n            for key in (\"avg_frame_rate\", \"r_frame_rate\"):\n                rate = video_stream.get(key)\n                if isinstance(rate, str) and \"/\" in rate:\n                    num, den = rate.split(\"/\", 1)\n                    try:\n                        num_f, den_f = float(num), float(den)\n                        if den_f &gt; 0:\n                            fps_val = num_f / den_f\n                            break\n                    except Exception:\n                        pass\n            width = int(video_stream.get(\"width\", 0) or 0)\n            height = int(video_stream.get(\"height\", 0) or 0)\n            duration = None\n            # Prefer format duration\n            if \"format\" in data and data[\"format\"].get(\"duration\"):\n                try:\n                    duration = float(data[\"format\"][\"duration\"])  # seconds\n                except Exception:\n                    duration = None\n            if duration is None and video_stream.get(\"duration\"):\n                try:\n                    duration = float(video_stream[\"duration\"])  # seconds\n                except Exception:\n                    duration = None\n            frame_count = int(fps_val * duration) if fps_val and duration else 0\n            return {\n                \"fps\": fps_val or 0.0,\n                \"frame_count\": frame_count,\n                \"width\": width,\n                \"height\": height,\n                \"duration\": duration or 0.0,\n            }\n    except (subprocess.CalledProcessError, FileNotFoundError, json.JSONDecodeError):\n        # Fall back to OpenCV below\n        pass\n\n    # Fallback: OpenCV probing\n    cap = cv2.VideoCapture(str(video_path))\n    try:\n        fps = cap.get(cv2.CAP_PROP_FPS)\n        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n        duration = frame_count / fps if fps and fps &gt; 0 else 0\n        return {\n            \"fps\": fps or 0.0,\n            \"frame_count\": frame_count,\n            \"width\": width,\n            \"height\": height,\n            \"duration\": duration,\n        }\n    finally:\n        cap.release()\n</code></pre>"},{"location":"api/overview/#video-stitching-module","title":"Video Stitching Module","text":""},{"location":"api/overview/#veotools.stitch.seamless","title":"veotools.stitch.seamless","text":"<p>Seamless video stitching for Veo Tools.</p> FUNCTION DESCRIPTION <code>stitch_videos</code> <p>Seamlessly stitch multiple videos (with audio) into a single timeline.</p> <code>stitch_with_transitions</code> <p>Stitch videos together with custom transition videos between them.</p> <code>create_transition_points</code> <p>Extract frames from two videos to analyze potential transition points.</p>"},{"location":"api/overview/#veotools.stitch.seamless-classes","title":"Classes","text":""},{"location":"api/overview/#veotools.stitch.seamless-functions","title":"Functions","text":""},{"location":"api/overview/#veotools.stitch.seamless.stitch_videos","title":"stitch_videos","text":"<pre><code>stitch_videos(video_paths: List[Path], overlap: float = 1.0, output_path: Optional[Path] = None, on_progress: Optional[Callable] = None) -&gt; VideoResult\n</code></pre> <p>Seamlessly stitch multiple videos (with audio) into a single timeline.</p> <p>Uses FFmpeg to concatenate videos while optionally trimming an overlap from the tail of each clip (except the last) to create smoother scene transitions. Both audio and video streams are preserved and re-encoded into a single H.264/AAC MP4.</p> PARAMETER DESCRIPTION <code>video_paths</code> <p>List of paths to video files to stitch together, in order.</p> <p> TYPE: <code>List[Path]</code> </p> <code>overlap</code> <p>Duration in seconds to trim from the end of each video (except the last one) to create smooth transitions. Defaults to 1.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>output_path</code> <p>Optional custom output path. If None, auto-generates a path using :class:<code>StorageManager</code>.</p> <p> TYPE: <code>Optional[Path]</code> DEFAULT: <code>None</code> </p> <code>on_progress</code> <p>Optional callback function called with progress updates (message, percent).</p> <p> TYPE: <code>Optional[Callable]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>VideoResult</code> <p>Object containing the stitched video path, metadata, and operation details.</p> <p> TYPE: <code>VideoResult</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If fewer than two videos are provided.</p> <code>FileNotFoundError</code> <p>If any input video file doesn't exist.</p> <code>RuntimeError</code> <p>If FFmpeg fails to stitch the videos.</p> <p>Examples:</p> <p>Stitch videos with default overlap:</p> <pre><code>&gt;&gt;&gt; video_files = [Path(\"part1.mp4\"), Path(\"part2.mp4\"), Path(\"part3.mp4\")]\n&gt;&gt;&gt; result = stitch_videos(video_files)\n&gt;&gt;&gt; print(f\"Stitched video: {result.path}\")\n</code></pre> <p>Stitch without overlap:</p> <pre><code>&gt;&gt;&gt; result = stitch_videos(video_files, overlap=0.0)\n</code></pre> <p>Stitch with progress tracking:</p> <pre><code>&gt;&gt;&gt; def show_progress(msg, pct):\n...     print(f\"Stitching: {msg} ({pct}%)\")\n&gt;&gt;&gt; result = stitch_videos(\n...     video_files,\n...     overlap=2.0,\n...     on_progress=show_progress\n... )\n</code></pre> <p>Custom output location:</p> <pre><code>&gt;&gt;&gt; result = stitch_videos(\n...     video_files,\n...     output_path=Path(\"final_movie.mp4\")\n... )\n</code></pre> Note <ul> <li>Videos are resized to match the first video's dimensions</li> <li>Uses H.264 encoding with CRF 23 for good quality/size balance</li> <li>Automatically handles frame rate consistency</li> <li>FFmpeg is used for final encoding if available, otherwise uses OpenCV</li> </ul> Source code in <code>src\\veotools\\stitch\\seamless.py</code> <pre><code>def stitch_videos(\n    video_paths: List[Path],\n    overlap: float = 1.0,\n    output_path: Optional[Path] = None,\n    on_progress: Optional[Callable] = None,\n) -&gt; VideoResult:\n    \"\"\"Seamlessly stitch multiple videos (with audio) into a single timeline.\n\n    Uses FFmpeg to concatenate videos while optionally trimming an overlap from\n    the tail of each clip (except the last) to create smoother scene transitions.\n    Both audio and video streams are preserved and re-encoded into a single\n    H.264/AAC MP4.\n\n    Args:\n        video_paths: List of paths to video files to stitch together, in order.\n        overlap: Duration in seconds to trim from the end of each video (except\n            the last one) to create smooth transitions. Defaults to 1.0.\n        output_path: Optional custom output path. If None, auto-generates a path\n            using :class:`StorageManager`.\n        on_progress: Optional callback function called with progress updates (message, percent).\n\n    Returns:\n        VideoResult: Object containing the stitched video path, metadata, and operation details.\n\n    Raises:\n        ValueError: If fewer than two videos are provided.\n        FileNotFoundError: If any input video file doesn't exist.\n        RuntimeError: If FFmpeg fails to stitch the videos.\n\n    Examples:\n        Stitch videos with default overlap:\n        &gt;&gt;&gt; video_files = [Path(\"part1.mp4\"), Path(\"part2.mp4\"), Path(\"part3.mp4\")]\n        &gt;&gt;&gt; result = stitch_videos(video_files)\n        &gt;&gt;&gt; print(f\"Stitched video: {result.path}\")\n\n        Stitch without overlap:\n        &gt;&gt;&gt; result = stitch_videos(video_files, overlap=0.0)\n\n        Stitch with progress tracking:\n        &gt;&gt;&gt; def show_progress(msg, pct):\n        ...     print(f\"Stitching: {msg} ({pct}%)\")\n        &gt;&gt;&gt; result = stitch_videos(\n        ...     video_files,\n        ...     overlap=2.0,\n        ...     on_progress=show_progress\n        ... )\n\n        Custom output location:\n        &gt;&gt;&gt; result = stitch_videos(\n        ...     video_files,\n        ...     output_path=Path(\"final_movie.mp4\")\n        ... )\n\n    Note:\n        - Videos are resized to match the first video's dimensions\n        - Uses H.264 encoding with CRF 23 for good quality/size balance\n        - Automatically handles frame rate consistency\n        - FFmpeg is used for final encoding if available, otherwise uses OpenCV\n    \"\"\"\n    if len(video_paths) &lt; 2:\n        raise ValueError(\"Need at least two videos to stitch\")\n\n    storage = StorageManager()\n    progress = ProgressTracker(on_progress)\n    result = VideoResult()\n\n    try:\n        progress.start(\"Preparing\")\n\n        for path in video_paths:\n            if not path.exists():\n                raise FileNotFoundError(f\"Video not found: {path}\")\n\n        clip_info: List[dict] = []\n        for path in video_paths:\n            info = get_video_info(path)\n            duration = float(info.get(\"duration\") or 0.0)\n            if duration &lt;= 0:\n                raise RuntimeError(f\"Unable to determine duration for {path}\")\n            clip_info.append(info)\n\n        # Determine if all clips contain audio. ffprobe via get_video_info doesn't\n        # expose audio presence, so detect separately.\n        audio_presence: List[bool] = []\n        for path in video_paths:\n            audio_presence.append(_has_audio(path))\n        include_audio = any(audio_presence)\n\n        if output_path is None:\n            filename = f\"stitched_{result.id[:8]}.mp4\"\n            output_path = storage.get_video_path(filename)\n        output_path.parent.mkdir(parents=True, exist_ok=True)\n\n        filter_parts: List[str] = []\n        video_refs: List[str] = []\n        audio_refs: List[str] = []\n\n        for idx, (path, info) in enumerate(zip(video_paths, clip_info)):\n            duration = float(info.get(\"duration\") or 0.0)\n            trim_end = duration\n            if overlap &gt; 0 and idx &lt; len(video_paths) - 1 and duration - overlap &gt; 0.01:\n                trim_end = duration - overlap\n\n            video_label = f\"v{idx}\"\n            if trim_end &lt; duration:\n                filter_parts.append(\n                    f\"[{idx}:v]trim=0:{trim_end:.6f},setpts=PTS-STARTPTS[{video_label}]\"\n                )\n            else:\n                filter_parts.append(\n                    f\"[{idx}:v]setpts=PTS-STARTPTS[{video_label}]\"\n                )\n            video_refs.append(f\"[{video_label}]\")\n\n            if include_audio:\n                audio_label = f\"a{idx}\"\n                if audio_presence[idx]:\n                    if trim_end &lt; duration:\n                        filter_parts.append(\n                            f\"[{idx}:a]atrim=0:{trim_end:.6f},asetpts=PTS-STARTPTS[{audio_label}]\"\n                        )\n                    else:\n                        filter_parts.append(\n                            f\"[{idx}:a]asetpts=PTS-STARTPTS[{audio_label}]\"\n                        )\n                else:\n                    filter_parts.append(\n                        f\"anullsrc=channel_layout=stereo:sample_rate=48000,atrim=0:{trim_end:.6f}[{audio_label}]\"\n                    )\n                audio_refs.append(f\"[{audio_label}]\")\n\n        if include_audio:\n            concat_inputs = \"\".join(v + a for v, a in zip(video_refs, audio_refs))\n            filter_parts.append(\n                f\"{concat_inputs}concat=n={len(video_paths)}:v=1:a=1[outv][outa]\"\n            )\n        else:\n            concat_inputs = \"\".join(video_refs)\n            filter_parts.append(\n                f\"{concat_inputs}concat=n={len(video_paths)}:v=1:a=0[outv]\"\n            )\n\n        filter_complex = \"; \".join(filter_parts)\n\n        cmd: List[str] = [\"ffmpeg\"]\n        for path in video_paths:\n            cmd.extend([\"-i\", str(path)])\n        cmd.extend([\n            \"-filter_complex\", filter_complex,\n            \"-map\", \"[outv]\",\n        ])\n        if include_audio:\n            cmd.extend([\"-map\", \"[outa]\"])\n        cmd.extend([\n            \"-c:v\", \"libx264\",\n            \"-preset\", \"fast\",\n            \"-crf\", \"21\",\n            \"-pix_fmt\", \"yuv420p\",\n        ])\n        if include_audio:\n            cmd.extend([\"-c:a\", \"aac\", \"-b:a\", \"192k\"])\n        cmd.extend([\n            \"-movflags\", \"+faststart\",\n            \"-y\",\n            str(output_path),\n        ])\n\n        progress.update(\"Encoding\", 90)\n        try:\n            subprocess.run(cmd, check=True, capture_output=True)\n        except subprocess.CalledProcessError as exc:\n            raise RuntimeError(\n                f\"Failed to stitch videos with ffmpeg: {exc.stderr.decode().strip() if exc.stderr else exc}\"\n            ) from exc\n\n        progress.complete(\"Complete\")\n\n        result.path = output_path\n        result.url = storage.get_url(output_path)\n        output_info = get_video_info(output_path)\n        result.metadata = VideoMetadata(\n            fps=float(output_info.get(\"fps\") or 0.0),\n            duration=float(output_info.get(\"duration\") or 0.0),\n            width=int(output_info.get(\"width\") or 0),\n            height=int(output_info.get(\"height\") or 0),\n        )\n        result.update_progress(\"Complete\", 100)\n\n    except Exception as exc:\n        result.mark_failed(exc)\n        raise\n\n    return result\n</code></pre>"},{"location":"api/overview/#veotools.stitch.seamless.stitch_with_transitions","title":"stitch_with_transitions","text":"<pre><code>stitch_with_transitions(video_paths: List[Path], transition_videos: List[Path], output_path: Optional[Path] = None, on_progress: Optional[Callable] = None) -&gt; VideoResult\n</code></pre> <p>Stitch videos together with custom transition videos between them.</p> <p>Combines multiple videos by inserting transition videos between each pair of main videos. The transitions are placed between consecutive videos to create smooth, cinematic connections between scenes.</p> PARAMETER DESCRIPTION <code>video_paths</code> <p>List of main video files to stitch together, in order.</p> <p> TYPE: <code>List[Path]</code> </p> <code>transition_videos</code> <p>List of transition videos to insert between main videos. Must have exactly len(video_paths) - 1 transitions.</p> <p> TYPE: <code>List[Path]</code> </p> <code>output_path</code> <p>Optional custom output path. If None, auto-generates a path using StorageManager.</p> <p> TYPE: <code>Optional[Path]</code> DEFAULT: <code>None</code> </p> <code>on_progress</code> <p>Optional callback function called with progress updates (message, percent).</p> <p> TYPE: <code>Optional[Callable]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>VideoResult</code> <p>Object containing the final stitched video with transitions.</p> <p> TYPE: <code>VideoResult</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If the number of transition videos doesn't match the requirement (should be one less than the number of main videos).</p> <code>FileNotFoundError</code> <p>If any video file doesn't exist.</p> <p>Examples:</p> <p>Add transitions between three video clips:</p> <pre><code>&gt;&gt;&gt; main_videos = [Path(\"scene1.mp4\"), Path(\"scene2.mp4\"), Path(\"scene3.mp4\")]\n&gt;&gt;&gt; transitions = [Path(\"fade1.mp4\"), Path(\"fade2.mp4\")]\n&gt;&gt;&gt; result = stitch_with_transitions(main_videos, transitions)\n&gt;&gt;&gt; print(f\"Final video with transitions: {result.path}\")\n</code></pre> <p>With progress tracking:</p> <pre><code>&gt;&gt;&gt; def track_progress(msg, pct):\n...     print(f\"Processing: {msg} - {pct}%\")\n&gt;&gt;&gt; result = stitch_with_transitions(\n...     main_videos,\n...     transitions,\n...     on_progress=track_progress\n... )\n</code></pre> Note <p>This function uses stitch_videos internally with overlap=0 to preserve transition videos exactly as provided.</p> Source code in <code>src\\veotools\\stitch\\seamless.py</code> <pre><code>def stitch_with_transitions(\n    video_paths: List[Path],\n    transition_videos: List[Path],\n    output_path: Optional[Path] = None,\n    on_progress: Optional[Callable] = None\n) -&gt; VideoResult:\n    \"\"\"Stitch videos together with custom transition videos between them.\n\n    Combines multiple videos by inserting transition videos between each pair\n    of main videos. The transitions are placed between consecutive videos to\n    create smooth, cinematic connections between scenes.\n\n    Args:\n        video_paths: List of main video files to stitch together, in order.\n        transition_videos: List of transition videos to insert between main videos.\n            Must have exactly len(video_paths) - 1 transitions.\n        output_path: Optional custom output path. If None, auto-generates a path\n            using StorageManager.\n        on_progress: Optional callback function called with progress updates (message, percent).\n\n    Returns:\n        VideoResult: Object containing the final stitched video with transitions.\n\n    Raises:\n        ValueError: If the number of transition videos doesn't match the requirement\n            (should be one less than the number of main videos).\n        FileNotFoundError: If any video file doesn't exist.\n\n    Examples:\n        Add transitions between three video clips:\n        &gt;&gt;&gt; main_videos = [Path(\"scene1.mp4\"), Path(\"scene2.mp4\"), Path(\"scene3.mp4\")]\n        &gt;&gt;&gt; transitions = [Path(\"fade1.mp4\"), Path(\"fade2.mp4\")]\n        &gt;&gt;&gt; result = stitch_with_transitions(main_videos, transitions)\n        &gt;&gt;&gt; print(f\"Final video with transitions: {result.path}\")\n\n        With progress tracking:\n        &gt;&gt;&gt; def track_progress(msg, pct):\n        ...     print(f\"Processing: {msg} - {pct}%\")\n        &gt;&gt;&gt; result = stitch_with_transitions(\n        ...     main_videos,\n        ...     transitions,\n        ...     on_progress=track_progress\n        ... )\n\n    Note:\n        This function uses stitch_videos internally with overlap=0 to preserve\n        transition videos exactly as provided.\n    \"\"\"\n    if len(transition_videos) != len(video_paths) - 1:\n        raise ValueError(f\"Need {len(video_paths)-1} transitions for {len(video_paths)} videos\")\n\n    combined_paths = []\n    for i, video in enumerate(video_paths[:-1]):\n        combined_paths.append(video)\n        combined_paths.append(transition_videos[i])\n    combined_paths.append(video_paths[-1])\n\n    return stitch_videos(\n        combined_paths,\n        overlap=0,\n        output_path=output_path,\n        on_progress=on_progress\n    )\n</code></pre>"},{"location":"api/overview/#veotools.stitch.seamless.create_transition_points","title":"create_transition_points","text":"<pre><code>create_transition_points(video_a: Path, video_b: Path, extract_points: Optional[dict] = None) -&gt; tuple\n</code></pre> <p>Extract frames from two videos to analyze potential transition points.</p> <p>Extracts representative frames from two videos that can be used to analyze how well they might transition together. Typically extracts the ending frame of the first video and the beginning frame of the second video.</p> PARAMETER DESCRIPTION <code>video_a</code> <p>Path to the first video file.</p> <p> TYPE: <code>Path</code> </p> <code>video_b</code> <p>Path to the second video file.</p> <p> TYPE: <code>Path</code> </p> <code>extract_points</code> <p>Optional dictionary specifying extraction points: - \"a_end\": Time offset for frame extraction from video_a (default: -1.0) - \"b_start\": Time offset for frame extraction from video_b (default: 1.0) If None, uses default values.</p> <p> TYPE: <code>Optional[dict]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>tuple</code> <p>A tuple containing (frame_a_path, frame_b_path) where: - frame_a_path: Path to extracted frame from video_a - frame_b_path: Path to extracted frame from video_b</p> <p> TYPE: <code>tuple</code> </p> RAISES DESCRIPTION <code>FileNotFoundError</code> <p>If either video file doesn't exist.</p> <code>RuntimeError</code> <p>If frame extraction fails for either video.</p> <p>Examples:</p> <p>Extract transition frames with defaults:</p> <pre><code>&gt;&gt;&gt; frame_a, frame_b = create_transition_points(\n...     Path(\"clip1.mp4\"),\n...     Path(\"clip2.mp4\")\n... )\n&gt;&gt;&gt; print(f\"Transition frames: {frame_a}, {frame_b}\")\n</code></pre> <p>Custom extraction points:</p> <pre><code>&gt;&gt;&gt; points = {\"a_end\": -2.0, \"b_start\": 0.5}\n&gt;&gt;&gt; frame_a, frame_b = create_transition_points(\n...     Path(\"scene1.mp4\"),\n...     Path(\"scene2.mp4\"),\n...     extract_points=points\n... )\n</code></pre> Note <ul> <li>Default extracts 1 second before the end of video_a</li> <li>Default extracts 1 second after the start of video_b</li> <li>Negative values in extract_points count from the end of the video</li> <li>These frames can be used to analyze color, composition, or content   similarity for better transition planning</li> </ul> Source code in <code>src\\veotools\\stitch\\seamless.py</code> <pre><code>def create_transition_points(\n    video_a: Path,\n    video_b: Path,\n    extract_points: Optional[dict] = None\n) -&gt; tuple:\n    \"\"\"Extract frames from two videos to analyze potential transition points.\n\n    Extracts representative frames from two videos that can be used to analyze\n    how well they might transition together. Typically extracts the ending frame\n    of the first video and the beginning frame of the second video.\n\n    Args:\n        video_a: Path to the first video file.\n        video_b: Path to the second video file.\n        extract_points: Optional dictionary specifying extraction points:\n            - \"a_end\": Time offset for frame extraction from video_a (default: -1.0)\n            - \"b_start\": Time offset for frame extraction from video_b (default: 1.0)\n            If None, uses default values.\n\n    Returns:\n        tuple: A tuple containing (frame_a_path, frame_b_path) where:\n            - frame_a_path: Path to extracted frame from video_a\n            - frame_b_path: Path to extracted frame from video_b\n\n    Raises:\n        FileNotFoundError: If either video file doesn't exist.\n        RuntimeError: If frame extraction fails for either video.\n\n    Examples:\n        Extract transition frames with defaults:\n        &gt;&gt;&gt; frame_a, frame_b = create_transition_points(\n        ...     Path(\"clip1.mp4\"),\n        ...     Path(\"clip2.mp4\")\n        ... )\n        &gt;&gt;&gt; print(f\"Transition frames: {frame_a}, {frame_b}\")\n\n        Custom extraction points:\n        &gt;&gt;&gt; points = {\"a_end\": -2.0, \"b_start\": 0.5}\n        &gt;&gt;&gt; frame_a, frame_b = create_transition_points(\n        ...     Path(\"scene1.mp4\"),\n        ...     Path(\"scene2.mp4\"),\n        ...     extract_points=points\n        ... )\n\n    Note:\n        - Default extracts 1 second before the end of video_a\n        - Default extracts 1 second after the start of video_b\n        - Negative values in extract_points count from the end of the video\n        - These frames can be used to analyze color, composition, or content\n          similarity for better transition planning\n    \"\"\"\n    from ..process.extractor import extract_frame\n\n    if extract_points is None:\n        extract_points = {\n            \"a_end\": -1.0,\n            \"b_start\": 1.0\n        }\n\n    frame_a = extract_frame(video_a, extract_points.get(\"a_end\", -1.0))\n    frame_b = extract_frame(video_b, extract_points.get(\"b_start\", 1.0))\n\n    return frame_a, frame_b\n</code></pre>"},{"location":"api/overview/#bridge-api-module","title":"Bridge API Module","text":""},{"location":"api/overview/#veotools.api.bridge","title":"veotools.api.bridge","text":"CLASS DESCRIPTION <code>Bridge</code> <p>A fluent API bridge for chaining video generation and processing operations.</p>"},{"location":"api/overview/#veotools.api.bridge-classes","title":"Classes","text":""},{"location":"api/overview/#veotools.api.bridge.Bridge","title":"Bridge","text":"<pre><code>Bridge(name: Optional[str] = None)\n</code></pre> <p>A fluent API bridge for chaining video generation and processing operations.</p> <p>The Bridge class provides a convenient, chainable interface for combining multiple video operations like generation, stitching, and media management. It maintains an internal workflow and media queue to track operations and intermediate results.</p> ATTRIBUTE DESCRIPTION <code>workflow</code> <p>Workflow object tracking all operations performed.</p> <p> </p> <code>media_queue</code> <p>List of media file paths in processing order.</p> <p> TYPE: <code>List[Path]</code> </p> <code>results</code> <p>List of VideoResult objects from generation operations.</p> <p> TYPE: <code>List[VideoResult]</code> </p> <code>storage</code> <p>StorageManager instance for file operations.</p> <p> </p> <p>Examples:</p> <p>Basic text-to-video generation:</p> <pre><code>&gt;&gt;&gt; bridge = Bridge(\"my_project\")\n&gt;&gt;&gt; result = bridge.generate(\"A cat playing\").save()\n</code></pre> <p>Chain multiple generations and stitch:</p> <pre><code>&gt;&gt;&gt; bridge = (Bridge(\"movie_project\")\n...     .generate(\"Opening scene\")\n...     .generate(\"Middle scene\")\n...     .generate(\"Ending scene\")\n...     .stitch(overlap=1.0)\n...     .save(Path(\"final_movie.mp4\")))\n</code></pre> <p>Image-to-video with continuation:</p> <pre><code>&gt;&gt;&gt; bridge = (Bridge()\n...     .add_media(\"photo.jpg\")\n...     .generate(\"The person starts walking\")\n...     .generate(\"They walk into the distance\")\n...     .stitch())\n</code></pre> METHOD DESCRIPTION <code>with_progress</code> <p>Set a progress callback for all subsequent operations.</p> <code>add_media</code> <p>Add media files to the processing queue.</p> <code>generate</code> <p>Generate a video using text prompt and optional media input.</p> <code>generate_transition</code> <p>Generate a transition video between the last two media items.</p> <code>stitch</code> <p>Stitch all videos in the queue into a single continuous video.</p> <code>save</code> <p>Save the final result to a specified path or return the current path.</p> <code>get_workflow</code> <p>Get the workflow object containing all performed operations.</p> <code>to_dict</code> <p>Convert the workflow to a dictionary representation.</p> <code>clear</code> <p>Clear the media queue, removing all queued media files.</p> Source code in <code>src\\veotools\\api\\bridge.py</code> <pre><code>def __init__(self, name: Optional[str] = None):\n    self.workflow = Workflow(name)\n    self.media_queue: List[Path] = []\n    self.results: List[VideoResult] = []\n    self.storage = StorageManager()\n    self._on_progress: Optional[Callable] = None\n</code></pre>"},{"location":"api/overview/#veotools.api.bridge.Bridge-functions","title":"Functions","text":""},{"location":"api/overview/#veotools.api.bridge.Bridge.with_progress","title":"with_progress","text":"<pre><code>with_progress(callback: Callable) -&gt; Bridge\n</code></pre> <p>Set a progress callback for all subsequent operations.</p> PARAMETER DESCRIPTION <code>callback</code> <p>Function called with progress updates (message: str, percent: int).</p> <p> TYPE: <code>Callable</code> </p> RETURNS DESCRIPTION <code>Bridge</code> <p>Self for method chaining.</p> <p> TYPE: <code>Bridge</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; def show_progress(msg, pct):\n...     print(f\"{msg}: {pct}%\")\n&gt;&gt;&gt; bridge = Bridge().with_progress(show_progress)\n</code></pre> Source code in <code>src\\veotools\\api\\bridge.py</code> <pre><code>def with_progress(self, callback: Callable) -&gt; 'Bridge':\n    \"\"\"Set a progress callback for all subsequent operations.\n\n    Args:\n        callback: Function called with progress updates (message: str, percent: int).\n\n    Returns:\n        Bridge: Self for method chaining.\n\n    Examples:\n        &gt;&gt;&gt; def show_progress(msg, pct):\n        ...     print(f\"{msg}: {pct}%\")\n        &gt;&gt;&gt; bridge = Bridge().with_progress(show_progress)\n    \"\"\"\n    self._on_progress = callback\n    return self\n</code></pre>"},{"location":"api/overview/#veotools.api.bridge.Bridge.add_media","title":"add_media","text":"<pre><code>add_media(media: Union[str, Path, List[Union[str, Path]]]) -&gt; Bridge\n</code></pre> <p>Add media files to the processing queue.</p> <p>Adds one or more media files (images or videos) to the internal queue. These files can be used as inputs for subsequent generation operations.</p> PARAMETER DESCRIPTION <code>media</code> <p>Single media path, or list of media paths to add to the queue.</p> <p> TYPE: <code>Union[str, Path, List[Union[str, Path]]]</code> </p> RETURNS DESCRIPTION <code>Bridge</code> <p>Self for method chaining.</p> <p> TYPE: <code>Bridge</code> </p> <p>Examples:</p> <p>Add a single image:</p> <pre><code>&gt;&gt;&gt; bridge = Bridge().add_media(\"photo.jpg\")\n</code></pre> <p>Add multiple videos:</p> <pre><code>&gt;&gt;&gt; files = [\"video1.mp4\", \"video2.mp4\", \"video3.mp4\"]\n&gt;&gt;&gt; bridge = Bridge().add_media(files)\n</code></pre> <p>Chain with Path objects:</p> <pre><code>&gt;&gt;&gt; bridge = Bridge().add_media(Path(\"input.mp4\"))\n</code></pre> Source code in <code>src\\veotools\\api\\bridge.py</code> <pre><code>def add_media(self, media: Union[str, Path, List[Union[str, Path]]]) -&gt; 'Bridge':\n    \"\"\"Add media files to the processing queue.\n\n    Adds one or more media files (images or videos) to the internal queue.\n    These files can be used as inputs for subsequent generation operations.\n\n    Args:\n        media: Single media path, or list of media paths to add to the queue.\n\n    Returns:\n        Bridge: Self for method chaining.\n\n    Examples:\n        Add a single image:\n        &gt;&gt;&gt; bridge = Bridge().add_media(\"photo.jpg\")\n\n        Add multiple videos:\n        &gt;&gt;&gt; files = [\"video1.mp4\", \"video2.mp4\", \"video3.mp4\"]\n        &gt;&gt;&gt; bridge = Bridge().add_media(files)\n\n        Chain with Path objects:\n        &gt;&gt;&gt; bridge = Bridge().add_media(Path(\"input.mp4\"))\n    \"\"\"\n    if isinstance(media, list):\n        for m in media:\n            self.media_queue.append(Path(m))\n            self.workflow.add_step(\"add_media\", {\"path\": str(m)})\n    else:\n        self.media_queue.append(Path(media))\n        self.workflow.add_step(\"add_media\", {\"path\": str(media)})\n    return self\n</code></pre>"},{"location":"api/overview/#veotools.api.bridge.Bridge.generate","title":"generate","text":"<pre><code>generate(prompt: str, model: str = 'veo-3.0-fast-generate-preview', **kwargs) -&gt; Bridge\n</code></pre> <p>Generate a video using text prompt and optional media input.</p> <p>Generates a video based on the prompt and the most recent media in the queue. The generation method is automatically selected based on the media type: - No media: text-to-video generation - Image media: image-to-video generation - Video media: video continuation generation</p> PARAMETER DESCRIPTION <code>prompt</code> <p>Text description for video generation.</p> <p> TYPE: <code>str</code> </p> <code>model</code> <p>Veo model to use. Defaults to \"veo-3.0-fast-generate-preview\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>'veo-3.0-fast-generate-preview'</code> </p> <code>**kwargs</code> <p>Additional generation parameters including: - extract_at: Time offset for video continuation (float) - duration_seconds: Video duration (int) - person_generation: Person policy (str) - enhance: Whether to enhance prompt (bool)</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Bridge</code> <p>Self for method chaining.</p> <p> TYPE: <code>Bridge</code> </p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If video generation fails.</p> <p>Examples:</p> <p>Text-to-video generation:</p> <pre><code>&gt;&gt;&gt; bridge = Bridge().generate(\"A sunset over mountains\")\n</code></pre> <p>Image-to-video with existing media:</p> <pre><code>&gt;&gt;&gt; bridge = (Bridge()\n...     .add_media(\"landscape.jpg\")\n...     .generate(\"Clouds moving across the sky\"))\n</code></pre> <p>Video continuation:</p> <pre><code>&gt;&gt;&gt; bridge = (Bridge()\n...     .add_media(\"scene1.mp4\")\n...     .generate(\"The action continues\", extract_at=-2.0))\n</code></pre> <p>Custom model and parameters:</p> <pre><code>&gt;&gt;&gt; bridge = Bridge().generate(\n...     \"A dancing robot\",\n...     model=\"veo-2.0\",\n...     duration_seconds=10,\n...     enhance=True\n... )\n</code></pre> Source code in <code>src\\veotools\\api\\bridge.py</code> <pre><code>def generate(self, prompt: str, model: str = \"veo-3.0-fast-generate-preview\", \n             **kwargs) -&gt; 'Bridge':\n    \"\"\"Generate a video using text prompt and optional media input.\n\n    Generates a video based on the prompt and the most recent media in the queue.\n    The generation method is automatically selected based on the media type:\n    - No media: text-to-video generation\n    - Image media: image-to-video generation\n    - Video media: video continuation generation\n\n    Args:\n        prompt: Text description for video generation.\n        model: Veo model to use. Defaults to \"veo-3.0-fast-generate-preview\".\n        **kwargs: Additional generation parameters including:\n            - extract_at: Time offset for video continuation (float)\n            - duration_seconds: Video duration (int)\n            - person_generation: Person policy (str)\n            - enhance: Whether to enhance prompt (bool)\n\n    Returns:\n        Bridge: Self for method chaining.\n\n    Raises:\n        RuntimeError: If video generation fails.\n\n    Examples:\n        Text-to-video generation:\n        &gt;&gt;&gt; bridge = Bridge().generate(\"A sunset over mountains\")\n\n        Image-to-video with existing media:\n        &gt;&gt;&gt; bridge = (Bridge()\n        ...     .add_media(\"landscape.jpg\")\n        ...     .generate(\"Clouds moving across the sky\"))\n\n        Video continuation:\n        &gt;&gt;&gt; bridge = (Bridge()\n        ...     .add_media(\"scene1.mp4\")\n        ...     .generate(\"The action continues\", extract_at=-2.0))\n\n        Custom model and parameters:\n        &gt;&gt;&gt; bridge = Bridge().generate(\n        ...     \"A dancing robot\",\n        ...     model=\"veo-2.0\",\n        ...     duration_seconds=10,\n        ...     enhance=True\n        ... )\n    \"\"\"\n    step = self.workflow.add_step(\"generate\", {\n        \"prompt\": prompt,\n        \"model\": model,\n        **kwargs\n    })\n\n    if self.media_queue:\n        last_media = self.media_queue[-1]\n\n        if last_media.suffix.lower() in ['.jpg', '.jpeg', '.png', '.gif', '.bmp']:\n            result = generate_from_image(\n                last_media,\n                prompt,\n                model=model,\n                on_progress=self._on_progress,\n                **kwargs\n            )\n        else:\n            result = generate_from_video(\n                last_media,\n                prompt,\n                extract_at=kwargs.pop(\"extract_at\", -1.0),\n                model=model,\n                on_progress=self._on_progress,\n                **kwargs\n            )\n    else:\n        result = generate_from_text(\n            prompt,\n            model=model,\n            on_progress=self._on_progress,\n            **kwargs\n        )\n\n    step.result = result\n    self.results.append(result)\n\n    if result.path:\n        self.media_queue.append(result.path)\n\n    return self\n</code></pre>"},{"location":"api/overview/#veotools.api.bridge.Bridge.generate_transition","title":"generate_transition","text":"<pre><code>generate_transition(prompt: Optional[str] = None, model: str = 'veo-3.0-fast-generate-preview') -&gt; Bridge\n</code></pre> <p>Generate a transition video between the last two media items.</p> <p>Creates a smooth transition video that bridges the gap between the two most recent media items in the queue. The transition is generated from a frame extracted near the end of the second-to-last video.</p> PARAMETER DESCRIPTION <code>prompt</code> <p>Description of the desired transition. If None, uses a default \"smooth cinematic transition between scenes\".</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>model</code> <p>Veo model to use. Defaults to \"veo-3.0-fast-generate-preview\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>'veo-3.0-fast-generate-preview'</code> </p> RETURNS DESCRIPTION <code>Bridge</code> <p>Self for method chaining.</p> <p> TYPE: <code>Bridge</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If fewer than 2 media items are in the queue.</p> <p>Examples:</p> <p>Generate default transition:</p> <pre><code>&gt;&gt;&gt; bridge = (Bridge()\n...     .add_media([\"scene1.mp4\", \"scene2.mp4\"])\n...     .generate_transition())\n</code></pre> <p>Custom transition prompt:</p> <pre><code>&gt;&gt;&gt; bridge = (Bridge()\n...     .generate(\"Day scene\")\n...     .generate(\"Night scene\")\n...     .generate_transition(\"Gradual sunset transition\"))\n</code></pre> Note <p>The transition video is inserted between the last two media items, creating a sequence like: [media_a, transition, media_b, ...]</p> Source code in <code>src\\veotools\\api\\bridge.py</code> <pre><code>def generate_transition(self, prompt: Optional[str] = None, \n                       model: str = \"veo-3.0-fast-generate-preview\") -&gt; 'Bridge':\n    \"\"\"Generate a transition video between the last two media items.\n\n    Creates a smooth transition video that bridges the gap between the two most\n    recent media items in the queue. The transition is generated from a frame\n    extracted near the end of the second-to-last video.\n\n    Args:\n        prompt: Description of the desired transition. If None, uses a default\n            \"smooth cinematic transition between scenes\".\n        model: Veo model to use. Defaults to \"veo-3.0-fast-generate-preview\".\n\n    Returns:\n        Bridge: Self for method chaining.\n\n    Raises:\n        ValueError: If fewer than 2 media items are in the queue.\n\n    Examples:\n        Generate default transition:\n        &gt;&gt;&gt; bridge = (Bridge()\n        ...     .add_media([\"scene1.mp4\", \"scene2.mp4\"])\n        ...     .generate_transition())\n\n        Custom transition prompt:\n        &gt;&gt;&gt; bridge = (Bridge()\n        ...     .generate(\"Day scene\")\n        ...     .generate(\"Night scene\")\n        ...     .generate_transition(\"Gradual sunset transition\"))\n\n    Note:\n        The transition video is inserted between the last two media items,\n        creating a sequence like: [media_a, transition, media_b, ...]\n    \"\"\"\n    if len(self.media_queue) &lt; 2:\n        raise ValueError(\"Need at least 2 media items to create transition\")\n\n    media_a = self.media_queue[-2]\n    media_b = self.media_queue[-1]\n\n    if not prompt:\n        prompt = \"smooth cinematic transition between scenes\"\n\n    step = self.workflow.add_step(\"generate_transition\", {\n        \"media_a\": str(media_a),\n        \"media_b\": str(media_b),\n        \"prompt\": prompt,\n        \"model\": model\n    })\n\n    result = generate_from_video(\n        media_a,\n        prompt,\n        extract_at=-0.5,\n        model=model,\n        on_progress=self._on_progress\n    )\n\n    step.result = result\n    self.results.append(result)\n\n    if result.path:\n        self.media_queue.insert(-1, result.path)\n\n    return self\n</code></pre>"},{"location":"api/overview/#veotools.api.bridge.Bridge.stitch","title":"stitch","text":"<pre><code>stitch(overlap: float = 1.0) -&gt; Bridge\n</code></pre> <p>Stitch all videos in the queue into a single continuous video.</p> <p>Combines all video files in the media queue into one seamless video. Non-video files (images) are automatically filtered out. The result replaces the entire media queue.</p> PARAMETER DESCRIPTION <code>overlap</code> <p>Duration in seconds to trim from the end of each video (except the last) for smooth transitions. Defaults to 1.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> RETURNS DESCRIPTION <code>Bridge</code> <p>Self for method chaining.</p> <p> TYPE: <code>Bridge</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If fewer than 2 videos are available for stitching.</p> <p>Examples:</p> <p>Stitch with default overlap:</p> <pre><code>&gt;&gt;&gt; bridge = (Bridge()\n...     .generate(\"Scene 1\")\n...     .generate(\"Scene 2\")\n...     .generate(\"Scene 3\")\n...     .stitch())\n</code></pre> <p>Stitch without overlap:</p> <pre><code>&gt;&gt;&gt; bridge = bridge.stitch(overlap=0.0)\n</code></pre> <p>Stitch with longer transitions:</p> <pre><code>&gt;&gt;&gt; bridge = bridge.stitch(overlap=2.5)\n</code></pre> Note <p>After stitching, the media queue contains only the final stitched video.</p> Source code in <code>src\\veotools\\api\\bridge.py</code> <pre><code>def stitch(self, overlap: float = 1.0) -&gt; 'Bridge':\n    \"\"\"Stitch all videos in the queue into a single continuous video.\n\n    Combines all video files in the media queue into one seamless video.\n    Non-video files (images) are automatically filtered out. The result\n    replaces the entire media queue.\n\n    Args:\n        overlap: Duration in seconds to trim from the end of each video\n            (except the last) for smooth transitions. Defaults to 1.0.\n\n    Returns:\n        Bridge: Self for method chaining.\n\n    Raises:\n        ValueError: If fewer than 2 videos are available for stitching.\n\n    Examples:\n        Stitch with default overlap:\n        &gt;&gt;&gt; bridge = (Bridge()\n        ...     .generate(\"Scene 1\")\n        ...     .generate(\"Scene 2\")\n        ...     .generate(\"Scene 3\")\n        ...     .stitch())\n\n        Stitch without overlap:\n        &gt;&gt;&gt; bridge = bridge.stitch(overlap=0.0)\n\n        Stitch with longer transitions:\n        &gt;&gt;&gt; bridge = bridge.stitch(overlap=2.5)\n\n    Note:\n        After stitching, the media queue contains only the final stitched video.\n    \"\"\"\n    if len(self.media_queue) &lt; 2:\n        raise ValueError(\"Need at least 2 videos to stitch\")\n\n    video_paths = [\n        p for p in self.media_queue \n        if p.suffix.lower() in ['.mp4', '.avi', '.mov', '.mkv']\n    ]\n\n    if len(video_paths) &lt; 2:\n        raise ValueError(\"Need at least 2 videos to stitch\")\n\n    step = self.workflow.add_step(\"stitch\", {\n        \"videos\": [str(p) for p in video_paths],\n        \"overlap\": overlap\n    })\n\n    result = stitch_videos(\n        video_paths,\n        overlap=overlap,\n        on_progress=self._on_progress\n    )\n\n    step.result = result\n    self.results.append(result)\n\n    if result.path:\n        self.media_queue = [result.path]\n\n    return self\n</code></pre>"},{"location":"api/overview/#veotools.api.bridge.Bridge.save","title":"save","text":"<pre><code>save(output_path: Optional[Union[str, Path]] = None) -&gt; Path\n</code></pre> <p>Save the final result to a specified path or return the current path.</p> <p>Saves the most recent media file in the queue to the specified output path, or returns the current path if no output path is provided.</p> PARAMETER DESCRIPTION <code>output_path</code> <p>Optional destination path. If provided, copies the current result to this location. If None, returns the current file path.</p> <p> TYPE: <code>Optional[Union[str, Path]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Path</code> <p>The path where the final result is located.</p> <p> TYPE: <code>Path</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If no media is available to save.</p> <p>Examples:</p> <p>Save to custom location:</p> <pre><code>&gt;&gt;&gt; final_path = bridge.save(\"my_video.mp4\")\n&gt;&gt;&gt; print(f\"Video saved to: {final_path}\")\n</code></pre> <p>Get current result path:</p> <pre><code>&gt;&gt;&gt; current_path = bridge.save()\n&gt;&gt;&gt; print(f\"Current result: {current_path}\")\n</code></pre> <p>Save with Path object:</p> <pre><code>&gt;&gt;&gt; output_dir = Path(\"outputs\")\n&gt;&gt;&gt; final_path = bridge.save(output_dir / \"final_video.mp4\")\n</code></pre> Source code in <code>src\\veotools\\api\\bridge.py</code> <pre><code>def save(self, output_path: Optional[Union[str, Path]] = None) -&gt; Path:\n    \"\"\"Save the final result to a specified path or return the current path.\n\n    Saves the most recent media file in the queue to the specified output path,\n    or returns the current path if no output path is provided.\n\n    Args:\n        output_path: Optional destination path. If provided, copies the current\n            result to this location. If None, returns the current file path.\n\n    Returns:\n        Path: The path where the final result is located.\n\n    Raises:\n        ValueError: If no media is available to save.\n\n    Examples:\n        Save to custom location:\n        &gt;&gt;&gt; final_path = bridge.save(\"my_video.mp4\")\n        &gt;&gt;&gt; print(f\"Video saved to: {final_path}\")\n\n        Get current result path:\n        &gt;&gt;&gt; current_path = bridge.save()\n        &gt;&gt;&gt; print(f\"Current result: {current_path}\")\n\n        Save with Path object:\n        &gt;&gt;&gt; output_dir = Path(\"outputs\")\n        &gt;&gt;&gt; final_path = bridge.save(output_dir / \"final_video.mp4\")\n    \"\"\"\n    if not self.media_queue:\n        raise ValueError(\"No media to save\")\n\n    last_media = self.media_queue[-1]\n\n    if output_path:\n        output_path = Path(output_path)\n        import shutil\n        shutil.copy2(last_media, output_path)\n        return output_path\n\n    return last_media\n</code></pre>"},{"location":"api/overview/#veotools.api.bridge.Bridge.get_workflow","title":"get_workflow","text":"<pre><code>get_workflow() -&gt; Workflow\n</code></pre> <p>Get the workflow object containing all performed operations.</p> RETURNS DESCRIPTION <code>Workflow</code> <p>The workflow tracking all operations and their parameters.</p> <p> TYPE: <code>Workflow</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; bridge = Bridge(\"project\").generate(\"A scene\").stitch()\n&gt;&gt;&gt; workflow = bridge.get_workflow()\n&gt;&gt;&gt; print(workflow.name)\n</code></pre> Source code in <code>src\\veotools\\api\\bridge.py</code> <pre><code>def get_workflow(self) -&gt; Workflow:\n    \"\"\"Get the workflow object containing all performed operations.\n\n    Returns:\n        Workflow: The workflow tracking all operations and their parameters.\n\n    Examples:\n        &gt;&gt;&gt; bridge = Bridge(\"project\").generate(\"A scene\").stitch()\n        &gt;&gt;&gt; workflow = bridge.get_workflow()\n        &gt;&gt;&gt; print(workflow.name)\n    \"\"\"\n    return self.workflow\n</code></pre>"},{"location":"api/overview/#veotools.api.bridge.Bridge.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict\n</code></pre> <p>Convert the workflow to a dictionary representation.</p> RETURNS DESCRIPTION <code>dict</code> <p>Dictionary containing workflow steps and metadata.</p> <p> TYPE: <code>dict</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; bridge = Bridge(\"test\").generate(\"Scene\")\n&gt;&gt;&gt; workflow_dict = bridge.to_dict()\n&gt;&gt;&gt; print(workflow_dict.keys())\n</code></pre> Source code in <code>src\\veotools\\api\\bridge.py</code> <pre><code>def to_dict(self) -&gt; dict:\n    \"\"\"Convert the workflow to a dictionary representation.\n\n    Returns:\n        dict: Dictionary containing workflow steps and metadata.\n\n    Examples:\n        &gt;&gt;&gt; bridge = Bridge(\"test\").generate(\"Scene\")\n        &gt;&gt;&gt; workflow_dict = bridge.to_dict()\n        &gt;&gt;&gt; print(workflow_dict.keys())\n    \"\"\"\n    return self.workflow.to_dict()\n</code></pre>"},{"location":"api/overview/#veotools.api.bridge.Bridge.clear","title":"clear","text":"<pre><code>clear() -&gt; Bridge\n</code></pre> <p>Clear the media queue, removing all queued media files.</p> RETURNS DESCRIPTION <code>Bridge</code> <p>Self for method chaining.</p> <p> TYPE: <code>Bridge</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; bridge = Bridge().add_media([\"a.mp4\", \"b.mp4\"]).clear()\n&gt;&gt;&gt; # Media queue is now empty\n</code></pre> Source code in <code>src\\veotools\\api\\bridge.py</code> <pre><code>def clear(self) -&gt; 'Bridge':\n    \"\"\"Clear the media queue, removing all queued media files.\n\n    Returns:\n        Bridge: Self for method chaining.\n\n    Examples:\n        &gt;&gt;&gt; bridge = Bridge().add_media([\"a.mp4\", \"b.mp4\"]).clear()\n        &gt;&gt;&gt; # Media queue is now empty\n    \"\"\"\n    self.media_queue.clear()\n    return self\n</code></pre>"},{"location":"api/overview/#veotools.api.bridge-functions","title":"Functions","text":""},{"location":"api/overview/#mcp-api-module","title":"MCP API Module","text":""},{"location":"api/overview/#veotools.api.mcp_api","title":"veotools.api.mcp_api","text":"<p>MCP-friendly API wrappers for Veo Tools.</p> <p>This module exposes small, deterministic, JSON-first functions intended for use in Model Context Protocol (MCP) servers. It builds on top of the existing blocking SDK functions by providing a non-blocking job lifecycle:</p> <ul> <li>generate_start(params) -&gt; submits a generation job and returns immediately</li> <li>generate_get(job_id) -&gt; fetches job status/progress/result</li> <li>generate_cancel(job_id) -&gt; requests cancellation for a running job</li> </ul> <p>It also provides environment/system helpers: - preflight() -&gt; checks API key, ffmpeg, and filesystem permissions - version() -&gt; returns package and key dependency versions</p> <p>Design notes: - Jobs are persisted as JSON files under StorageManager's base directory   (\"output/ops\"). This allows stateless MCP handlers to inspect progress   and results across processes. - A background thread runs the blocking generation call and updates job state   via the JobStore. Cancellation is cooperative: the on_progress callback   checks a cancel flag in the persisted job state and raises Cancelled.</p> FUNCTION DESCRIPTION <code>preflight</code> <p>Check environment and system prerequisites for video generation.</p> <code>version</code> <p>Report package and dependency versions in a JSON-friendly format.</p> <code>generate_start</code> <p>Start a video generation job and return immediately with job details.</p> <code>generate_get</code> <p>Get the current status and results of a generation job.</p> <code>generate_cancel</code> <p>Request cancellation of a running generation job.</p> <code>list_models</code> <p>List available video generation models with their capabilities.</p> <code>cache_create_from_files</code> <p>Create a cached content handle from local file paths.</p> <code>cache_get</code> <p>Retrieve cached content metadata by cache name.</p> <code>cache_list</code> <p>List all cached content entries with their metadata.</p> <code>cache_update</code> <p>Update TTL or expiration time for a cached content entry.</p> <code>cache_delete</code> <p>Delete a cached content entry by name.</p> <code>plan_scenes</code> <p>Generate a structured Gemini-authored scene plan.</p>"},{"location":"api/overview/#veotools.api.mcp_api-classes","title":"Classes","text":""},{"location":"api/overview/#veotools.api.mcp_api.Cancelled","title":"Cancelled","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised to signal cooperative cancellation of a generation job.</p> <p>This exception is raised internally when a job's cancel_requested flag is set to True, allowing for graceful termination of long-running operations.</p>"},{"location":"api/overview/#veotools.api.mcp_api.JobRecord","title":"JobRecord  <code>dataclass</code>","text":"<pre><code>JobRecord(job_id: str, status: str, progress: int, message: str, created_at: float, updated_at: float, cancel_requested: bool, kind: str, params: Dict[str, Any], result: Optional[Dict[str, Any]] = None, error_code: Optional[str] = None, error_message: Optional[str] = None, remote_operation_id: Optional[str] = None)\n</code></pre> <p>Data class representing a generation job's state and metadata.</p> <p>Stores all information about a generation job including status, progress, parameters, results, and error information. Used for job persistence and state management across processes.</p> ATTRIBUTE DESCRIPTION <code>job_id</code> <p>Unique identifier for the job.</p> <p> TYPE: <code>str</code> </p> <code>status</code> <p>Current job status (pending|processing|complete|failed|cancelled).</p> <p> TYPE: <code>str</code> </p> <code>progress</code> <p>Progress percentage (0-100).</p> <p> TYPE: <code>int</code> </p> <code>message</code> <p>Current status message.</p> <p> TYPE: <code>str</code> </p> <code>created_at</code> <p>Unix timestamp when job was created.</p> <p> TYPE: <code>float</code> </p> <code>updated_at</code> <p>Unix timestamp of last update.</p> <p> TYPE: <code>float</code> </p> <code>cancel_requested</code> <p>Whether cancellation has been requested.</p> <p> TYPE: <code>bool</code> </p> <code>kind</code> <p>Generation type (text|image|video).</p> <p> TYPE: <code>str</code> </p> <code>params</code> <p>Dictionary of generation parameters.</p> <p> TYPE: <code>Dict[str, Any]</code> </p> <code>result</code> <p>Optional result data when job completes.</p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> </p> <code>error_code</code> <p>Optional error code if job fails.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>error_message</code> <p>Optional error description if job fails.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>remote_operation_id</code> <p>Optional ID from the remote API operation.</p> <p> TYPE: <code>Optional[str]</code> </p> METHOD DESCRIPTION <code>to_json</code> <p>Convert the job record to JSON string representation.</p>"},{"location":"api/overview/#veotools.api.mcp_api.JobRecord-functions","title":"Functions","text":""},{"location":"api/overview/#veotools.api.mcp_api.JobRecord.to_json","title":"to_json","text":"<pre><code>to_json() -&gt; str\n</code></pre> <p>Convert the job record to JSON string representation.</p> RETURNS DESCRIPTION <code>str</code> <p>JSON string representation of the job record.</p> <p> TYPE: <code>str</code> </p> Source code in <code>src\\veotools\\api\\mcp_api.py</code> <pre><code>def to_json(self) -&gt; str:\n    \"\"\"Convert the job record to JSON string representation.\n\n    Returns:\n        str: JSON string representation of the job record.\n    \"\"\"\n    return json.dumps(asdict(self), ensure_ascii=False)\n</code></pre>"},{"location":"api/overview/#veotools.api.mcp_api.JobStore","title":"JobStore","text":"<pre><code>JobStore(storage: Optional[StorageManager] = None)\n</code></pre> <p>File-based persistence layer for generation jobs.</p> <p>Manages storage and retrieval of job records using JSON files in the filesystem. Each job is stored as a separate JSON file under the <code>output/ops/{job_id}.json</code> path structure.</p> <p>This design allows stateless MCP handlers to inspect job progress and results across different processes and sessions.</p> ATTRIBUTE DESCRIPTION <code>storage</code> <p>StorageManager instance for base path management.</p> <p> </p> <code>ops_dir</code> <p>Directory path where job files are stored.</p> <p> </p> <p>Initialize the job store with optional custom storage manager.</p> PARAMETER DESCRIPTION <code>storage</code> <p>Optional StorageManager instance. If None, creates a new one.</p> <p> TYPE: <code>Optional[StorageManager]</code> DEFAULT: <code>None</code> </p> METHOD DESCRIPTION <code>create</code> <p>Create a new job record on disk.</p> <code>read</code> <p>Read a job record from disk.</p> <code>update</code> <p>Update a job record with new values and persist to disk.</p> <code>request_cancel</code> <p>Request cancellation of a job by setting the cancel flag.</p> Source code in <code>src\\veotools\\api\\mcp_api.py</code> <pre><code>def __init__(self, storage: Optional[StorageManager] = None):\n    \"\"\"Initialize the job store with optional custom storage manager.\n\n    Args:\n        storage: Optional StorageManager instance. If None, creates a new one.\n    \"\"\"\n    self.storage = storage or StorageManager()\n    self.ops_dir = self.storage.base_path / \"ops\"\n    self.ops_dir.mkdir(exist_ok=True)\n</code></pre>"},{"location":"api/overview/#veotools.api.mcp_api.JobStore-functions","title":"Functions","text":""},{"location":"api/overview/#veotools.api.mcp_api.JobStore.create","title":"create","text":"<pre><code>create(record: JobRecord) -&gt; None\n</code></pre> <p>Create a new job record on disk.</p> PARAMETER DESCRIPTION <code>record</code> <p>JobRecord instance to persist.</p> <p> TYPE: <code>JobRecord</code> </p> RAISES DESCRIPTION <code>OSError</code> <p>If file creation fails.</p> Source code in <code>src\\veotools\\api\\mcp_api.py</code> <pre><code>def create(self, record: JobRecord) -&gt; None:\n    \"\"\"Create a new job record on disk.\n\n    Args:\n        record: JobRecord instance to persist.\n\n    Raises:\n        OSError: If file creation fails.\n    \"\"\"\n    path = self._path(record.job_id)\n    path.write_text(record.to_json(), encoding=\"utf-8\")\n</code></pre>"},{"location":"api/overview/#veotools.api.mcp_api.JobStore.read","title":"read","text":"<pre><code>read(job_id: str) -&gt; Optional[JobRecord]\n</code></pre> <p>Read a job record from disk.</p> PARAMETER DESCRIPTION <code>job_id</code> <p>The unique job identifier.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>JobRecord</code> <p>The job record if found, None otherwise.</p> <p> TYPE: <code>Optional[JobRecord]</code> </p> RAISES DESCRIPTION <code>JSONDecodeError</code> <p>If the stored JSON is invalid.</p> Source code in <code>src\\veotools\\api\\mcp_api.py</code> <pre><code>def read(self, job_id: str) -&gt; Optional[JobRecord]:\n    \"\"\"Read a job record from disk.\n\n    Args:\n        job_id: The unique job identifier.\n\n    Returns:\n        JobRecord: The job record if found, None otherwise.\n\n    Raises:\n        json.JSONDecodeError: If the stored JSON is invalid.\n    \"\"\"\n    path = self._path(job_id)\n    if not path.exists():\n        return None\n    data = json.loads(path.read_text(encoding=\"utf-8\"))\n    return JobRecord(**data)\n</code></pre>"},{"location":"api/overview/#veotools.api.mcp_api.JobStore.update","title":"update","text":"<pre><code>update(record: JobRecord, **updates: Any) -&gt; JobRecord\n</code></pre> <p>Update a job record with new values and persist to disk.</p> PARAMETER DESCRIPTION <code>record</code> <p>The JobRecord instance to update.</p> <p> TYPE: <code>JobRecord</code> </p> <code>**updates</code> <p>Key-value pairs of attributes to update.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>JobRecord</code> <p>The updated job record.</p> <p> TYPE: <code>JobRecord</code> </p> RAISES DESCRIPTION <code>OSError</code> <p>If file write fails.</p> Source code in <code>src\\veotools\\api\\mcp_api.py</code> <pre><code>def update(self, record: JobRecord, **updates: Any) -&gt; JobRecord:\n    \"\"\"Update a job record with new values and persist to disk.\n\n    Args:\n        record: The JobRecord instance to update.\n        **updates: Key-value pairs of attributes to update.\n\n    Returns:\n        JobRecord: The updated job record.\n\n    Raises:\n        OSError: If file write fails.\n    \"\"\"\n    for k, v in updates.items():\n        setattr(record, k, v)\n    record.updated_at = time.time()\n    self._path(record.job_id).write_text(record.to_json(), encoding=\"utf-8\")\n    return record\n</code></pre>"},{"location":"api/overview/#veotools.api.mcp_api.JobStore.request_cancel","title":"request_cancel","text":"<pre><code>request_cancel(job_id: str) -&gt; Optional[JobRecord]\n</code></pre> <p>Request cancellation of a job by setting the cancel flag.</p> PARAMETER DESCRIPTION <code>job_id</code> <p>The unique job identifier.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>JobRecord</code> <p>Updated job record if found, None otherwise.</p> <p> TYPE: <code>Optional[JobRecord]</code> </p> RAISES DESCRIPTION <code>OSError</code> <p>If file write fails.</p> Source code in <code>src\\veotools\\api\\mcp_api.py</code> <pre><code>def request_cancel(self, job_id: str) -&gt; Optional[JobRecord]:\n    \"\"\"Request cancellation of a job by setting the cancel flag.\n\n    Args:\n        job_id: The unique job identifier.\n\n    Returns:\n        JobRecord: Updated job record if found, None otherwise.\n\n    Raises:\n        OSError: If file write fails.\n    \"\"\"\n    record = self.read(job_id)\n    if not record:\n        return None\n    record.cancel_requested = True\n    record.updated_at = time.time()\n    self._path(job_id).write_text(record.to_json(), encoding=\"utf-8\")\n    return record\n</code></pre>"},{"location":"api/overview/#veotools.api.mcp_api-functions","title":"Functions","text":""},{"location":"api/overview/#veotools.api.mcp_api.preflight","title":"preflight","text":"<pre><code>preflight() -&gt; Dict[str, Any]\n</code></pre> <p>Check environment and system prerequisites for video generation.</p> <p>Performs comprehensive system checks to ensure all required dependencies and configurations are available for successful video generation operations. This includes API key validation, FFmpeg availability, and filesystem permissions.</p> RETURNS DESCRIPTION <code>dict</code> <p>JSON-serializable dictionary containing: - ok (bool): Overall system readiness status - provider (str): Active video provider identifier - api_key_present (bool): Whether the current provider's API key is set - ffmpeg (dict): FFmpeg installation status and version info - write_permissions (bool): Whether output directory is writable - base_path (str): Absolute path to the base output directory</p> <p> TYPE: <code>Dict[str, Any]</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; status = preflight()\n&gt;&gt;&gt; if not status['ok']:\n...     print(\"System not ready for generation\")\n...     if not status['api_key_present']:\n...         print(\"Please supply the API key for the configured provider\")\n...     if not status['ffmpeg']['installed']:\n...         print(\"Please install FFmpeg for video processing\")\n&gt;&gt;&gt; else:\n...     print(f\"System ready! Output directory: {status['base_path']}\")\n</code></pre> Note <p>This function is designed to be called before starting any video generation operations to ensure the environment is properly configured.</p> Source code in <code>src\\veotools\\api\\mcp_api.py</code> <pre><code>def preflight() -&gt; Dict[str, Any]:\n    \"\"\"Check environment and system prerequisites for video generation.\n\n    Performs comprehensive system checks to ensure all required dependencies\n    and configurations are available for successful video generation operations.\n    This includes API key validation, FFmpeg availability, and filesystem permissions.\n\n    Returns:\n        dict: JSON-serializable dictionary containing:\n            - ok (bool): Overall system readiness status\n            - provider (str): Active video provider identifier\n            - api_key_present (bool): Whether the current provider's API key is set\n            - ffmpeg (dict): FFmpeg installation status and version info\n            - write_permissions (bool): Whether output directory is writable\n            - base_path (str): Absolute path to the base output directory\n\n    Examples:\n        &gt;&gt;&gt; status = preflight()\n        &gt;&gt;&gt; if not status['ok']:\n        ...     print(\"System not ready for generation\")\n        ...     if not status['api_key_present']:\n        ...         print(\"Please supply the API key for the configured provider\")\n        ...     if not status['ffmpeg']['installed']:\n        ...         print(\"Please install FFmpeg for video processing\")\n        &gt;&gt;&gt; else:\n        ...     print(f\"System ready! Output directory: {status['base_path']}\")\n\n    Note:\n        This function is designed to be called before starting any video generation\n        operations to ensure the environment is properly configured.\n    \"\"\"\n    \"\"\"Check environment and system prerequisites.\n\n    Returns a JSON-serializable dict with pass/fail details.\n    \"\"\"\n    storage = StorageManager()\n    base = storage.base_path\n\n    provider = (os.getenv(\"VEO_PROVIDER\", \"google\") or \"google\").strip().lower()\n\n    if provider == \"daydreams\":\n        api_key_present = bool(os.getenv(\"DAYDREAMS_API_KEY\"))\n    else:\n        api_key_present = bool(os.getenv(\"GEMINI_API_KEY\"))\n\n    # ffmpeg\n    ffmpeg_installed = False\n    ffmpeg_version = None\n    try:\n        res = subprocess.run([\"ffmpeg\", \"-version\"], capture_output=True, text=True)\n        if res.returncode == 0:\n            ffmpeg_installed = True\n            first_line = (res.stdout or res.stderr).splitlines()[0] if (res.stdout or res.stderr) else \"\"\n            ffmpeg_version = first_line.strip()\n    except FileNotFoundError:\n        ffmpeg_installed = False\n\n    # write permissions\n    write_permissions = False\n    try:\n        base.mkdir(exist_ok=True)\n        test_file = base / \".write_test\"\n        test_file.write_text(\"ok\", encoding=\"utf-8\")\n        test_file.unlink()\n        write_permissions = True\n    except Exception:\n        write_permissions = False\n\n    return {\n        \"ok\": api_key_present and write_permissions,\n        \"provider\": provider,\n        \"api_key_present\": api_key_present,\n        \"ffmpeg\": {\"installed\": ffmpeg_installed, \"version\": ffmpeg_version},\n        \"write_permissions\": write_permissions,\n        \"base_path\": str(base.resolve()),\n    }\n</code></pre>"},{"location":"api/overview/#veotools.api.mcp_api.version","title":"version","text":"<pre><code>version() -&gt; Dict[str, Any]\n</code></pre> <p>Report package and dependency versions in a JSON-friendly format.</p> <p>Collects version information for veotools and its key dependencies, providing a comprehensive overview of the current software environment. Useful for debugging and support purposes.</p> RETURNS DESCRIPTION <code>dict</code> <p>Dictionary containing: - veotools (str|None): veotools package version - dependencies (dict): Versions of key Python packages:     - google-genai: Google GenerativeAI library version     - opencv-python: OpenCV library version     - requests: HTTP requests library version     - python-dotenv: Environment file loader version - ffmpeg (str|None): FFmpeg version string if available</p> <p> TYPE: <code>Dict[str, Any]</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; versions = version()\n&gt;&gt;&gt; print(f\"veotools: {versions['veotools']}\")\n&gt;&gt;&gt; print(f\"Google GenAI: {versions['dependencies']['google-genai']}\")\n&gt;&gt;&gt; if versions['ffmpeg']:\n...     print(f\"FFmpeg: {versions['ffmpeg']}\")\n&gt;&gt;&gt; else:\n...     print(\"FFmpeg not available\")\n</code></pre> Note <p>Returns None for any package that cannot be found or queried. This is expected behavior and not an error condition.</p> Source code in <code>src\\veotools\\api\\mcp_api.py</code> <pre><code>def version() -&gt; Dict[str, Any]:\n    \"\"\"Report package and dependency versions in a JSON-friendly format.\n\n    Collects version information for veotools and its key dependencies,\n    providing a comprehensive overview of the current software environment.\n    Useful for debugging and support purposes.\n\n    Returns:\n        dict: Dictionary containing:\n            - veotools (str|None): veotools package version\n            - dependencies (dict): Versions of key Python packages:\n                - google-genai: Google GenerativeAI library version\n                - opencv-python: OpenCV library version  \n                - requests: HTTP requests library version\n                - python-dotenv: Environment file loader version\n            - ffmpeg (str|None): FFmpeg version string if available\n\n    Examples:\n        &gt;&gt;&gt; versions = version()\n        &gt;&gt;&gt; print(f\"veotools: {versions['veotools']}\")\n        &gt;&gt;&gt; print(f\"Google GenAI: {versions['dependencies']['google-genai']}\")\n        &gt;&gt;&gt; if versions['ffmpeg']:\n        ...     print(f\"FFmpeg: {versions['ffmpeg']}\")\n        &gt;&gt;&gt; else:\n        ...     print(\"FFmpeg not available\")\n\n    Note:\n        Returns None for any package that cannot be found or queried.\n        This is expected behavior and not an error condition.\n    \"\"\"\n    \"\"\"Report package and dependency versions in a JSON-friendly format.\"\"\"\n    from importlib.metadata import PackageNotFoundError, version as pkg_version\n    import veotools as veo\n\n    def safe_ver(name: str) -&gt; Optional[str]:\n        try:\n            return pkg_version(name)\n        except PackageNotFoundError:\n            return None\n        except Exception:\n            return None\n\n    ffmpeg_info = None\n    try:\n        res = subprocess.run([\"ffmpeg\", \"-version\"], capture_output=True, text=True)\n        if res.returncode == 0:\n            ffmpeg_info = (res.stdout or res.stderr).splitlines()[0].strip()\n    except Exception:\n        ffmpeg_info = None\n\n    return {\n        \"veotools\": getattr(veo, \"__version__\", None),\n        \"dependencies\": {\n            \"google-genai\": safe_ver(\"google-genai\"),\n            \"opencv-python\": safe_ver(\"opencv-python\"),\n            \"requests\": safe_ver(\"requests\"),\n            \"python-dotenv\": safe_ver(\"python-dotenv\"),\n        },\n        \"ffmpeg\": ffmpeg_info,\n    }\n</code></pre>"},{"location":"api/overview/#veotools.api.mcp_api.generate_start","title":"generate_start","text":"<pre><code>generate_start(params: Dict[str, Any]) -&gt; Dict[str, Any]\n</code></pre> <p>Start a video generation job and return immediately with job details.</p> <p>Initiates a video generation job in the background and returns immediately with job tracking information. The actual generation runs asynchronously and can be monitored using generate_get().</p> PARAMETER DESCRIPTION <code>params</code> <p>Generation parameters dictionary containing: - prompt (str): Required text description for generation - model (str, optional): Model to use (defaults to veo-3.0-fast-generate-preview) - input_image_path (str, optional): Path to input image for image-to-video - input_video_path (str, optional): Path to input video for continuation - extract_at (float, optional): Time offset for video continuation - options (dict, optional): Additional model-specific options</p> <p> TYPE: <code>Dict[str, Any]</code> </p> RETURNS DESCRIPTION <code>dict</code> <p>Job information containing: - job_id (str): Unique job identifier for tracking - status (str): Initial job status (\"processing\") - progress (int): Initial progress (0) - message (str): Status message - kind (str): Generation type (text|image|video) - created_at (float): Job creation timestamp</p> <p> TYPE: <code>Dict[str, Any]</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If required parameters are missing or invalid.</p> <code>FileNotFoundError</code> <p>If input media files don't exist.</p> <p>Examples:</p> <p>Start text-to-video generation:</p> <pre><code>&gt;&gt;&gt; job = generate_start({\"prompt\": \"A sunset over mountains\"})\n&gt;&gt;&gt; print(f\"Job started: {job['job_id']}\")\n</code></pre> <p>Start image-to-video generation:</p> <pre><code>&gt;&gt;&gt; job = generate_start({\n...     \"prompt\": \"The person starts walking\",\n...     \"input_image_path\": \"photo.jpg\"\n... })\n</code></pre> <p>Start video continuation:</p> <pre><code>&gt;&gt;&gt; job = generate_start({\n...     \"prompt\": \"The action continues\",\n...     \"input_video_path\": \"scene1.mp4\",\n...     \"extract_at\": -2.0\n... })\n</code></pre> <p>Start with custom model and options:</p> <pre><code>&gt;&gt;&gt; job = generate_start({\n...     \"prompt\": \"A dancing robot\",\n...     \"model\": \"veo-2.0\",\n...     \"options\": {\"duration_seconds\": 10, \"enhance\": True}\n... })\n</code></pre> Note <p>The job runs in a background thread. Use generate_get() to check progress and retrieve results when complete.</p> Source code in <code>src\\veotools\\api\\mcp_api.py</code> <pre><code>def generate_start(params: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"Start a video generation job and return immediately with job details.\n\n    Initiates a video generation job in the background and returns immediately\n    with job tracking information. The actual generation runs asynchronously\n    and can be monitored using generate_get().\n\n    Args:\n        params: Generation parameters dictionary containing:\n            - prompt (str): Required text description for generation\n            - model (str, optional): Model to use (defaults to veo-3.0-fast-generate-preview)\n            - input_image_path (str, optional): Path to input image for image-to-video\n            - input_video_path (str, optional): Path to input video for continuation\n            - extract_at (float, optional): Time offset for video continuation\n            - options (dict, optional): Additional model-specific options\n\n    Returns:\n        dict: Job information containing:\n            - job_id (str): Unique job identifier for tracking\n            - status (str): Initial job status (\"processing\")\n            - progress (int): Initial progress (0)\n            - message (str): Status message\n            - kind (str): Generation type (text|image|video)\n            - created_at (float): Job creation timestamp\n\n    Raises:\n        ValueError: If required parameters are missing or invalid.\n        FileNotFoundError: If input media files don't exist.\n\n    Examples:\n        Start text-to-video generation:\n        &gt;&gt;&gt; job = generate_start({\"prompt\": \"A sunset over mountains\"})\n        &gt;&gt;&gt; print(f\"Job started: {job['job_id']}\")\n\n        Start image-to-video generation:\n        &gt;&gt;&gt; job = generate_start({\n        ...     \"prompt\": \"The person starts walking\",\n        ...     \"input_image_path\": \"photo.jpg\"\n        ... })\n\n        Start video continuation:\n        &gt;&gt;&gt; job = generate_start({\n        ...     \"prompt\": \"The action continues\",\n        ...     \"input_video_path\": \"scene1.mp4\",\n        ...     \"extract_at\": -2.0\n        ... })\n\n        Start with custom model and options:\n        &gt;&gt;&gt; job = generate_start({\n        ...     \"prompt\": \"A dancing robot\",\n        ...     \"model\": \"veo-2.0\",\n        ...     \"options\": {\"duration_seconds\": 10, \"enhance\": True}\n        ... })\n\n    Note:\n        The job runs in a background thread. Use generate_get() to check\n        progress and retrieve results when complete.\n    \"\"\"\n    \"\"\"Start a generation job and return immediately.\n\n    Expected params keys:\n      - prompt: str (required)\n      - model: str (optional; default used by underlying SDK)\n      - input_image_path: str (optional)\n      - input_video_path: str (optional)\n      - extract_at: float (optional; for video continuation)\n      - options: dict (optional; forwarded to SDK functions)\n    \"\"\"\n    _validate_generate_inputs(params)\n\n    kind = \"text\"\n    if params.get(\"input_image_path\"):\n        kind = \"image\"\n    elif params.get(\"input_video_path\"):\n        kind = \"video\"\n\n    store = JobStore()\n    record = _build_job(kind, params)\n    store.create(record)\n\n    # Start background worker\n    worker = threading.Thread(target=_run_generation, args=(record.job_id,), daemon=True)\n    worker.start()\n\n    return {\n        \"job_id\": record.job_id,\n        \"status\": record.status,\n        \"progress\": record.progress,\n        \"message\": record.message,\n        \"kind\": record.kind,\n        \"created_at\": record.created_at,\n    }\n</code></pre>"},{"location":"api/overview/#veotools.api.mcp_api.generate_get","title":"generate_get","text":"<pre><code>generate_get(job_id: str) -&gt; Dict[str, Any]\n</code></pre> <p>Get the current status and results of a generation job.</p> <p>Retrieves the current state of a generation job including progress, status, and results if complete. This function can be called repeatedly to monitor job progress.</p> PARAMETER DESCRIPTION <code>job_id</code> <p>The unique job identifier returned by generate_start().</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>dict</code> <p>Job status information containing: - job_id (str): The job identifier - status (str): Current status (processing|complete|failed|cancelled) - progress (int): Progress percentage (0-100) - message (str): Current status message - kind (str): Generation type (text|image|video) - remote_operation_id (str|None): Remote API operation ID if available - updated_at (float): Last update timestamp - result (dict, optional): Generation results when status is \"complete\" - error_code (str, optional): Error code if status is \"failed\" - error_message (str, optional): Error description if status is \"failed\"</p> <p> TYPE: <code>Dict[str, Any]</code> </p> <code>Dict[str, Any]</code> <p>If job_id is not found, returns: - error_code (str): \"VALIDATION\" - error_message (str): Error description</p> <p>Examples:</p> <p>Check job progress:</p> <pre><code>&gt;&gt;&gt; status = generate_get(job_id)\n&gt;&gt;&gt; print(f\"Progress: {status['progress']}% - {status['message']}\")\n</code></pre> <p>Wait for completion:</p> <pre><code>&gt;&gt;&gt; import time\n&gt;&gt;&gt; while True:\n...     status = generate_get(job_id)\n...     if status['status'] == 'complete':\n...         print(f\"Video ready: {status['result']['path']}\")\n...         break\n...     elif status['status'] == 'failed':\n...         print(f\"Generation failed: {status['error_message']}\")\n...         break\n...     time.sleep(5)\n</code></pre> <p>Handle different outcomes:</p> <pre><code>&gt;&gt;&gt; status = generate_get(job_id)\n&gt;&gt;&gt; if status['status'] == 'complete':\n...     video_path = status['result']['path']\n...     metadata = status['result']['metadata']\n...     print(f\"Success! Video: {video_path}\")\n...     print(f\"Duration: {metadata['duration']}s\")\n... elif status['status'] == 'failed':\n...     print(f\"Error ({status['error_code']}): {status['error_message']}\")\n... else:\n...     print(f\"Still processing: {status['progress']}%\")\n</code></pre> Source code in <code>src\\veotools\\api\\mcp_api.py</code> <pre><code>def generate_get(job_id: str) -&gt; Dict[str, Any]:\n    \"\"\"Get the current status and results of a generation job.\n\n    Retrieves the current state of a generation job including progress,\n    status, and results if complete. This function can be called repeatedly\n    to monitor job progress.\n\n    Args:\n        job_id: The unique job identifier returned by generate_start().\n\n    Returns:\n        dict: Job status information containing:\n            - job_id (str): The job identifier\n            - status (str): Current status (processing|complete|failed|cancelled)\n            - progress (int): Progress percentage (0-100)\n            - message (str): Current status message\n            - kind (str): Generation type (text|image|video)\n            - remote_operation_id (str|None): Remote API operation ID if available\n            - updated_at (float): Last update timestamp\n            - result (dict, optional): Generation results when status is \"complete\"\n            - error_code (str, optional): Error code if status is \"failed\"\n            - error_message (str, optional): Error description if status is \"failed\"\n\n        If job_id is not found, returns:\n            - error_code (str): \"VALIDATION\"\n            - error_message (str): Error description\n\n    Examples:\n        Check job progress:\n        &gt;&gt;&gt; status = generate_get(job_id)\n        &gt;&gt;&gt; print(f\"Progress: {status['progress']}% - {status['message']}\")\n\n        Wait for completion:\n        &gt;&gt;&gt; import time\n        &gt;&gt;&gt; while True:\n        ...     status = generate_get(job_id)\n        ...     if status['status'] == 'complete':\n        ...         print(f\"Video ready: {status['result']['path']}\")\n        ...         break\n        ...     elif status['status'] == 'failed':\n        ...         print(f\"Generation failed: {status['error_message']}\")\n        ...         break\n        ...     time.sleep(5)\n\n        Handle different outcomes:\n        &gt;&gt;&gt; status = generate_get(job_id)\n        &gt;&gt;&gt; if status['status'] == 'complete':\n        ...     video_path = status['result']['path']\n        ...     metadata = status['result']['metadata']\n        ...     print(f\"Success! Video: {video_path}\")\n        ...     print(f\"Duration: {metadata['duration']}s\")\n        ... elif status['status'] == 'failed':\n        ...     print(f\"Error ({status['error_code']}): {status['error_message']}\")\n        ... else:\n        ...     print(f\"Still processing: {status['progress']}%\")\n    \"\"\"\n    \"\"\"Get the current status of a generation job.\"\"\"\n    store = JobStore()\n    record = store.read(job_id)\n    if not record:\n        return {\"error_code\": \"VALIDATION\", \"error_message\": f\"job_id not found: {job_id}\"}\n\n    payload: Dict[str, Any] = {\n        \"job_id\": record.job_id,\n        \"status\": record.status,\n        \"progress\": record.progress,\n        \"message\": record.message,\n        \"kind\": record.kind,\n        \"remote_operation_id\": record.remote_operation_id,\n        \"updated_at\": record.updated_at,\n    }\n    if record.result:\n        payload[\"result\"] = record.result\n    if record.error_code:\n        payload[\"error_code\"] = record.error_code\n        payload[\"error_message\"] = record.error_message\n    return payload\n</code></pre>"},{"location":"api/overview/#veotools.api.mcp_api.generate_cancel","title":"generate_cancel","text":"<pre><code>generate_cancel(job_id: str) -&gt; Dict[str, Any]\n</code></pre> <p>Request cancellation of a running generation job.</p> <p>Attempts to cancel a generation job that is currently processing. Cancellation is cooperative - the job will stop at the next progress update checkpoint. Already completed or failed jobs cannot be cancelled.</p> PARAMETER DESCRIPTION <code>job_id</code> <p>The unique job identifier to cancel.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>dict</code> <p>Cancellation response containing: - job_id (str): The job identifier - status (str): \"cancelling\" if request was accepted</p> <p> TYPE: <code>Dict[str, Any]</code> </p> <code>Dict[str, Any]</code> <p>If job_id is not found, returns: - error_code (str): \"VALIDATION\" - error_message (str): Error description</p> <p>Examples:</p> <p>Cancel a running job:</p> <pre><code>&gt;&gt;&gt; response = generate_cancel(job_id)\n&gt;&gt;&gt; if 'error_code' not in response:\n...     print(f\"Cancellation requested for job {response['job_id']}\")\n... else:\n...     print(f\"Cancel failed: {response['error_message']}\")\n</code></pre> <p>Check if cancellation succeeded:</p> <pre><code>&gt;&gt;&gt; generate_cancel(job_id)\n&gt;&gt;&gt; time.sleep(2)\n&gt;&gt;&gt; status = generate_get(job_id)\n&gt;&gt;&gt; if status['status'] == 'cancelled':\n...     print(\"Job successfully cancelled\")\n</code></pre> Note <p>Cancellation may not be immediate - the job will stop at the next progress checkpoint. Monitor with generate_get() to confirm cancellation.</p> Source code in <code>src\\veotools\\api\\mcp_api.py</code> <pre><code>def generate_cancel(job_id: str) -&gt; Dict[str, Any]:\n    \"\"\"Request cancellation of a running generation job.\n\n    Attempts to cancel a generation job that is currently processing.\n    Cancellation is cooperative - the job will stop at the next progress\n    update checkpoint. Already completed or failed jobs cannot be cancelled.\n\n    Args:\n        job_id: The unique job identifier to cancel.\n\n    Returns:\n        dict: Cancellation response containing:\n            - job_id (str): The job identifier\n            - status (str): \"cancelling\" if request was accepted\n\n        If job_id is not found, returns:\n            - error_code (str): \"VALIDATION\"\n            - error_message (str): Error description\n\n    Examples:\n        Cancel a running job:\n        &gt;&gt;&gt; response = generate_cancel(job_id)\n        &gt;&gt;&gt; if 'error_code' not in response:\n        ...     print(f\"Cancellation requested for job {response['job_id']}\")\n        ... else:\n        ...     print(f\"Cancel failed: {response['error_message']}\")\n\n        Check if cancellation succeeded:\n        &gt;&gt;&gt; generate_cancel(job_id)\n        &gt;&gt;&gt; time.sleep(2)\n        &gt;&gt;&gt; status = generate_get(job_id)\n        &gt;&gt;&gt; if status['status'] == 'cancelled':\n        ...     print(\"Job successfully cancelled\")\n\n    Note:\n        Cancellation may not be immediate - the job will stop at the next\n        progress checkpoint. Monitor with generate_get() to confirm cancellation.\n    \"\"\"\n    \"\"\"Request cancellation of a running generation job.\"\"\"\n    store = JobStore()\n    record = store.request_cancel(job_id)\n    if not record:\n        return {\"error_code\": \"VALIDATION\", \"error_message\": f\"job_id not found: {job_id}\"}\n    return {\"job_id\": job_id, \"status\": \"cancelling\"}\n</code></pre>"},{"location":"api/overview/#veotools.api.mcp_api.list_models","title":"list_models","text":"<pre><code>list_models(include_remote: bool = True) -&gt; Dict[str, Any]\n</code></pre> <p>List available video generation models with their capabilities.</p> <p>Retrieves information about available Veo models including their capabilities, default settings, and performance characteristics. Combines static model registry with optional remote model discovery.</p> PARAMETER DESCRIPTION <code>include_remote</code> <p>Whether to include models discovered from the remote API. If True, attempts to fetch additional model information from Google's API. If False, returns only the static model registry. Defaults to True.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>dict</code> <p>Model information containing: - models (list): List of model dictionaries, each containing:     - id (str): Model identifier (e.g., \"veo-3.0-fast-generate-preview\")     - name (str): Human-readable model name     - capabilities (dict): Feature flags:         - supports_duration (bool): Can specify custom duration         - supports_enhance (bool): Can enhance prompts         - supports_fps (bool): Can specify frame rate         - supports_audio (bool): Can generate audio     - default_duration (float|None): Default video duration in seconds     - generation_time (float|None): Estimated generation time in seconds     - source (str): Data source (\"static\", \"remote\", or \"static+remote\")</p> <p> TYPE: <code>Dict[str, Any]</code> </p> <p>Examples:</p> <p>List all available models:</p> <pre><code>&gt;&gt;&gt; models = list_models()\n&gt;&gt;&gt; for model in models['models']:\n...     print(f\"{model['name']} ({model['id']})\")\n...     if model['capabilities']['supports_duration']:\n...         print(f\"  Default duration: {model['default_duration']}s\")\n</code></pre> <p>Find models with specific capabilities:</p> <pre><code>&gt;&gt;&gt; models = list_models()\n&gt;&gt;&gt; audio_models = [\n...     m for m in models['models']\n...     if m['capabilities']['supports_audio']\n... ]\n&gt;&gt;&gt; print(f\"Found {len(audio_models)} models with audio support\")\n</code></pre> <p>Use only static model registry:</p> <pre><code>&gt;&gt;&gt; models = list_models(include_remote=False)\n&gt;&gt;&gt; static_models = [m for m in models['models'] if m['source'] == 'static']\n</code></pre> Note <p>Results are cached for 10 minutes to improve performance. Remote model discovery failures are silently ignored - static registry is always available.</p> Source code in <code>src\\veotools\\api\\mcp_api.py</code> <pre><code>def list_models(include_remote: bool = True) -&gt; Dict[str, Any]:\n    \"\"\"List available video generation models with their capabilities.\n\n    Retrieves information about available Veo models including their capabilities,\n    default settings, and performance characteristics. Combines static model\n    registry with optional remote model discovery.\n\n    Args:\n        include_remote: Whether to include models discovered from the remote API.\n            If True, attempts to fetch additional model information from Google's API.\n            If False, returns only the static model registry. Defaults to True.\n\n    Returns:\n        dict: Model information containing:\n            - models (list): List of model dictionaries, each containing:\n                - id (str): Model identifier (e.g., \"veo-3.0-fast-generate-preview\")\n                - name (str): Human-readable model name\n                - capabilities (dict): Feature flags:\n                    - supports_duration (bool): Can specify custom duration\n                    - supports_enhance (bool): Can enhance prompts\n                    - supports_fps (bool): Can specify frame rate\n                    - supports_audio (bool): Can generate audio\n                - default_duration (float|None): Default video duration in seconds\n                - generation_time (float|None): Estimated generation time in seconds\n                - source (str): Data source (\"static\", \"remote\", or \"static+remote\")\n\n    Examples:\n        List all available models:\n        &gt;&gt;&gt; models = list_models()\n        &gt;&gt;&gt; for model in models['models']:\n        ...     print(f\"{model['name']} ({model['id']})\")\n        ...     if model['capabilities']['supports_duration']:\n        ...         print(f\"  Default duration: {model['default_duration']}s\")\n\n        Find models with specific capabilities:\n        &gt;&gt;&gt; models = list_models()\n        &gt;&gt;&gt; audio_models = [\n        ...     m for m in models['models']\n        ...     if m['capabilities']['supports_audio']\n        ... ]\n        &gt;&gt;&gt; print(f\"Found {len(audio_models)} models with audio support\")\n\n        Use only static model registry:\n        &gt;&gt;&gt; models = list_models(include_remote=False)\n        &gt;&gt;&gt; static_models = [m for m in models['models'] if m['source'] == 'static']\n\n    Note:\n        Results are cached for 10 minutes to improve performance. Remote model\n        discovery failures are silently ignored - static registry is always available.\n    \"\"\"\n    \"\"\"List available models and capability flags.\n\n    Returns a JSON dict: { models: [ {id, name, capabilities, default_duration, generation_time, source} ] }\n    \"\"\"\n    models: Dict[str, Dict[str, Any]] = {}\n\n    # Seed from static registry\n    for model_id, cfg in ModelConfig.MODELS.items():\n        models[model_id] = {\n            \"id\": model_id,\n            \"name\": cfg.get(\"name\", model_id),\n            \"capabilities\": {\n                \"supports_duration\": cfg.get(\"supports_duration\", False),\n                \"supports_enhance\": cfg.get(\"supports_enhance\", False),\n                \"supports_fps\": cfg.get(\"supports_fps\", False),\n                \"supports_audio\": cfg.get(\"supports_audio\", False),\n            },\n            \"default_duration\": cfg.get(\"default_duration\"),\n            \"generation_time\": cfg.get(\"generation_time\"),\n            \"source\": \"static\",\n        }\n\n    # Optionally merge from remote discovery (best-effort)\n    if include_remote:\n        try:\n            wrapper = VeoClient()\n            client = wrapper.client\n            provider = wrapper.provider\n\n            if provider == \"daydreams\":\n                payload = client.list_models()\n                for remote in payload.get(\"data\", []):\n                    model_id = remote.get(\"id\")\n                    if not model_id:\n                        continue\n                    entry = models.get(model_id, {\n                        \"id\": model_id,\n                        \"name\": remote.get(\"id\"),\n                        \"capabilities\": {},\n                    })\n                    caps = remote.get(\"capabilities\", {})\n                    entry[\"capabilities\"].update({\n                        \"supports_audio\": bool(caps.get(\"supportsAudio\")),\n                        \"supports_streaming\": bool(caps.get(\"supportsStreaming\")),\n                        \"supports_system_messages\": bool(caps.get(\"supportsSystemMessages\")),\n                        \"supports_json\": bool(caps.get(\"supportsJson\")),\n                    })\n                    entry[\"source\"] = (entry.get(\"source\") or \"\") + (\"+remote\" if entry.get(\"source\") else \"remote\")\n                    models[model_id] = entry\n            elif hasattr(client, \"models\") and hasattr(client.models, \"list\"):\n                for remote in client.models.list():\n                    raw_name = getattr(remote, \"name\", \"\") or \"\"\n                    model_id = raw_name.replace(\"models/\", \"\") if raw_name else getattr(remote, \"base_model_id\", None)\n                    if not model_id:\n                        continue\n                    entry = models.get(model_id, {\n                        \"id\": model_id,\n                        \"name\": getattr(remote, \"display_name\", model_id),\n                        \"capabilities\": {},\n                    })\n                    entry[\"source\"] = (entry.get(\"source\") or \"\") + (\"+remote\" if entry.get(\"source\") else \"remote\")\n                    models[model_id] = entry\n        except Exception:\n            # Ignore remote discovery errors; static list is sufficient\n            pass\n\n    # Basic cache to disk for 10 minutes\n    try:\n        store = JobStore()\n        cache_path = store.ops_dir / \"models.json\"\n        now = time.time()\n        if cache_path.exists():\n            try:\n                cached = json.loads(cache_path.read_text(encoding=\"utf-8\"))\n                if now - float(cached.get(\"updated_at\", 0)) &lt; 600:\n                    # Merge remote source flags if needed, else return cache\n                    return cached.get(\"data\", {\"models\": list(models.values())})\n            except Exception:\n                pass\n        payload = {\"models\": list(models.values())}\n        cache_path.write_text(json.dumps({\"updated_at\": now, \"data\": payload}), encoding=\"utf-8\")\n        return payload\n    except Exception:\n        return {\"models\": list(models.values())}\n</code></pre>"},{"location":"api/overview/#veotools.api.mcp_api.cache_create_from_files","title":"cache_create_from_files","text":"<pre><code>cache_create_from_files(model: str, files: list[str], system_instruction: Optional[str] = None) -&gt; Dict[str, Any]\n</code></pre> <p>Create a cached content handle from local file paths.</p> <p>Uploads local files to create a cached content context that can be reused across multiple API calls for efficiency. This is particularly useful when working with large files or when making multiple requests with the same context.</p> PARAMETER DESCRIPTION <code>model</code> <p>The model identifier to associate with the cached content.</p> <p> TYPE: <code>str</code> </p> <code>files</code> <p>List of local file paths to upload and cache.</p> <p> TYPE: <code>list[str]</code> </p> <code>system_instruction</code> <p>Optional system instruction to include with the cache.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>dict</code> <p>Cache creation result containing: - name (str): Unique cache identifier for future reference - model (str): The associated model identifier - system_instruction (str|None): The system instruction if provided - contents_count (int): Number of files successfully cached</p> <p> TYPE: <code>Dict[str, Any]</code> </p> <code>Dict[str, Any]</code> <p>On failure, returns: - error_code (str): Error classification - error_message (str): Detailed error description</p> <p>Examples:</p> <p>Cache multiple reference images:</p> <pre><code>&gt;&gt;&gt; result = cache_create_from_files(\n...     \"veo-3.0-fast-generate-preview\",\n...     [\"ref1.jpg\", \"ref2.jpg\", \"ref3.jpg\"],\n...     \"These are reference images for style consistency\"\n... )\n&gt;&gt;&gt; if 'name' in result:\n...     cache_name = result['name']\n...     print(f\"Cache created: {cache_name}\")\n... else:\n...     print(f\"Cache creation failed: {result['error_message']}\")\n</code></pre> <p>Cache video reference:</p> <pre><code>&gt;&gt;&gt; result = cache_create_from_files(\n...     \"veo-2.0\",\n...     [\"reference_video.mp4\"]\n... )\n</code></pre> Note <p>Files are uploaded to Google's servers as part of the caching process. Ensure you have appropriate permissions for the files and comply with Google's usage policies.</p> Source code in <code>src\\veotools\\api\\mcp_api.py</code> <pre><code>def cache_create_from_files(model: str, files: list[str], system_instruction: Optional[str] = None) -&gt; Dict[str, Any]:\n    \"\"\"Create a cached content handle from local file paths.\n\n    Uploads local files to create a cached content context that can be reused\n    across multiple API calls for efficiency. This is particularly useful when\n    working with large files or when making multiple requests with the same context.\n\n    Args:\n        model: The model identifier to associate with the cached content.\n        files: List of local file paths to upload and cache.\n        system_instruction: Optional system instruction to include with the cache.\n\n    Returns:\n        dict: Cache creation result containing:\n            - name (str): Unique cache identifier for future reference\n            - model (str): The associated model identifier\n            - system_instruction (str|None): The system instruction if provided\n            - contents_count (int): Number of files successfully cached\n\n        On failure, returns:\n            - error_code (str): Error classification\n            - error_message (str): Detailed error description\n\n    Examples:\n        Cache multiple reference images:\n        &gt;&gt;&gt; result = cache_create_from_files(\n        ...     \"veo-3.0-fast-generate-preview\",\n        ...     [\"ref1.jpg\", \"ref2.jpg\", \"ref3.jpg\"],\n        ...     \"These are reference images for style consistency\"\n        ... )\n        &gt;&gt;&gt; if 'name' in result:\n        ...     cache_name = result['name']\n        ...     print(f\"Cache created: {cache_name}\")\n        ... else:\n        ...     print(f\"Cache creation failed: {result['error_message']}\")\n\n        Cache video reference:\n        &gt;&gt;&gt; result = cache_create_from_files(\n        ...     \"veo-2.0\",\n        ...     [\"reference_video.mp4\"]\n        ... )\n\n    Raises:\n        The function catches all exceptions and returns them as error dictionaries\n        rather than raising them directly.\n\n    Note:\n        Files are uploaded to Google's servers as part of the caching process.\n        Ensure you have appropriate permissions for the files and comply with\n        Google's usage policies.\n    \"\"\"\n    \"\"\"Create a cached content handle from local file paths.\n\n    Returns { name, model, system_instruction?, contents_count } or { error_code, error_message } on failure.\n    \"\"\"\n    try:\n        wrapper = VeoClient()\n        if wrapper.provider != \"google\":\n            return {\"error_code\": \"UNSUPPORTED\", \"error_message\": \"Cache APIs are only available for the Google provider\"}\n        client = wrapper.client\n        uploaded = []\n        for f in files:\n            p = Path(f)\n            if not p.exists():\n                return {\"error_code\": \"VALIDATION\", \"error_message\": f\"File not found: {f}\"}\n            uploaded.append(client.files.upload(file=p))\n        cfg = types.CreateCachedContentConfig(\n            contents=uploaded,\n            system_instruction=system_instruction if system_instruction else None,\n        )\n        cache = client.caches.create(model=model, config=cfg)\n        return {\n            \"name\": getattr(cache, \"name\", None),\n            \"model\": model,\n            \"system_instruction\": system_instruction,\n            \"contents_count\": len(uploaded),\n        }\n    except Exception as e:\n        return {\"error_code\": \"UNKNOWN\", \"error_message\": str(e)}\n</code></pre>"},{"location":"api/overview/#veotools.api.mcp_api.cache_get","title":"cache_get","text":"<pre><code>cache_get(name: str) -&gt; Dict[str, Any]\n</code></pre> <p>Retrieve cached content metadata by cache name.</p> <p>Fetches information about a previously created cached content entry, including lifecycle information like expiration times and creation dates.</p> PARAMETER DESCRIPTION <code>name</code> <p>The unique cache identifier returned by cache_create_from_files().</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>dict</code> <p>Cache metadata containing: - name (str): The cache identifier - ttl (str|None): Time-to-live if available - expire_time (str|None): Expiration timestamp if available - create_time (str|None): Creation timestamp if available</p> <p> TYPE: <code>Dict[str, Any]</code> </p> <code>Dict[str, Any]</code> <p>On failure, returns: - error_code (str): Error classification - error_message (str): Detailed error description</p> <p>Examples:</p> <p>Check cache status:</p> <pre><code>&gt;&gt;&gt; cache_info = cache_get(cache_name)\n&gt;&gt;&gt; if 'error_code' not in cache_info:\n...     print(f\"Cache {cache_info['name']} is active\")\n...     if cache_info.get('expire_time'):\n...         print(f\"Expires: {cache_info['expire_time']}\")\n... else:\n...     print(f\"Cache not found: {cache_info['error_message']}\")\n</code></pre> Note <p>Available metadata fields may vary depending on the Google GenAI library version and cache configuration.</p> Source code in <code>src\\veotools\\api\\mcp_api.py</code> <pre><code>def cache_get(name: str) -&gt; Dict[str, Any]:\n    \"\"\"Retrieve cached content metadata by cache name.\n\n    Fetches information about a previously created cached content entry,\n    including lifecycle information like expiration times and creation dates.\n\n    Args:\n        name: The unique cache identifier returned by cache_create_from_files().\n\n    Returns:\n        dict: Cache metadata containing:\n            - name (str): The cache identifier\n            - ttl (str|None): Time-to-live if available\n            - expire_time (str|None): Expiration timestamp if available\n            - create_time (str|None): Creation timestamp if available\n\n        On failure, returns:\n            - error_code (str): Error classification\n            - error_message (str): Detailed error description\n\n    Examples:\n        Check cache status:\n        &gt;&gt;&gt; cache_info = cache_get(cache_name)\n        &gt;&gt;&gt; if 'error_code' not in cache_info:\n        ...     print(f\"Cache {cache_info['name']} is active\")\n        ...     if cache_info.get('expire_time'):\n        ...         print(f\"Expires: {cache_info['expire_time']}\")\n        ... else:\n        ...     print(f\"Cache not found: {cache_info['error_message']}\")\n\n    Note:\n        Available metadata fields may vary depending on the Google GenAI\n        library version and cache configuration.\n    \"\"\"\n    \"\"\"Retrieve cached content metadata by name.\n\n    Returns minimal metadata; fields vary by library version.\n    \"\"\"\n    try:\n        wrapper = VeoClient()\n        if wrapper.provider != \"google\":\n            return {\"error_code\": \"UNSUPPORTED\", \"error_message\": \"Cache APIs are only available for the Google provider\"}\n        client = wrapper.client\n        cache = client.caches.get(name=name)\n        out: Dict[str, Any] = {\"name\": getattr(cache, \"name\", name)}\n        # Attempt to surface lifecycle info when available\n        for k in (\"ttl\", \"expire_time\", \"create_time\"):\n            v = getattr(cache, k, None)\n            if v is not None:\n                out[k] = v\n        return out\n    except Exception as e:\n        return {\"error_code\": \"UNKNOWN\", \"error_message\": str(e)}\n</code></pre>"},{"location":"api/overview/#veotools.api.mcp_api.cache_list","title":"cache_list","text":"<pre><code>cache_list() -&gt; Dict[str, Any]\n</code></pre> <p>List all cached content entries with their metadata.</p> <p>Retrieves a list of all cached content entries accessible to the current API key, including their metadata and lifecycle information.</p> RETURNS DESCRIPTION <code>dict</code> <p>Cache listing containing: - caches (list): List of cache entries, each containing:     - name (str): Cache identifier     - model (str|None): Associated model if available     - display_name (str|None): Human-readable name if available     - create_time (str|None): Creation timestamp if available     - update_time (str|None): Last update timestamp if available     - expire_time (str|None): Expiration timestamp if available     - usage_metadata (dict|None): Usage statistics if available</p> <p> TYPE: <code>Dict[str, Any]</code> </p> <code>Dict[str, Any]</code> <p>On failure, returns: - error_code (str): Error classification - error_message (str): Detailed error description</p> <p>Examples:</p> <p>List all caches:</p> <pre><code>&gt;&gt;&gt; cache_list_result = cache_list()\n&gt;&gt;&gt; if 'caches' in cache_list_result:\n...     for cache in cache_list_result['caches']:\n...         print(f\"Cache: {cache['name']}\")\n...         if cache.get('model'):\n...             print(f\"  Model: {cache['model']}\")\n...         if cache.get('expire_time'):\n...             print(f\"  Expires: {cache['expire_time']}\")\n... else:\n...     print(f\"Failed to list caches: {cache_list_result['error_message']}\")\n</code></pre> <p>Find caches by model:</p> <pre><code>&gt;&gt;&gt; result = cache_list()\n&gt;&gt;&gt; if 'caches' in result:\n...     veo3_caches = [\n...         c for c in result['caches']\n...         if c.get('model', '').startswith('veo-3')\n...     ]\n</code></pre> Note <p>Metadata availability depends on the Google GenAI library version and individual cache configurations.</p> Source code in <code>src\\veotools\\api\\mcp_api.py</code> <pre><code>def cache_list() -&gt; Dict[str, Any]:\n    \"\"\"List all cached content entries with their metadata.\n\n    Retrieves a list of all cached content entries accessible to the current\n    API key, including their metadata and lifecycle information.\n\n    Returns:\n        dict: Cache listing containing:\n            - caches (list): List of cache entries, each containing:\n                - name (str): Cache identifier\n                - model (str|None): Associated model if available\n                - display_name (str|None): Human-readable name if available\n                - create_time (str|None): Creation timestamp if available\n                - update_time (str|None): Last update timestamp if available\n                - expire_time (str|None): Expiration timestamp if available\n                - usage_metadata (dict|None): Usage statistics if available\n\n        On failure, returns:\n            - error_code (str): Error classification\n            - error_message (str): Detailed error description\n\n    Examples:\n        List all caches:\n        &gt;&gt;&gt; cache_list_result = cache_list()\n        &gt;&gt;&gt; if 'caches' in cache_list_result:\n        ...     for cache in cache_list_result['caches']:\n        ...         print(f\"Cache: {cache['name']}\")\n        ...         if cache.get('model'):\n        ...             print(f\"  Model: {cache['model']}\")\n        ...         if cache.get('expire_time'):\n        ...             print(f\"  Expires: {cache['expire_time']}\")\n        ... else:\n        ...     print(f\"Failed to list caches: {cache_list_result['error_message']}\")\n\n        Find caches by model:\n        &gt;&gt;&gt; result = cache_list()\n        &gt;&gt;&gt; if 'caches' in result:\n        ...     veo3_caches = [\n        ...         c for c in result['caches']\n        ...         if c.get('model', '').startswith('veo-3')\n        ...     ]\n\n    Note:\n        Metadata availability depends on the Google GenAI library version\n        and individual cache configurations.\n    \"\"\"\n    \"\"\"List cached content metadata entries.\n\n    Returns { caches: [ {name, model?, display_name?, create_time?, update_time?, expire_time?, usage_metadata?} ] }\n    \"\"\"\n    try:\n        wrapper = VeoClient()\n        if wrapper.provider != \"google\":\n            return {\"error_code\": \"UNSUPPORTED\", \"error_message\": \"Cache APIs are only available for the Google provider\"}\n        client = wrapper.client\n        items = []\n        for cache in client.caches.list():\n            entry: Dict[str, Any] = {\"name\": getattr(cache, \"name\", None)}\n            for k in (\"model\", \"display_name\", \"create_time\", \"update_time\", \"expire_time\", \"usage_metadata\"):\n                v = getattr(cache, k, None)\n                if v is not None:\n                    entry[k] = v\n            items.append(entry)\n        return {\"caches\": items}\n    except Exception as e:\n        return {\"error_code\": \"UNKNOWN\", \"error_message\": str(e)}\n</code></pre>"},{"location":"api/overview/#veotools.api.mcp_api.cache_update","title":"cache_update","text":"<pre><code>cache_update(name: str, ttl_seconds: Optional[int] = None, expire_time_iso: Optional[str] = None) -&gt; Dict[str, Any]\n</code></pre> <p>Update TTL or expiration time for a cached content entry.</p> <p>Modifies the lifecycle settings of an existing cached content entry. You can specify either a TTL (time-to-live) in seconds or an absolute expiration time, but not both.</p> PARAMETER DESCRIPTION <code>name</code> <p>The unique cache identifier to update.</p> <p> TYPE: <code>str</code> </p> <code>ttl_seconds</code> <p>Optional time-to-live in seconds (e.g., 300 for 5 minutes).</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>expire_time_iso</code> <p>Optional timezone-aware ISO-8601 datetime string (e.g., \"2024-01-15T10:30:00Z\").</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>dict</code> <p>Update result containing: - name (str): The cache identifier - expire_time (str|None): New expiration time if available - ttl (str|None): New TTL setting if available - update_time (str|None): Update timestamp if available</p> <p> TYPE: <code>Dict[str, Any]</code> </p> <code>Dict[str, Any]</code> <p>On failure, returns: - error_code (str): Error classification - error_message (str): Detailed error description</p> <p>Examples:</p> <p>Extend cache TTL to 1 hour:</p> <pre><code>&gt;&gt;&gt; result = cache_update(cache_name, ttl_seconds=3600)\n&gt;&gt;&gt; if 'error_code' not in result:\n...     print(f\"Cache TTL updated: {result.get('ttl')}\")\n... else:\n...     print(f\"Update failed: {result['error_message']}\")\n</code></pre> <p>Set specific expiration time:</p> <pre><code>&gt;&gt;&gt; result = cache_update(\n...     cache_name,\n...     expire_time_iso=\"2024-12-31T23:59:59Z\"\n... )\n</code></pre> <p>Extend by 30 minutes:</p> <pre><code>&gt;&gt;&gt; result = cache_update(cache_name, ttl_seconds=1800)\n</code></pre> Note <ul> <li>Only one of ttl_seconds or expire_time_iso should be provided</li> <li>TTL is relative to the current time when the update is processed</li> <li>expire_time_iso should be in UTC timezone for consistency</li> </ul> Source code in <code>src\\veotools\\api\\mcp_api.py</code> <pre><code>def cache_update(name: str, ttl_seconds: Optional[int] = None, expire_time_iso: Optional[str] = None) -&gt; Dict[str, Any]:\n    \"\"\"Update TTL or expiration time for a cached content entry.\n\n    Modifies the lifecycle settings of an existing cached content entry.\n    You can specify either a TTL (time-to-live) in seconds or an absolute\n    expiration time, but not both.\n\n    Args:\n        name: The unique cache identifier to update.\n        ttl_seconds: Optional time-to-live in seconds (e.g., 300 for 5 minutes).\n        expire_time_iso: Optional timezone-aware ISO-8601 datetime string\n            (e.g., \"2024-01-15T10:30:00Z\").\n\n    Returns:\n        dict: Update result containing:\n            - name (str): The cache identifier\n            - expire_time (str|None): New expiration time if available\n            - ttl (str|None): New TTL setting if available\n            - update_time (str|None): Update timestamp if available\n\n        On failure, returns:\n            - error_code (str): Error classification\n            - error_message (str): Detailed error description\n\n    Examples:\n        Extend cache TTL to 1 hour:\n        &gt;&gt;&gt; result = cache_update(cache_name, ttl_seconds=3600)\n        &gt;&gt;&gt; if 'error_code' not in result:\n        ...     print(f\"Cache TTL updated: {result.get('ttl')}\")\n        ... else:\n        ...     print(f\"Update failed: {result['error_message']}\")\n\n        Set specific expiration time:\n        &gt;&gt;&gt; result = cache_update(\n        ...     cache_name,\n        ...     expire_time_iso=\"2024-12-31T23:59:59Z\"\n        ... )\n\n        Extend by 30 minutes:\n        &gt;&gt;&gt; result = cache_update(cache_name, ttl_seconds=1800)\n\n    Raises:\n        Returns error dict instead of raising exceptions directly.\n\n    Note:\n        - Only one of ttl_seconds or expire_time_iso should be provided\n        - TTL is relative to the current time when the update is processed\n        - expire_time_iso should be in UTC timezone for consistency\n    \"\"\"\n    \"\"\"Update TTL or expire_time for a cache (one or the other).\n\n    - ttl_seconds: integer seconds for TTL (e.g., 300)\n    - expire_time_iso: timezone-aware ISO-8601 datetime string\n    \"\"\"\n    try:\n        wrapper = VeoClient()\n        if wrapper.provider != \"google\":\n            return {\"error_code\": \"UNSUPPORTED\", \"error_message\": \"Cache APIs are only available for the Google provider\"}\n        client = wrapper.client\n        cfg_kwargs: Dict[str, Any] = {}\n        if ttl_seconds is not None:\n            cfg_kwargs[\"ttl\"] = f\"{int(ttl_seconds)}s\"\n        if expire_time_iso:\n            cfg_kwargs[\"expire_time\"] = expire_time_iso\n        if not cfg_kwargs:\n            return {\"error_code\": \"VALIDATION\", \"error_message\": \"Provide ttl_seconds or expire_time_iso\"}\n        updated = client.caches.update(\n            name=name,\n            config=types.UpdateCachedContentConfig(**cfg_kwargs),\n        )\n        out: Dict[str, Any] = {\"name\": getattr(updated, \"name\", name)}\n        for k in (\"expire_time\", \"ttl\", \"update_time\"):\n            v = getattr(updated, k, None)\n            if v is not None:\n                out[k] = v\n        return out\n    except Exception as e:\n        return {\"error_code\": \"UNKNOWN\", \"error_message\": str(e)}\n</code></pre>"},{"location":"api/overview/#veotools.api.mcp_api.cache_delete","title":"cache_delete","text":"<pre><code>cache_delete(name: str) -&gt; Dict[str, Any]\n</code></pre> <p>Delete a cached content entry by name.</p> <p>Permanently removes a cached content entry and all associated files from the system. This action cannot be undone.</p> PARAMETER DESCRIPTION <code>name</code> <p>The unique cache identifier to delete.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>dict</code> <p>Deletion result containing: - deleted (bool): True if deletion was successful - name (str): The cache identifier that was deleted</p> <p> TYPE: <code>Dict[str, Any]</code> </p> <code>Dict[str, Any]</code> <p>On failure, returns: - error_code (str): Error classification - error_message (str): Detailed error description</p> <p>Examples:</p> <p>Delete a specific cache:</p> <pre><code>&gt;&gt;&gt; result = cache_delete(cache_name)\n&gt;&gt;&gt; if result.get('deleted'):\n...     print(f\"Cache {result['name']} deleted successfully\")\n... else:\n...     print(f\"Deletion failed: {result.get('error_message')}\")\n</code></pre> <p>Delete with error handling:</p> <pre><code>&gt;&gt;&gt; result = cache_delete(\"non-existent-cache\")\n&gt;&gt;&gt; if 'error_code' in result:\n...     print(f\"Error: {result['error_message']}\")\n</code></pre> Note <p>Deletion is permanent and cannot be reversed. Ensure you no longer need the cached content before calling this function.</p> Source code in <code>src\\veotools\\api\\mcp_api.py</code> <pre><code>def cache_delete(name: str) -&gt; Dict[str, Any]:\n    \"\"\"Delete a cached content entry by name.\n\n    Permanently removes a cached content entry and all associated files\n    from the system. This action cannot be undone.\n\n    Args:\n        name: The unique cache identifier to delete.\n\n    Returns:\n        dict: Deletion result containing:\n            - deleted (bool): True if deletion was successful\n            - name (str): The cache identifier that was deleted\n\n        On failure, returns:\n            - error_code (str): Error classification\n            - error_message (str): Detailed error description\n\n    Examples:\n        Delete a specific cache:\n        &gt;&gt;&gt; result = cache_delete(cache_name)\n        &gt;&gt;&gt; if result.get('deleted'):\n        ...     print(f\"Cache {result['name']} deleted successfully\")\n        ... else:\n        ...     print(f\"Deletion failed: {result.get('error_message')}\")\n\n        Delete with error handling:\n        &gt;&gt;&gt; result = cache_delete(\"non-existent-cache\")\n        &gt;&gt;&gt; if 'error_code' in result:\n        ...     print(f\"Error: {result['error_message']}\")\n\n    Note:\n        Deletion is permanent and cannot be reversed. Ensure you no longer\n        need the cached content before calling this function.\n    \"\"\"\n    \"\"\"Delete a cached content entry by name.\"\"\"\n    try:\n        wrapper = VeoClient()\n        if wrapper.provider != \"google\":\n            return {\"error_code\": \"UNSUPPORTED\", \"error_message\": \"Cache APIs are only available for the Google provider\"}\n        client = wrapper.client\n        client.caches.delete(name)\n        return {\"deleted\": True, \"name\": name}\n    except Exception as e:\n        return {\"error_code\": \"UNKNOWN\", \"error_message\": str(e)}\n</code></pre>"},{"location":"api/overview/#veotools.api.mcp_api.plan_scenes","title":"plan_scenes","text":"<pre><code>plan_scenes(*, idea: str, number_of_scenes: int = 4, character_description: Optional[str] = None, character_characteristics: Optional[str] = None, video_type: Optional[str] = None, video_characteristics: Optional[str] = None, camera_angle: Optional[str] = None, additional_context: Optional[str] = None, references: Optional[List[Dict[str, Any]]] = None, model: Optional[str] = None) -&gt; Dict[str, Any]\n</code></pre> <p>Generate a structured Gemini-authored scene plan.</p> PARAMETER DESCRIPTION <code>idea</code> <p>Core concept for the plan.</p> <p> TYPE: <code>str</code> </p> <code>number_of_scenes</code> <p>Number of clips to request.</p> <p> TYPE: <code>int</code> DEFAULT: <code>4</code> </p> <code>character_description</code> <p>Baseline character description passed to Gemini.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>character_characteristics</code> <p>Character personality notes.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>video_type</code> <p>Label for the production (e.g., vlog, trailer).</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>video_characteristics</code> <p>Overall stylistic guidance.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>camera_angle</code> <p>Primary camera/perspective direction.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>additional_context</code> <p>Extra instructions for Gemini.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>references</code> <p>Optional list of character reference dicts.</p> <p> TYPE: <code>Optional[List[Dict[str, Any]]]</code> DEFAULT: <code>None</code> </p> <code>model</code> <p>Gemini model override.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>dict</code> <p>Parsed plan payload or error structure.</p> <p> TYPE: <code>Dict[str, Any]</code> </p> Source code in <code>src\\veotools\\api\\mcp_api.py</code> <pre><code>def plan_scenes(\n    *,\n    idea: str,\n    number_of_scenes: int = 4,\n    character_description: Optional[str] = None,\n    character_characteristics: Optional[str] = None,\n    video_type: Optional[str] = None,\n    video_characteristics: Optional[str] = None,\n    camera_angle: Optional[str] = None,\n    additional_context: Optional[str] = None,\n    references: Optional[List[Dict[str, Any]]] = None,\n    model: Optional[str] = None,\n) -&gt; Dict[str, Any]:\n    \"\"\"Generate a structured Gemini-authored scene plan.\n\n    Args:\n        idea: Core concept for the plan.\n        number_of_scenes: Number of clips to request.\n        character_description: Baseline character description passed to Gemini.\n        character_characteristics: Character personality notes.\n        video_type: Label for the production (e.g., vlog, trailer).\n        video_characteristics: Overall stylistic guidance.\n        camera_angle: Primary camera/perspective direction.\n        additional_context: Extra instructions for Gemini.\n        references: Optional list of character reference dicts.\n        model: Gemini model override.\n\n    Returns:\n        dict: Parsed plan payload or error structure.\n    \"\"\"\n\n    try:\n        kwargs: Dict[str, Any] = {\"number_of_scenes\": number_of_scenes}\n        if additional_context:\n            kwargs[\"additional_context\"] = additional_context\n        if character_description:\n            kwargs[\"character_description\"] = character_description\n        if character_characteristics:\n            kwargs[\"character_characteristics\"] = character_characteristics\n        if references:\n            kwargs[\"character_references\"] = references\n        if video_type:\n            kwargs[\"video_type\"] = video_type\n        if video_characteristics:\n            kwargs[\"video_characteristics\"] = video_characteristics\n        if camera_angle:\n            kwargs[\"camera_angle\"] = camera_angle\n        if model:\n            kwargs[\"model\"] = model\n\n        plan = generate_scene_plan(idea, **kwargs)\n        return json.loads(plan.model_dump_json())\n    except ValueError as e:\n        return {\"error_code\": \"VALIDATION\", \"error_message\": str(e)}\n    except Exception as e:\n        return {\"error_code\": \"UNKNOWN\", \"error_message\": str(e)}\n</code></pre>"},{"location":"api/overview/#planning-modules","title":"Planning Modules","text":""},{"location":"api/overview/#veotools.plan.scene_writer","title":"veotools.plan.scene_writer","text":"<p>Scene planning helpers powered by Gemini.</p> CLASS DESCRIPTION <code>Clip</code> <p>Defines a single video segment or shot.</p> <code>CharacterProfile</code> <p>A detailed, consistent profile of the character's core attributes.</p> <code>ScenePlan</code> <p>Structured response containing characters and clips.</p> <code>SceneWriter</code> <p>High-level helper for generating structured scene plans.</p> FUNCTION DESCRIPTION <code>generate_scene_plan</code> <p>Convenience wrapper around :class:<code>SceneWriter</code> for one-off calls.</p>"},{"location":"api/overview/#veotools.plan.scene_writer-classes","title":"Classes","text":""},{"location":"api/overview/#veotools.plan.scene_writer.Shot","title":"Shot","text":"<p>               Bases: <code>BaseModel</code></p> <p>Technical camera details for a specific clip.</p>"},{"location":"api/overview/#veotools.plan.scene_writer.Subject","title":"Subject","text":"<p>               Bases: <code>BaseModel</code></p> <p>Describes the character's appearance and wardrobe within a specific clip.</p>"},{"location":"api/overview/#veotools.plan.scene_writer.Scene","title":"Scene","text":"<p>               Bases: <code>BaseModel</code></p> <p>Describes the setting and environment of the clip.</p>"},{"location":"api/overview/#veotools.plan.scene_writer.VisualDetails","title":"VisualDetails","text":"<p>               Bases: <code>BaseModel</code></p> <p>Actions and props within the clip.</p>"},{"location":"api/overview/#veotools.plan.scene_writer.Cinematography","title":"Cinematography","text":"<p>               Bases: <code>BaseModel</code></p> <p>Lighting, tone, and colour direction for the clip.</p>"},{"location":"api/overview/#veotools.plan.scene_writer.AudioTrack","title":"AudioTrack","text":"<p>               Bases: <code>BaseModel</code></p> <p>Defines the sound elements specific to this clip.</p>"},{"location":"api/overview/#veotools.plan.scene_writer.Dialogue","title":"Dialogue","text":"<p>               Bases: <code>BaseModel</code></p> <p>Defines the spoken lines and how they are presented.</p>"},{"location":"api/overview/#veotools.plan.scene_writer.Performance","title":"Performance","text":"<p>               Bases: <code>BaseModel</code></p> <p>Controls for the character's animated performance in this clip.</p>"},{"location":"api/overview/#veotools.plan.scene_writer.Clip","title":"Clip","text":"<p>               Bases: <code>BaseModel</code></p> <p>Defines a single video segment or shot.</p>"},{"location":"api/overview/#veotools.plan.scene_writer.CharacterProfile","title":"CharacterProfile","text":"<p>               Bases: <code>BaseModel</code></p> <p>A detailed, consistent profile of the character's core attributes.</p>"},{"location":"api/overview/#veotools.plan.scene_writer.ScenePlan","title":"ScenePlan","text":"<p>               Bases: <code>BaseModel</code></p> <p>Structured response containing characters and clips.</p> METHOD DESCRIPTION <code>model_dump_json</code> <p>Provide JSON serialisation helper that matches BaseModel signature.</p>"},{"location":"api/overview/#veotools.plan.scene_writer.ScenePlan-functions","title":"Functions","text":""},{"location":"api/overview/#veotools.plan.scene_writer.ScenePlan.model_dump_json","title":"model_dump_json","text":"<pre><code>model_dump_json(*args, **kwargs) -&gt; str\n</code></pre> <p>Provide JSON serialisation helper that matches BaseModel signature.</p> Source code in <code>src\\veotools\\plan\\scene_writer.py</code> <pre><code>def model_dump_json(self, *args, **kwargs) -&gt; str:  # type: ignore[override]\n    \"\"\"Provide JSON serialisation helper that matches BaseModel signature.\"\"\"\n    return super().model_dump_json(*args, **kwargs)\n</code></pre>"},{"location":"api/overview/#veotools.plan.scene_writer.SceneWriter","title":"SceneWriter","text":"<pre><code>SceneWriter(model: str = 'gemini-2.5-pro')\n</code></pre> <p>High-level helper for generating structured scene plans.</p> METHOD DESCRIPTION <code>generate</code> <p>Generate a structured scene plan from a high-level idea.</p> Source code in <code>src\\veotools\\plan\\scene_writer.py</code> <pre><code>def __init__(self, model: str = \"gemini-2.5-pro\"):\n    self.model = model\n    wrapper = VeoClient()\n    self._provider = getattr(wrapper, \"provider\", \"google\")\n    self._client = getattr(wrapper, \"client\", None)\n    if self._client is None:\n        raise RuntimeError(\"Failed to initialise provider client for scene writing\")\n</code></pre>"},{"location":"api/overview/#veotools.plan.scene_writer.SceneWriter-functions","title":"Functions","text":""},{"location":"api/overview/#veotools.plan.scene_writer.SceneWriter.generate","title":"generate","text":"<pre><code>generate(idea: str, *, number_of_scenes: int = 4, additional_context: Optional[str] = None, character_description: Optional[str] = None, character_characteristics: Optional[str] = None, character_references: Optional[Sequence[CharacterProfile | dict]] = None, video_type: Optional[str] = None, video_characteristics: Optional[str] = None, camera_angle: Optional[str] = None, model: Optional[str] = None, config: Optional[GenerateContentConfig] = None, response_model: type[ScenePlan] = ScenePlan, save_path: Optional[Path | str] = None) -&gt; ScenePlan\n</code></pre> <p>Generate a structured scene plan from a high-level idea.</p> PARAMETER DESCRIPTION <code>idea</code> <p>Core concept for the story (plain text).</p> <p> TYPE: <code>str</code> </p> <code>number_of_scenes</code> <p>Desired number of clips in the plan.</p> <p> TYPE: <code>int</code> DEFAULT: <code>4</code> </p> <code>additional_context</code> <p>Optional instructions or style notes.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>character_description</code> <p>Baseline description of the lead character.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>character_characteristics</code> <p>Personality traits for the character.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>character_references</code> <p>Optional list of reference profiles passed to Gemini.</p> <p> TYPE: <code>Optional[Sequence[CharacterProfile | dict]]</code> DEFAULT: <code>None</code> </p> <code>video_type</code> <p>High-level type of production (e.g., \"vlog\", \"commercial\").</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>video_characteristics</code> <p>Overall stylistic notes (e.g., \"realistic, 4k, cinematic\").</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>camera_angle</code> <p>Primary camera/perspective guidance for the scenes.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>model</code> <p>Override the Gemini model name.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>config</code> <p>Custom GenerateContentConfig; if omitted a JSON schema config is used.</p> <p> TYPE: <code>Optional[GenerateContentConfig]</code> DEFAULT: <code>None</code> </p> <code>response_model</code> <p>Pydantic model describing the expected response payload.</p> <p> TYPE: <code>type[ScenePlan]</code> DEFAULT: <code>ScenePlan</code> </p> <code>save_path</code> <p>Optional file path for writing the raw JSON plan.</p> <p> TYPE: <code>Optional[Path | str]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>ScenePlan</code> <p>Parsed response from Gemini respecting the response_model schema.</p> <p> TYPE: <code>ScenePlan</code> </p> Source code in <code>src\\veotools\\plan\\scene_writer.py</code> <pre><code>def generate(\n    self,\n    idea: str,\n    *,\n    number_of_scenes: int = 4,\n    additional_context: Optional[str] = None,\n    character_description: Optional[str] = None,\n    character_characteristics: Optional[str] = None,\n    character_references: Optional[Sequence[CharacterProfile | dict]] = None,\n    video_type: Optional[str] = None,\n    video_characteristics: Optional[str] = None,\n    camera_angle: Optional[str] = None,\n    model: Optional[str] = None,\n    config: Optional[types.GenerateContentConfig] = None,\n    response_model: type[ScenePlan] = ScenePlan,\n    save_path: Optional[Path | str] = None,\n) -&gt; ScenePlan:\n    \"\"\"Generate a structured scene plan from a high-level idea.\n\n    Args:\n        idea: Core concept for the story (plain text).\n        number_of_scenes: Desired number of clips in the plan.\n        additional_context: Optional instructions or style notes.\n        character_description: Baseline description of the lead character.\n        character_characteristics: Personality traits for the character.\n        character_references: Optional list of reference profiles passed to Gemini.\n        video_type: High-level type of production (e.g., \"vlog\", \"commercial\").\n        video_characteristics: Overall stylistic notes (e.g., \"realistic, 4k, cinematic\").\n        camera_angle: Primary camera/perspective guidance for the scenes.\n        model: Override the Gemini model name.\n        config: Custom GenerateContentConfig; if omitted a JSON schema config is used.\n        response_model: Pydantic model describing the expected response payload.\n        save_path: Optional file path for writing the raw JSON plan.\n\n    Returns:\n        ScenePlan: Parsed response from Gemini respecting the response_model schema.\n    \"\"\"\n\n    optional_inputs, custom_guidance = self._build_prompt_sections(\n        character_description=character_description,\n        character_characteristics=character_characteristics,\n        additional_context=additional_context,\n        character_references=character_references,\n        video_type=video_type,\n        video_characteristics=video_characteristics,\n        camera_angle=camera_angle,\n    )\n\n    prompt = BASE_GUIDANCE.format(\n        idea=idea.strip(),\n        number_of_scenes=number_of_scenes,\n        optional_inputs=optional_inputs,\n        custom_guidance=custom_guidance,\n    )\n\n    schema = response_model.model_json_schema()\n\n    if self._provider == \"daydreams\":\n        raw = self._generate_plan_daydreams(prompt, schema, model or self.model)\n    else:\n        request_config = config or types.GenerateContentConfig(\n            response_mime_type=\"application/json\",\n            response_json_schema=schema,\n        )\n        raw = self._generate_plan_google(prompt, request_config, model or self.model)\n\n    if not raw:\n        raise RuntimeError(\"Scene writer returned an empty response\")\n\n    plan = response_model.model_validate_json(raw)\n\n    if save_path:\n        path = Path(save_path)\n        path.parent.mkdir(parents=True, exist_ok=True)\n        path.write_text(plan.model_dump_json(indent=2), encoding=\"utf-8\")\n\n    return plan\n</code></pre>"},{"location":"api/overview/#veotools.plan.scene_writer-functions","title":"Functions","text":""},{"location":"api/overview/#veotools.plan.scene_writer.generate_scene_plan","title":"generate_scene_plan","text":"<pre><code>generate_scene_plan(idea: str, *, number_of_scenes: int = 4, additional_context: Optional[str] = None, character_description: Optional[str] = None, character_characteristics: Optional[str] = None, character_references: Optional[Sequence[CharacterProfile | dict]] = None, video_type: Optional[str] = None, video_characteristics: Optional[str] = None, camera_angle: Optional[str] = None, model: str = 'gemini-2.5-pro', save_path: Optional[Path | str] = None) -&gt; ScenePlan\n</code></pre> <p>Convenience wrapper around :class:<code>SceneWriter</code> for one-off calls.</p> Source code in <code>src\\veotools\\plan\\scene_writer.py</code> <pre><code>def generate_scene_plan(\n    idea: str,\n    *,\n    number_of_scenes: int = 4,\n    additional_context: Optional[str] = None,\n    character_description: Optional[str] = None,\n    character_characteristics: Optional[str] = None,\n    character_references: Optional[Sequence[CharacterProfile | dict]] = None,\n    video_type: Optional[str] = None,\n    video_characteristics: Optional[str] = None,\n    camera_angle: Optional[str] = None,\n    model: str = \"gemini-2.5-pro\",\n    save_path: Optional[Path | str] = None,\n) -&gt; ScenePlan:\n    \"\"\"Convenience wrapper around :class:`SceneWriter` for one-off calls.\"\"\"\n\n    writer = SceneWriter(model=model)\n    return writer.generate(\n        idea,\n        number_of_scenes=number_of_scenes,\n        additional_context=additional_context,\n        character_description=character_description,\n        character_characteristics=character_characteristics,\n        character_references=character_references,\n        video_type=video_type,\n        video_characteristics=video_characteristics,\n        camera_angle=camera_angle,\n        save_path=save_path,\n    )\n</code></pre>"},{"location":"api/overview/#veotools.plan.executor","title":"veotools.plan.executor","text":"<p>Execute scene plans into rendered videos.</p> CLASS DESCRIPTION <code>PlanExecutionResult</code> <p>Container for executing a :class:<code>ScenePlan</code>.</p> FUNCTION DESCRIPTION <code>execute_scene_plan</code> <p>Render all clips in a scene plan and optionally stitch the results.</p>"},{"location":"api/overview/#veotools.plan.executor-classes","title":"Classes","text":""},{"location":"api/overview/#veotools.plan.executor.PlanExecutionResult","title":"PlanExecutionResult  <code>dataclass</code>","text":"<pre><code>PlanExecutionResult(plan: ScenePlan, clip_prompts: List[str], clip_results: List[VideoResult], final_result: Optional[VideoResult])\n</code></pre> <p>Container for executing a :class:<code>ScenePlan</code>.</p> ATTRIBUTE DESCRIPTION <code>plan</code> <p>The validated plan that was executed.</p> <p> TYPE: <code>ScenePlan</code> </p> <code>clip_prompts</code> <p>Prompts used for each clip in execution order.</p> <p> TYPE: <code>List[str]</code> </p> <code>clip_results</code> <p>Video results returned by Veo for each clip.</p> <p> TYPE: <code>List[VideoResult]</code> </p> <code>final_result</code> <p>The stitched video result, if stitching was requested.</p> <p> TYPE: <code>Optional[VideoResult]</code> </p> METHOD DESCRIPTION <code>to_dict</code> <p>Convert execution details to a JSON-friendly dictionary.</p>"},{"location":"api/overview/#veotools.plan.executor.PlanExecutionResult-functions","title":"Functions","text":""},{"location":"api/overview/#veotools.plan.executor.PlanExecutionResult.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict[str, object]\n</code></pre> <p>Convert execution details to a JSON-friendly dictionary.</p> Source code in <code>src\\veotools\\plan\\executor.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, object]:\n    \"\"\"Convert execution details to a JSON-friendly dictionary.\"\"\"\n\n    return {\n        \"clips\": [\n            {\n                \"prompt\": prompt,\n                \"result\": result.to_dict(),\n            }\n            for prompt, result in zip(self.clip_prompts, self.clip_results)\n        ],\n        \"final_result\": self.final_result.to_dict() if self.final_result else None,\n    }\n</code></pre>"},{"location":"api/overview/#veotools.plan.executor-functions","title":"Functions","text":""},{"location":"api/overview/#veotools.plan.executor.execute_scene_plan","title":"execute_scene_plan","text":"<pre><code>execute_scene_plan(plan: ScenePlan | Path | str | Dict[str, object], *, model: str = 'veo-3.0-generate-001', prompt_builder: Optional[PromptBuilder] = None, image_provider: Optional[ImageProvider] = None, clip_options: Optional[ClipOptionsProvider] = None, stitch: bool = True, overlap: float = 1.0, auto_seed_last_frame: bool = False, seed_frame_offset: float = -0.5, on_progress: Optional[Callable[[str, int], None]] = None) -&gt; PlanExecutionResult\n</code></pre> <p>Render all clips in a scene plan and optionally stitch the results.</p> PARAMETER DESCRIPTION <code>plan</code> <p>ScenePlan instance, dict payload, or path to plan JSON.</p> <p> TYPE: <code>ScenePlan | Path | str | Dict[str, object]</code> </p> <code>model</code> <p>Default Veo model to use for clip generation.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'veo-3.0-generate-001'</code> </p> <code>prompt_builder</code> <p>Optional callable to customize prompts per clip.</p> <p> TYPE: <code>Optional[PromptBuilder]</code> DEFAULT: <code>None</code> </p> <code>image_provider</code> <p>Optional callable returning a seed image path per clip.</p> <p> TYPE: <code>Optional[ImageProvider]</code> DEFAULT: <code>None</code> </p> <code>clip_options</code> <p>Optional callable providing extra keyword args per clip.</p> <p> TYPE: <code>Optional[ClipOptionsProvider]</code> DEFAULT: <code>None</code> </p> <code>stitch</code> <p>Whether to stitch the rendered clips into a final timeline.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>overlap</code> <p>Overlap trimming in seconds when stitching.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>auto_seed_last_frame</code> <p>When True, extract a frame from each rendered clip and feed it as the seed image for the next clip (unless an image_provider supplies one explicitly).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>seed_frame_offset</code> <p>Time offset (seconds) used when extracting the frame from each clip for auto seeding. Defaults to -0.5 (half a second from end).</p> <p> TYPE: <code>float</code> DEFAULT: <code>-0.5</code> </p> <code>on_progress</code> <p>Optional progress callback used for generation and stitching.</p> <p> TYPE: <code>Optional[Callable[[str, int], None]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>PlanExecutionResult</code> <p>PlanExecutionResult containing individual clip results and optional final video.</p> Source code in <code>src\\veotools\\plan\\executor.py</code> <pre><code>def execute_scene_plan(\n    plan: ScenePlan | Path | str | Dict[str, object],\n    *,\n    model: str = \"veo-3.0-generate-001\",\n    prompt_builder: Optional[PromptBuilder] = None,\n    image_provider: Optional[ImageProvider] = None,\n    clip_options: Optional[ClipOptionsProvider] = None,\n    stitch: bool = True,\n    overlap: float = 1.0,\n    auto_seed_last_frame: bool = False,\n    seed_frame_offset: float = -0.5,\n    on_progress: Optional[Callable[[str, int], None]] = None,\n) -&gt; PlanExecutionResult:\n    \"\"\"Render all clips in a scene plan and optionally stitch the results.\n\n    Args:\n        plan: ScenePlan instance, dict payload, or path to plan JSON.\n        model: Default Veo model to use for clip generation.\n        prompt_builder: Optional callable to customize prompts per clip.\n        image_provider: Optional callable returning a seed image path per clip.\n        clip_options: Optional callable providing extra keyword args per clip.\n        stitch: Whether to stitch the rendered clips into a final timeline.\n        overlap: Overlap trimming in seconds when stitching.\n        auto_seed_last_frame: When True, extract a frame from each rendered clip and\n            feed it as the seed image for the next clip (unless an image_provider\n            supplies one explicitly).\n        seed_frame_offset: Time offset (seconds) used when extracting the frame\n            from each clip for auto seeding. Defaults to -0.5 (half a second from end).\n        on_progress: Optional progress callback used for generation and stitching.\n\n    Returns:\n        PlanExecutionResult containing individual clip results and optional final video.\n    \"\"\"\n\n    scene_plan = _load_plan_like(plan)\n    prompt_builder = prompt_builder or _default_prompt_builder\n    clip_prompts: List[str] = []\n    clip_results: List[VideoResult] = []\n\n    total_clips = len(scene_plan.clips)\n\n    last_seed_frame: Optional[Path] = None\n\n    for idx, clip in enumerate(scene_plan.clips):\n        prompt = prompt_builder(clip)\n        clip_prompts.append(prompt)\n\n        per_clip_kwargs = clip_options(clip, idx, scene_plan) if clip_options else {}\n        per_clip_kwargs = dict(per_clip_kwargs or {})\n        clip_model = per_clip_kwargs.pop(\"model\", model)\n        # Respect aspect ratio from the plan if not overridden.\n        per_clip_kwargs.setdefault(\"aspect_ratio\", clip.aspect_ratio)\n\n        def _progress_wrapper(message: str, percent: int) -&gt; None:\n            if on_progress:\n                on_progress(\n                    f\"Clip {idx + 1}/{total_clips}: {message}\",\n                    percent,\n                )\n\n        wrapped_progress = _progress_wrapper if on_progress else None\n\n        image_path: Optional[Path] = None\n        if image_provider:\n            provided = image_provider(clip, idx, scene_plan)\n            if provided:\n                image_path = Path(provided)\n\n        if image_path is None and auto_seed_last_frame and last_seed_frame:\n            image_path = last_seed_frame\n\n        if image_path:\n            result = generate_from_image(\n                image_path,\n                prompt,\n                model=clip_model,\n                on_progress=wrapped_progress,\n                **per_clip_kwargs,\n            )\n        else:\n            result = generate_from_text(\n                prompt,\n                model=clip_model,\n                on_progress=wrapped_progress,\n                **per_clip_kwargs,\n            )\n\n        clip_results.append(result)\n\n        if auto_seed_last_frame and result.path and idx &lt; total_clips - 1:\n            try:\n                last_seed_frame = extract_frame(\n                    result.path,\n                    time_offset=seed_frame_offset,\n                )\n            except Exception:\n                last_seed_frame = None\n\n    clip_paths: List[Path] = [\n        result.path for result in clip_results if result.path is not None\n    ]\n\n    final_result: Optional[VideoResult] = None\n    if stitch and len(clip_paths) &gt;= 2:\n\n        def _stitch_progress(message: str, percent: int) -&gt; None:\n            if on_progress:\n                on_progress(f\"Stitching: {message}\", percent)\n\n        final_result = stitch_videos(\n            clip_paths,\n            overlap=overlap,\n            on_progress=_stitch_progress if on_progress else None,\n        )\n\n    return PlanExecutionResult(\n        plan=scene_plan,\n        clip_prompts=clip_prompts,\n        clip_results=clip_results,\n        final_result=final_result,\n    )\n</code></pre>"},{"location":"api/generate/video/","title":"Video Generation API","text":"<p>The video generation module provides functions for creating videos from various inputs using Google's Veo models.</p>"},{"location":"api/generate/video/#functions","title":"Functions","text":""},{"location":"api/generate/video/#generate_from_text","title":"generate_from_text","text":""},{"location":"api/generate/video/#veotools.generate.video.generate_from_text","title":"veotools.generate.video.generate_from_text","text":"<pre><code>generate_from_text(prompt: str, model: str = 'veo-3.0-fast-generate-preview', duration_seconds: Optional[int] = None, on_progress: Optional[Callable] = None, **kwargs) -&gt; VideoResult\n</code></pre> <p>Generate a video from a text prompt.</p> <p>Automatically selects the active video provider (Google GenAI or Daydreams Router) based on configuration. Falls back to Google behaviour when no provider override is specified.</p> Source code in <code>src\\veotools\\generate\\video.py</code> <pre><code>def generate_from_text(\n    prompt: str,\n    model: str = \"veo-3.0-fast-generate-preview\",\n    duration_seconds: Optional[int] = None,\n    on_progress: Optional[Callable] = None,\n    **kwargs,\n) -&gt; VideoResult:\n    \"\"\"Generate a video from a text prompt.\n\n    Automatically selects the active video provider (Google GenAI or Daydreams Router)\n    based on configuration. Falls back to Google behaviour when no provider override\n    is specified.\n    \"\"\"\n\n    veo_client = VeoClient()\n    if veo_client.provider == \"daydreams\":\n        return _generate_from_text_daydreams(\n            veo_client.client,\n            prompt,\n            model=model,\n            duration_seconds=duration_seconds,\n            on_progress=on_progress,\n            **kwargs,\n        )\n\n    return _generate_from_text_google(\n        veo_client.client,\n        prompt,\n        model=model,\n        duration_seconds=duration_seconds,\n        on_progress=on_progress,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/generate/video/#generate_from_image","title":"generate_from_image","text":""},{"location":"api/generate/video/#veotools.generate.video.generate_from_image","title":"veotools.generate.video.generate_from_image","text":"<pre><code>generate_from_image(image_path: Path, prompt: str, model: str = 'veo-3.0-fast-generate-preview', on_progress: Optional[Callable] = None, **kwargs) -&gt; VideoResult\n</code></pre> <p>Generate a video from an image seed.</p> Source code in <code>src\\veotools\\generate\\video.py</code> <pre><code>def generate_from_image(\n    image_path: Path,\n    prompt: str,\n    model: str = \"veo-3.0-fast-generate-preview\",\n    on_progress: Optional[Callable] = None,\n    **kwargs,\n) -&gt; VideoResult:\n    \"\"\"Generate a video from an image seed.\"\"\"\n\n    veo_client = VeoClient()\n    if veo_client.provider == \"daydreams\":\n        raise NotImplementedError(\n            \"Daydreams Router provider does not currently support image-to-video generation in veotools\"\n        )\n\n    return _generate_from_image_google(\n        veo_client.client,\n        image_path,\n        prompt,\n        model=model,\n        on_progress=on_progress,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/generate/video/#generate_from_video","title":"generate_from_video","text":""},{"location":"api/generate/video/#veotools.generate.video.generate_from_video","title":"veotools.generate.video.generate_from_video","text":"<pre><code>generate_from_video(video_path: Path, prompt: str, extract_at: float = -1.0, model: str = 'veo-3.0-fast-generate-preview', on_progress: Optional[Callable] = None, **kwargs) -&gt; VideoResult\n</code></pre> <p>Generate a video continuation from an existing clip.</p> Source code in <code>src\\veotools\\generate\\video.py</code> <pre><code>def generate_from_video(\n    video_path: Path,\n    prompt: str,\n    extract_at: float = -1.0,\n    model: str = \"veo-3.0-fast-generate-preview\",\n    on_progress: Optional[Callable] = None,\n    **kwargs,\n) -&gt; VideoResult:\n    \"\"\"Generate a video continuation from an existing clip.\"\"\"\n\n    veo_client = VeoClient()\n    if veo_client.provider == \"daydreams\":\n        raise NotImplementedError(\n            \"Daydreams Router provider does not currently support video-seeded continuation in veotools\"\n        )\n\n    return _generate_from_video_google(\n        veo_client.client,\n        video_path,\n        prompt,\n        extract_at=extract_at,\n        model=model,\n        on_progress=on_progress,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/generate/video/#examples","title":"Examples","text":""},{"location":"api/generate/video/#text-to-video","title":"Text to Video","text":"<pre><code>import veotools as veo\n\nveo.init()\n\n# Basic generation\nresult = veo.generate_from_text(\n    prompt=\"A cat playing piano\",\n    model=\"veo-3.0-fast-generate-preview\"\n)\n\n# With progress tracking\ndef on_progress(message: str, percent: int):\n    print(f\"{message}: {percent}%\")\n\nresult = veo.generate_from_text(\n    prompt=\"Sunset over mountains\",\n    model=\"veo-3.0-fast-generate-preview\",\n    on_progress=on_progress\n)\n</code></pre>"},{"location":"api/generate/video/#image-to-video","title":"Image to Video","text":"<pre><code># Generate video from an image\nresult = veo.generate_from_image(\n    image_path=\"landscape.jpg\",\n    prompt=\"The landscape comes to life with moving clouds\",\n    model=\"veo-3.0-fast-generate-preview\"\n)\n</code></pre>"},{"location":"api/generate/video/#video-continuation","title":"Video Continuation","text":"<pre><code># Continue from existing video\nresult = veo.generate_from_video(\n    video_path=\"intro.mp4\",\n    prompt=\"The scene transitions to a forest\",\n    extract_at=-1.0,  # Use last frame\n    model=\"veo-3.0-fast-generate-preview\"\n)\n</code></pre>"},{"location":"api/generate/video/#model-support","title":"Model Support","text":"<p>Different Veo models support different features:</p> Model Duration Control Aspect Ratio Audio Generation Time veo-3.0-fast-generate-preview \u274c \u2705 (16:9) \u2705 ~1 min veo-3.0-generate-preview \u274c \u2705 (16:9) \u2705 ~2 min veo-2.0-generate-001 \u2705 \u2705 (16:9, 9:16) \u274c ~3 min"},{"location":"api/generate/video/#safety-settings","title":"Safety Settings","text":"<p>You can pass safety settings to control content generation:</p> <pre><code>from google.genai import types\n\nsafety_settings = [\n    types.SafetySetting(\n        category=\"HARM_CATEGORY_HARASSMENT\",\n        threshold=\"BLOCK_ONLY_HIGH\"\n    )\n]\n\nresult = veo.generate_from_text(\n    prompt=\"A friendly conversation\",\n    safety_settings=safety_settings\n)\n</code></pre>"}]}